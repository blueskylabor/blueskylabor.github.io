<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>ebxeax</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="ebxeax">
<meta property="og:url" content="http://ebxeax.github.io/page/5/index.html">
<meta property="og:site_name" content="ebxeax">
<meta property="og:locale">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="ebxeax" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">ebxeax</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Suche"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://ebxeax.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-指令系统" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/%E6%8C%87%E4%BB%A4%E7%B3%BB%E7%BB%9F/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="指令系统"><a href="#指令系统" class="headerlink" title="指令系统"></a>指令系统</h1><h2 id="指令系统-1"><a href="#指令系统-1" class="headerlink" title="指令系统"></a>指令系统</h2><p>是指令集体系结构ISA的核心<br>ISA主要包括：</p>
<ul>
<li>指令格式</li>
<li>数据类型及格式</li>
<li>操作数的存放方式</li>
<li>程序可访问的寄存器个数、位数和编号 </li>
<li>存储空间大小和编址方式</li>
<li>寻址方式</li>
<li>指令执行过程的控制方式等</li>
</ul>
<h3 id="指令的基本格式"><a href="#指令的基本格式" class="headerlink" title="指令的基本格式"></a>指令的基本格式</h3><p>一条指令包括操作码和地址码字段</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>操作码</td>
<td>地址码</td>
</tr>
</tbody></table>
<p>操作码:</p>
<ul>
<li>指出指令应执行的操作</li>
<li>识别指令</li>
<li>了解指令功能</li>
<li>区分操作数地址内容的组成和使用方法</li>
</ul>
<p>地址码：</p>
<ul>
<li>给出被操作的信息的地址</li>
<li>参加运算的一个或多个操作数所在的地址</li>
<li>运算结果的保存地址</li>
<li>程序的转移地址</li>
<li>被调用的子程序的入口地址等</li>
</ul>
<p>指令长度是指一条指令中包含的二进制代码的位数<br>指令字长取决于</p>
<ul>
<li>操作码的长度</li>
<li>操作数地址码的长度</li>
<li>操作数地址个数</li>
</ul>
<p>单字长指令：等于机器字长<br>半字长指令：一半机器字长<br>双字长指令：二倍机器字长<br>定长指令字结构：一个指令系统所有指令的长度都是相等的  </p>
<h4 id="零地址指令：无显示地址"><a href="#零地址指令：无显示地址" class="headerlink" title="零地址指令：无显示地址"></a>零地址指令：无显示地址</h4><table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>OP</td>
</tr>
</tbody></table>
<ul>
<li>不需要操作数的指令</li>
<li>零地址运算指令仅用于堆栈计算机，通常参与运算的两个操作数隐含的从栈顶和次栈顶弹出，送至运算器，运算结果再隐含的压入堆栈</li>
</ul>
<p>一地址指令：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>OP</td>
<td>$A_1$</td>
</tr>
</tbody></table>
<p>OP($A_1$) $\to$ $A_1$</p>
<ul>
<li>只有目的操作数，按$A_1$地址读取操作数，进行OP操作后，结果存回原地址</li>
</ul>
<p>(ACC)OP($A_1$) $\to$ ACC</p>
<ul>
<li>隐含约定目的地址的双操作数指令，按指令地址$A_1$地址读取操作数，指令可隐含约定另一个操作数由ACC提供，运算结果也将存放在ACC中</li>
<li>若指令长度为32位，操作码占8位，1个地址码字段占24位，指令操作数直接寻址范围$2^{24}&#x3D;16M$</li>
</ul>
<h4 id="二地址指令"><a href="#二地址指令" class="headerlink" title="二地址指令"></a>二地址指令</h4><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>OP</td>
<td>$A_1$</td>
<td>$A_2$</td>
</tr>
</tbody></table>
<p>($A_1$)OP($A_2$) $\to$ $A_1$</p>
<ul>
<li>常用的算术和逻辑运算指令，需要两个操作数，需要分别给出目的操作数和源操作数，其中目的操作数地址还用于存放本次运算结果</li>
<li>指令字长位32位，操作码占8位，两个地址码各占12位，则指令操作数的直接寻址范围$2^{12}&#x3D;4K$</li>
</ul>
<h4 id="三地址指令"><a href="#三地址指令" class="headerlink" title="三地址指令"></a>三地址指令</h4><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>OP</td>
<td>$A_1$</td>
<td>$A_2$</td>
<td>$A_3$(结果)</td>
</tr>
</tbody></table>
<p>($A_1$)OP($A_2$) $\to$ $A_3$</p>
<ul>
<li>指令字长位32位，操作码占8位，3个地址码各占8位，直接寻址范围$2^8&#x3D;256$，地址字段为主存地址，则完成一条三地址需要4次访存，取指令1次，取两个操作数2次，存放结果1次</li>
</ul>
<h4 id="四地址指令"><a href="#四地址指令" class="headerlink" title="四地址指令"></a>四地址指令</h4><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>OP</td>
<td>$A_1$</td>
<td>$A_2$</td>
<td>$A_3$</td>
<td>$A_4$</td>
</tr>
</tbody></table>
<p>($A_1$)OP($A_2$) $\to$ $A_3$ ，$A_4$ &#x3D; 下一条执行指令的地址</p>
<ul>
<li>地址字长为32位，操作码占8位，4个地址码各占6位，直接寻址范围$2^6&#x3D;64$</li>
</ul>
<h3 id="定长操作码指令格式"><a href="#定长操作码指令格式" class="headerlink" title="定长操作码指令格式"></a>定长操作码指令格式</h3><p>在指令字的最高位部分分配固定的若干位（定长）表示操作码。<br>n位操作码字段的指令系统最大能表示$2^{n}$条指令</p>
<h3 id="扩展操作码指令格式"><a href="#扩展操作码指令格式" class="headerlink" title="扩展操作码指令格式"></a>扩展操作码指令格式</h3><ul>
<li>不允许短码是长码的前缀</li>
<li>各指令的操作码一定不能重复</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>0000</td>
<td>0001</td>
<td>0010</td>
<td>0011</td>
</tr>
<tr>
<td>0100</td>
<td>0101</td>
<td>0110</td>
<td>0111</td>
</tr>
<tr>
<td>1000</td>
<td>1001</td>
<td>1010</td>
<td>1011</td>
</tr>
<tr>
<td>1100</td>
<td>1101</td>
<td>1110</td>
<td>1111</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>操作码情况</td>
<td>OP</td>
<td>$A_1$</td>
<td>$A_2$</td>
<td>$A_3$</td>
<td>说明</td>
</tr>
<tr>
<td>15条三地址</td>
<td>0000-1110</td>
<td></td>
<td></td>
<td></td>
<td>余出16-15&#x3D;1，1*2^4&#x3D;16种</td>
</tr>
<tr>
<td>12条二地址</td>
<td>1111</td>
<td>0000-1011</td>
<td></td>
<td></td>
<td>余出16-12&#x3D;4，4*2^4&#x3D;64种</td>
</tr>
<tr>
<td>62条一地址</td>
<td>1111</td>
<td>(1100-1110)&#x2F;1111</td>
<td>(0000-1111)&#x2F;(0000-1101)</td>
<td></td>
<td>余出64-62&#x3D;2，2*2^4&#x3D;32种</td>
</tr>
<tr>
<td>32条零地址</td>
<td>1111</td>
<td>1111</td>
<td>1110-1111</td>
<td>0000-1111</td>
<td></td>
</tr>
</tbody></table>
<h3 id="指令的操作类型"><a href="#指令的操作类型" class="headerlink" title="指令的操作类型"></a>指令的操作类型</h3><ul>
<li>数据传送</li>
<li>算术和逻辑运算</li>
<li>移位</li>
<li>转移</li>
<li>输入输出</li>
</ul>
<h2 id="指令寻址方式"><a href="#指令寻址方式" class="headerlink" title="指令寻址方式"></a>指令寻址方式</h2><p>确定本条指令的数据地址以及下一条待执行指令的地址，分为：</p>
<ul>
<li><p>指令寻址：寻找下条要执行的指令<br>（1）顺序寻址<br>通过PC+(1)，自动形成下一条指令<br>（2）跳跃寻址<br>通过转移指令实现，下条指令的地址不由PC自动给出，而由本条指令给出下条指令地址的计算方式。是否可跳跃受到状态寄存器和操作数的控制，跳跃的结果是当前指令修改PC值，下一条指令仍通过PC给出</p>
</li>
<li><p>数据寻址：寻找本条指令的数据<br>通常在指令字中设一个字段指明寻址方式</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>操作码</td>
<td>寻址特征</td>
<td>形式地址A</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="常见数据寻址方式"><a href="#常见数据寻址方式" class="headerlink" title="常见数据寻址方式"></a>常见数据寻址方式</h3><h4 id="隐含寻址"><a href="#隐含寻址" class="headerlink" title="隐含寻址"></a>隐含寻址</h4><p>不明显的给出操作数，在指令中隐含操作数地址</p>
<ul>
<li>优：有利于缩短指令字长</li>
<li>缺：需要增加存储操作数或隐含地址的硬件</li>
</ul>
<h4 id="立即（数）寻址"><a href="#立即（数）寻址" class="headerlink" title="立即（数）寻址"></a>立即（数）寻址</h4><p>指令的地址字段指出的不是操作数地址，而是操作数本身，又称立即数，#表示立即寻址特征，使用补码表示</p>
<ul>
<li>优：指令在执行阶段不访问主存，指令执行时间最短</li>
<li>缺：A的位数限制立即数的范围</li>
</ul>
<h4 id="直接寻址"><a href="#直接寻址" class="headerlink" title="直接寻址"></a>直接寻址</h4><p>指令中的形式地址A是操作数的真实地址EA，EA&#x3D;A  </p>
<ul>
<li>优：简单，访存1次，不需要专门计算操作数的地址</li>
<li>缺：A的位数决定了指令操作数的寻址范围，操作数的地址不易修改</li>
</ul>
<h4 id="间接寻址"><a href="#间接寻址" class="headerlink" title="间接寻址"></a>间接寻址</h4><p>指令的地址字段给出的形式地址不是操作数的真正地址，而是操作数有效地址的存储单元地址，EA&#x3D;(A)，间接寻址可以迭代多次<br>间接寻址，主存第一位表示是否为多次间址</p>
<ul>
<li>优：可扩大寻址范围（有效地址EA的位数大于形式地址A的位数），便于编制程序（用间址寻址可方便的完成子程序返回）</li>
<li>缺：访问速度慢</li>
</ul>
<h4 id="寄存器寻址"><a href="#寄存器寻址" class="headerlink" title="寄存器寻址"></a>寄存器寻址</h4><p>指令字中直接给出操作数所在的寄存器编号EA&#x3D; $R_i$，操作数在由$R_i$所指的寄存器内</p>
<ul>
<li>优：指令执行阶段不访存，只访问寄存器，寄存器对应地址码长度较小，使得指令字短且因不用访存，所以执行速度快，支持向量&#x2F;矩阵运算</li>
<li>缺:寄存器昂贵，有限</li>
</ul>
<h4 id="寄存器间接寻址"><a href="#寄存器间接寻址" class="headerlink" title="寄存器间接寻址"></a>寄存器间接寻址</h4><p>寄存器$R_i$中给出的不是一个操作数，而是操作数所在主存单元的地址EA&#x3D;($R_i$)</p>
<ul>
<li>优：与一般间址寻址速度快</li>
<li>缺：需要访存</li>
</ul>
<h4 id="相对寻址"><a href="#相对寻址" class="headerlink" title="相对寻址"></a>相对寻址</h4><p>PC的内容加上指令格式的形式地址A而形成操作数的有效地址EA&#x3D;(PC)+A，A是相对于当前PC的值的位移量，可正可负，用补码表示，A的位数决定寻址范围</p>
<ul>
<li>操作数的地址是不固定的，随PC的值变化而变化，且与指令地址之间相差一个固定值，便于程序浮动，广泛用于转移指令</li>
<li>JMP A，CPU从存储器取出一字节，自动执行(PC)+1 $\to$ PC，若转移指令的地址为X，且占2B，取出该指令后，PC自增2，(PC)&#x3D;X+2，执行完该指令，会自动跳转至X+2+A的地址继续执行</li>
</ul>
<h4 id="基址寻址"><a href="#基址寻址" class="headerlink" title="基址寻址"></a>基址寻址</h4><p>将CPU的基址寄存器BR的内容加上指令格式的形式地址A形成操作数的有效地址EA&#x3D;(BR)+A，基址寄存器可采用专用寄存器也可为通用寄存器</p>
<ul>
<li><p>基址寄存器面向操作系统，内容通过操作系统或管理程序确定，主要用于解决程序逻辑空间与存储器物理空间的无关性</p>
</li>
<li><p>执行过程中基址寄存器内容不变，形式地址可变（偏移量）</p>
</li>
<li><p>采用通用寄存器作为基址寄存器，用户可决定使用哪个寄存器，内容由操作系统确定</p>
</li>
<li><p>优：可扩大寻址范围（基址寄存器位数大于形式地址A的位数），用户不必考虑自己的程序存于主存哪个区域，有利于多道程序设计，可用于制成浮动程序</p>
</li>
<li><p>缺：偏移量位数较短</p>
</li>
</ul>
<h4 id="变址寻址"><a href="#变址寻址" class="headerlink" title="变址寻址"></a>变址寻址</h4><p>有效地址EA等于指令字中的形式地址A与变址寄存器IX的内容之和，EA&#x3D;(IX)+A</p>
<ul>
<li>IX可使用专用寄存器或通用寄存器</li>
<li>变址寄存器面向用户，在程序执行过程，变址寄存器内容可由用户改变（作为偏移量），形式地址A不变（作为基地址）</li>
<li>可扩大寻址范围（变址寄存器位数大于形式地址A的位数），适合编制循环程序，偏移量的位数（IX）足以表示整个存储空间</li>
</ul>
<h4 id="堆栈寻址"><a href="#堆栈寻址" class="headerlink" title="堆栈寻址"></a>堆栈寻址</h4><p>堆栈是存储器（或专用寄存器组）中一块特定的、按照后进先出（LIFO）的原则管理的存储区，存储区读写单元地址是用一个特定寄存器给出的称为堆栈指针(SP)，分为硬堆栈（不适合做大容量堆栈）和软堆栈（主存划出一段区域）</p>
<table>
<thead>
<tr>
<th>寻址方式</th>
<th>有效地址</th>
<th>访存次数</th>
</tr>
</thead>
<tbody><tr>
<td>隐含寻址</td>
<td>程序指定</td>
<td>0</td>
</tr>
<tr>
<td>立即寻址</td>
<td>A是操作数</td>
<td>0</td>
</tr>
<tr>
<td>直接寻址</td>
<td>EA&#x3D;A</td>
<td>1</td>
</tr>
<tr>
<td>一次间接寻址</td>
<td>EA&#x3D;(A)</td>
<td>2</td>
</tr>
<tr>
<td>寄存器寻址</td>
<td>EA&#x3D; $R_i$</td>
<td>0</td>
</tr>
<tr>
<td>寄存器间接一次寻址</td>
<td>EA &#x3D; ($R_i$)</td>
<td>1</td>
</tr>
<tr>
<td>相对寻址</td>
<td>EA&#x3D;(PC)+A</td>
<td>1</td>
</tr>
<tr>
<td>基址寻址</td>
<td>EA&#x3D;(BR)+A</td>
<td>1</td>
</tr>
<tr>
<td>变址寻址</td>
<td>EA&#x3D;(IX)+A</td>
<td>1</td>
</tr>
</tbody></table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/%E6%8C%87%E4%BB%A4%E7%B3%BB%E7%BB%9F/" data-id="clkqazu8i001ddlbi6mkhhqw6" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-YOLO_001_from-CNN-to-YOLOv1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/YOLO_001_from-CNN-to-YOLOv1/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn1.png" alt="cnn1"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn2%20(2).png" alt="cnn2 (2)"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn3.png" alt="cnn3"></p>
<p><strong>分类猫和狗</strong></p>
<p>使用一个还不错的相机采集图片(12M)   </p>
<p>RGB figure 36M 元素  </p>
<p>使用100大小的单隐藏层MLP 模型有3.6B &#x3D; 14GB 元素   </p>
<p>远多于世界上所有的猫狗总数(900M dog 600M cat)  </p>
<p><strong>两个原则</strong></p>
<p>平移不变性  </p>
<p>局部性  </p>
<p><strong>重新考察全连接层</strong>  </p>
<p>将输入和输出变形为矩阵（宽度，高度）</p>
<p>将权重变形为4-D张量（h,w）到（h’,w’）<br>$$<br>h_{i,j}&#x3D;\sum_{k,l}w_{i,j,k,l}x_{k,l}&#x3D;\sum_{a,b}&#x3D;v_{i,j,a,b}x_{i+a,j+b}<br>$$<br>V是W的重新索引<br>$$<br>v_{i,j,a,b}&#x3D;w_{i,j,i+a,j+b}<br>$$</p>
<p><strong>原则#1 - 平移不变性</strong></p>
<p>x的平移导致h的平移<br>$$<br>h_{i,j}&#x3D;\sum_{a,b}v_{i,j,a,b}x_{i+a,j+b}<br>$$<br>v不应依赖于（i, j）  </p>
<p>解决方案：<br>$$<br>v_{i,j,a,b}&#x3D;v_{a, b},<br>h_{i,j}&#x3D;\sum_{a,b}v_{a,b}x_{i+a,j+b}<br>$$<br>这就是交叉相关  </p>
<p><strong>原则#2 - 局部性</strong></p>
<h3 id="局部性"><a href="#局部性" class="headerlink" title="局部性"></a>局部性</h3><p>$$<br>\begin{aligned}<br>&amp;为了收集用来训练参数[\mathbf{H}]<em>{i, j}的相关信息，\<br>&amp;我们不应偏离到距(i, j)很远的地方。\<br>&amp;这意味着在|a|&gt; \Delta或|b| &gt; \Delta的范围之外，\<br>&amp;我们可以设置[\mathbf{V}]</em>{a, b} &#x3D; 0。\<br>&amp;因此，我们可以将[\mathbf{H}]<em>{i, j}重写为:\<br>&amp;[\mathbf{H}]*</em>{i, j} &#x3D; u + \sum_*{a &#x3D; -\Delta}^{\Delta} \sum*_{b &#x3D; -\Delta}^{\Delta} [\mathbf{V}]<em>*{a, b} [\mathbf{X}]</em>{i+a, j+b}.<br>\end{aligned}<br>$$<br>当图像处理的局部区域很小时，卷积神经网络与多层感知机的训练差异可能是巨大的：以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。</p>
<p>参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。</p>
<p>以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。</p>
<p>但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn4.png" alt="cnn4"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn5.png" alt="image-20220127104222384"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn6.png" alt="cnn6"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn7.png" alt="cnn7"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn8.png" alt="cnn8"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn9.png" alt="cnn9"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20220127105601246.png" alt="image-20220127105601246"></p>
<h2 id="Sharing-Weight"><a href="#Sharing-Weight" class="headerlink" title="Sharing-Weight"></a>Sharing-Weight</h2><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn11.png" alt="cnn11"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20220127110147649.png" alt="image-20220127110147649"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn12.png" alt="cnn12"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn13.png" alt="cnn13"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn14.png" alt="cnn14"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn15.png" alt="cnn15"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn16.png" alt="cnn16"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn17.png" alt="cnn17"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn18.png" alt="cnn18"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn19.png" alt="cnn19"></p>
<h2 id="Pooling-Max-Pooling"><a href="#Pooling-Max-Pooling" class="headerlink" title="Pooling - Max Pooling"></a>Pooling - Max Pooling</h2><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn20.png" alt="cnn20"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn20.1.png" alt="cnn20.1"></p>
<p><strong>Max-Pooling:选取最大的值 也可选取其他的采用 当然也可不做采用前提是性能足够</strong></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn21.png" alt="cnn21"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn22.png" alt="cnn22"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/cnn23.png" alt="cnn23"></p>
<p><strong>但CNN无法直接对一个放大的图像做识别，需要data augmentation(对数据集进行旋转，放大，缩小，等操作)</strong></p>
<h1 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h1><h2 id="Bounding-Box"><a href="#Bounding-Box" class="headerlink" title="Bounding-Box"></a>Bounding-Box</h2><p>将一张图片分割为有限个单元格(Cell,图中红色网格)<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/split-image.png" alt="split-pic"><br>每一个输出和标签都是针对每一个单元格的物体中心(midpiont,图中蓝色圆点)<br>每一个单元格会有[X1, Y1, X2, Y2]<br>对应的物体中心会有一个[X, Y, W, H]<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/bounding-box1.png" alt="bb1"><br>X, Y 在[0, 1]内表示水平或垂直的距离<br>W, H &gt; 1 表示物体水平或垂直方向上高于该单元格 数值表示水平或垂直方向的单位长度的倍数<br>[0.95, 0.55, 0.5, 1.5]&#x3D;&gt;显然图像靠近右下角 单元格不能表示出完整的物体<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/bounding-box2.png" alt="bb2"><br>根据 [X, Y, W, H] &#x3D;&gt; [0.95, 0.55, 0.5, 1.5] 计算得到Bounding Box(图中蓝色网格)</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/b-box-seq.png" alt="bbx3"></p>
<h2 id="Image-Label"><a href="#Image-Label" class="headerlink" title="Image-Label"></a>Image-Label</h2><p>$$<br>\begin{aligned}<br>&amp;label_{cell}&#x3D;[C_1,C_2,\cdots,C_{20},P_c,X,Y,W,H]\<br>&amp;[C_1,C_2,\cdots,C_{20}]:20\space different\space classes\<br>&amp;[P_c]:Probability\space for\space there\space is\space an\space object(0or1)\<br>&amp;[X,Y,W,H]:Bounding-Box\<br>&amp;pred_{cell}&#x3D;[C_1,C_2,\cdots,C_{20},P_{c1},X_1,Y_1,W_1,H_1,P_{c2},X_2,Y_2,W_2,H_2]\<br>&amp;Taget\space shape\space for\space one \space images:(S, S, 25)\<br>&amp;Predication\space shape \space for\space one\space images:(S,S,30)\<br>\end{aligned}<br>$$</p>
<h2 id="Model-Framework"><a href="#Model-Framework" class="headerlink" title="Model-Framework"></a>Model-Framework</h2><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/yolov1-modelfw.png" alt="yolov1"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/YOLO_001_from-CNN-to-YOLOv1/" data-id="clkqazu8j001edlbi348vercv" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2022-06-15-trafficReg" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/2022-06-15-trafficReg/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD<br>原文链接：<a target="_blank" rel="noopener" href="https://medium.com/geoai/an-end-to-end-solution-on-the-cloud-to-monitor-traffic-flow-using-deep-learning-9dfdfd00b621">https://medium.com/geoai/an-end-to-end-solution-on-the-cloud-to-monitor-traffic-flow-using-deep-learning-9dfdfd00b621</a></p>
<p>本文将大致介绍如何结合监控视频流，ArcGIS，ArcGIS API for API，AWS等技术来监测车流量。</p>
<p>目录：</p>
<p>交通治理以及研究问题描述<br>实时视频流以及数据标注<br>目标检测：在AWS上训练YOLO3模型<br>流程架构<br>使用Dashboard应用实时监测路况<br>基于历史数据的异常行为监测<br>结论以及展望<br>致谢以及参考文献<br>交通治理以及问题研究描述<br>车流量是监测城市环境状态的一个重要要素。控制道路上车流量是一个非常基本的需求。在一些大城市，通常使用监控相机监测繁忙的道路，高速公路，以及十字路口。交通局工作人员通常对事故，路面覆盖物（雨，冰，雪），路面犯罪，抛锚，超速，拥堵，行人数量等信息感兴趣。监控相机可以帮助更好治理路况维持公共安全。国家高速公路安全管理局的一项研究表明，36%的碰撞事故都发生在道路交汇处。因此，十字路口时城市交通拥堵的罪魁祸首，也是交管中心重点监测对象。为了监测和管理路况，交通十字路口通常安装了许多相机。监测相机可以时固定，也可以是可遥控的PTZ相机。</p>
<p>监控抓拍图-昼</p>
<p>监控抓拍图-夜<br>于是，华盛顿区域的交通部门需要Esri定制一个云解决方案，方案需要满足以下需求：1）监测110个交通路口的路况（小汽车，公交，卡车，摩托车，行人），并且使用GIS将其可视化。2）监测路口流量异常。3）监测处在危险区域的行人。这个解决方案不仅需要监控相机，还需要将空间数据和深度学习框架结合。</p>
<p>本文将介绍，如何使用ArcGIS，ArcGIS API for Pyhon，AWS以及Keras深度学习框架实现这个解决方案。解决方案是使用AWS环境中的GPU来加速实时处理视频流，从而进行模型训练和推断预测。ArcGIS API for Python将空间信息如视频流的位置与深度学习框架结合，并且使用ArcGIS Enterprise将时间信息一同保存。</p>
<p>实时视频流以及数据标注<br>深度学习模型需要大量的训练数据。作者通过Traffic Land的REST API服务获取华盛顿111个路口的实视监控视频。作者使用Python代码，从TrafficLand服务上面获取了这111个监控相机的1000多张日夜抓拍图。作者将这些训练数据图片放到一个文件夹里面，然后使用LabelImg的工具人工标注图片中的目标物。最后将标注信息导出为txt文件，txt文件可以被绝大多数的目标检测算法使用。</p>
<p>使用LabelImg软件标注<br>目标检测：在AWS上训练YOLO3模型<br>我们的目的是从实时的视频中识别目标物。YOLO是一个目标检测很火的算法，该算法在实时应用中的精准度十分的高。YOLO可以生成目标物在图片中的位置，并且告诉用户该目标物的类别。YOLO只需要在网络中进行一次前向衍生就能够提供预测。早些版本的YOLO如YOLO2无法识别细小的目标物，因为YOLO2的计算层降低了输入图片的分辨率。除此之外，YOLO2还缺少一些牛叉的技术，如residual blocks，skip connections 以及 upsampling。YOLO3增加计算层以及YOLO2中那些没有的牛叉的功能。YOLO3算法在模型里面3个不同的位置，对尺寸不同的要素图运用1*1的识别窗口来实现目标检测。关于YOLO的原理有很多的博客和资源，这里不不做赘述。你可以在这些参考文献里了解YOLO的原理。</p>
<p>YOLO模型<br>来自111个监控相机的1000多张带有标签的日夜抓拍图将被用作训练数据，在AWS上面训练YOLO3模型。笔者使用了AWS的EC2实例，EC2实例提供专门用来深度学习的镜像，这些镜像通常预装自带了Tensorflow，PyTorch，Keras等框架，可以用于训练复杂的深度学习模型。笔者使用了预训练的YOLO3模型以及转移学习技术。随后作者对比了预测结果和实际结果。训练模型的IOU达95%。笔者使用了现成的开源Github代码训练YOLO3 模型。</p>
<p>流程架构<br>为了在AWS上面搭建一个实时流程，我们使用了如下架构来实现路况监测：1）我们使用并行处理来加速从TrafficLand REST API取视频流程的过程。2）紧接着，抓拍图被传到AWS EC2实例上的YOLO3模型里面。YOLO3对每一个片中的目标识别并且分类。总体上使用一个NVIDIA Tesla K80 GPU，我们可以在10内完成111张图片的抓取以及预测。3）最后我们将YOLO3的预测结果传到AWS的GeoEvent中的大数据库中，从而可以在大屏幕上对每一类数据进行可视化。每一个监控相机的相关照片都保存在S3 bucket云存储中。</p>
<p>流程架构设计图<br>我们的IT团队在AWS配置好了深度学习EC2实例以及ArcGIS GeoEvent Server。GeoEvent Server将实时的流数据与带有位置数据的要素类或大数据库相结合。为了将GeoEvent Server和EC2实例上的YOLO3深度学习模型相连接，笔者在GeoEvent服务中配置了输入连接器，处理器，输出连接器。GeoEvent服务可以通过用户图形界面创建，类似Model Builder的创建方法。</p>
<p>GeoEvent输入连接器定义了来自YOLO3模型的事件数据结构，并且把事件数据传送给GeoEvent处理器。如果你的数据结构有差异，GeoEvent将无法读取事件数据。GeoEvent中有好几种常用格式（文本，RSS，ESRI要素JSON，JSON）和协议（系统文件，HTTP，TCP，UDP，WebSocket，ESRI要素服务）的输入连接器。建立输入连接器是，用户要新建一个GeoEvent Definition，GeoEvent Definition里面定义了事件数据的数据结构。下图分别展示了GeoJSON格式的GeoEvent Definition，和RestAPI的数据通信渠道。因此，每一个相机的YOLO3模型的输出结果都会使用相机位置信息转换成GeoJSON。</p>
<p>GeoEvent Definition</p>
<p>GeoEvent Input Connector<br>GeoEvent处理器是GeoEvent服务里面的一个可配置元素。GeoEvent服务提供基于事件数据的分析，比如对事件数据进行识别，对事件数据进行扩充。由于我们的流程没有对事件数据做任何处理，因此我们的流程中将不会使用任何GeoEvent处理器。</p>
<p>GeoEvent输出连接器的作用是将GeoEvent数据重新转成符合各种协议的流数据。我们配置了两个GeoEvent服务：1）一个实时GeoEvent服务，用于获取实时数据以及大屏可视化。2）一个历史GeoEvent服务，用于保存历史数据要素类可以后续用于异常分析。这两个服务的不同之处在于，实时服务只保存最新的111条记录，然而历史服务会保存之前所有生成的记录。我们可以算一下按每秒111条记录的速率，一天数据量将达到9.6百万（111相机<em>24小时</em>60分*60秒）。</p>
<p>Update a Feature Ouput Connector</p>
<p>Add a Feature Output Connector<br>使用Dashboard应用实时监测路况<br>我们使用了Dashboard应用来实现自动化监测实时交通路况。Dashboard应用展示了111个实时视频流的位置，以及每个路口各种车型的计数。Dashboard的数据来自GeoEvent服务生成的要素类。用户通过Dashboard可以判断华盛顿区域行人或者车辆拥堵的具体位置。用户要可以看到每个监控相机的实时画面。下图展示了Dashboard应用的界面。Dashboard根据用户当前浏览的地图区域更新左侧的统计数据。</p>
<p>Dashboard应用</p>
<p>查询某一个路口<br>基于历史数据的异常行为监测<br>交通部门还想知道每一种车型和行人在路口的动态流量是如何的。为了解决这个问题，我们在一个礼拜后使用历史GeoEvent服务生成的数据来计算每一种车型和行人每天每一分钟的流量状况。我们简单计算一下，会有约1百万种可能的组合（111相机<em>7天</em>24小时*60分钟）。你可以把它比作一个异常监测的查询表。我们将每一种车型的计数与历史技术做比较，如果计数高于历史计数30%，那么我们称为流量异常，并且将异常展现在地图上。我们将异常事件写进一个单独的要素类里面。下图展示了某个路口的行人和车辆的异常状况。</p>
<p>交通部门还特别在意行人路口行为。他们主要想知道是否有行人不使用人行道。解决这个问题有很多方法。一半方法是检测处图像中的人行道，然后将人行道范围外的行人标为异常。</p>
<p>某路口异常行为<br>笔者使用了另外一种方法。笔者找了一个路口连续运行YOLO3识别目标物五个小时，然后将所有行人类别的图片坐标（矩形框四个角的像素坐标）提取出，计算每一个矩形框右下，左下像素坐标的平均值，然后将每一个矩形框转换成一个像素点，我们之所以使用左下，右下的坐标，是因为这两个坐标更加贴近地面，可以更好的代表地面上的人行道。下图中每一个红色点代表这个小时内所有行人的位置。这份数据可以揭示行人过马路的规律。由于大多数人都使用人行道过马路，图中可以看到红点都聚集在人行道附近。反之，图片中的其他位置没有红点。</p>
<p>原始路口抓拍图</p>
<p>红点代表行人的历史位置<br>为了将图像中高密度和低密度的红点分开，笔者使用了DBSCAN分析算法。DBSCAN更具点的空间分布以及每个点周围的噪声情况来识别高密度点聚类。DBSCAN还会将一些距离点聚类区域较远的点标为outlier。下图就是使用DBSCAN标记出了不在人行道区域的行人。</p>
<p>未使用人行道的行人 1</p>
<p>未使用人行道的行人 2<br>结论以及展望<br>本文，笔者介绍了GeoAI团队开发的交通路况监测云解决方案。该解决方案可以1）访问获取实时视频数据，2）使用AWS上的YOLO3模型实时识别小汽车，巴士，卡车，摩托车，以及行人，3）将YOLO3的结果发送到AWS上面的GeoEvent服务，在Dashboard应用上会展示路况，并且使用大数据库中的历史数据进行分析，4）根据目标物的数量来分析异常事件，例如监测处于危险位置的行人。GIS在展示相机地理位置以及实时路况起到了关键的作用。未来我们还会基于此方案进行目标追踪，测速，统计车道流量等。</p>
<p>致谢以及参考文献<br>感谢Daniel Wilson配置AWS以及S3图片云存储，让整个流程快了10倍。感谢Joel McCune配置的Dashboard应用。感谢RJ Sunderman 在ArcGIS GeoEvent Server上的帮助，感谢Alberto Nieto联系交通部门启动了这个项目，使得我们可以将YOLO3，ArcGIS GeoEvent Server，AWS添加到Alberto之前的成果中去，实现实时云处理。最后感谢Mark Carlson配置了ArcGIS GeoEvent Server，以及AWS的深度学习镜像。我们还将这个解决方案复制到了Azure上面。如果你有疑问，或者有意向合作，欢迎联系我们。</p>
<h1 id="1-https-developers-arcgis-com-python2-https-aws-amazon-com-machine-learning-amis3-http-www-arcgis-com-index-html4-http-www-trafficland-com5-https-github-com-tzutalin-labelImg6-https-arxiv-org-abs-1506-02640https-arxiv-org-abs-1612-08242https-arxiv-org-abs-1804-02767https-lilianweng-github-io-lil-log-2018-12-27-object-detection-part-4-html-yolo-you-only-look-oncehttps-towardsdatascience-com-yolo-v3-object-detection-53fb7d3bfe6b7-https-github-com-qqwweee-keras-yolo38-https-enterprise-arcgis-com-en-geoevent-latest-get-started-a-quick-tour-of-geoevent-server-htm9-https-enterprise-arcgis-com-en-geoevent-latest-administer-managing-big-data-stores-htm10-http-desktop-arcgis-com-en-arcmap-10-3-analyze-modelbuilder-what-is-modelbuilder-htm11-https-www-esri-com-en-us-arcgis-products-operations-dashboard-overview"><a href="#1-https-developers-arcgis-com-python2-https-aws-amazon-com-machine-learning-amis3-http-www-arcgis-com-index-html4-http-www-trafficland-com5-https-github-com-tzutalin-labelImg6-https-arxiv-org-abs-1506-02640https-arxiv-org-abs-1612-08242https-arxiv-org-abs-1804-02767https-lilianweng-github-io-lil-log-2018-12-27-object-detection-part-4-html-yolo-you-only-look-oncehttps-towardsdatascience-com-yolo-v3-object-detection-53fb7d3bfe6b7-https-github-com-qqwweee-keras-yolo38-https-enterprise-arcgis-com-en-geoevent-latest-get-started-a-quick-tour-of-geoevent-server-htm9-https-enterprise-arcgis-com-en-geoevent-latest-administer-managing-big-data-stores-htm10-http-desktop-arcgis-com-en-arcmap-10-3-analyze-modelbuilder-what-is-modelbuilder-htm11-https-www-esri-com-en-us-arcgis-products-operations-dashboard-overview" class="headerlink" title="1] https://developers.arcgis.com/python2] https://aws.amazon.com/machine-learning/amis3] http://www.arcgis.com/index.html4] http://www.trafficland.com5] https://github.com/tzutalin/labelImg6] https://arxiv.org/abs/1506.02640https://arxiv.org/abs/1612.08242https://arxiv.org/abs/1804.02767https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html#yolo-you-only-look-oncehttps://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b7] https://github.com/qqwweee/keras-yolo38] https://enterprise.arcgis.com/en/geoevent/latest/get-started/a-quick-tour-of-geoevent-server.htm9] https://enterprise.arcgis.com/en/geoevent/latest/administer/managing-big-data-stores.htm10] http://desktop.arcgis.com/en/arcmap/10.3/analyze/modelbuilder/what-is-modelbuilder.htm11] https://www.esri.com/en-us/arcgis/products/operations-dashboard/overview"></a>1] <a target="_blank" rel="noopener" href="https://developers.arcgis.com/python">https://developers.arcgis.com/python</a><br>2] <a target="_blank" rel="noopener" href="https://aws.amazon.com/machine-learning/amis">https://aws.amazon.com/machine-learning/amis</a><br>3] <a target="_blank" rel="noopener" href="http://www.arcgis.com/index.html">http://www.arcgis.com/index.html</a><br>4] <a target="_blank" rel="noopener" href="http://www.trafficland.com/">http://www.trafficland.com</a><br>5] <a target="_blank" rel="noopener" href="https://github.com/tzutalin/labelImg">https://github.com/tzutalin/labelImg</a><br>6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.02640https://arxiv.org/abs/1612.08242https://arxiv.org/abs/1804.02767https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html#yolo-you-only-look-oncehttps://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b">https://arxiv.org/abs/1506.02640https://arxiv.org/abs/1612.08242https://arxiv.org/abs/1804.02767https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html#yolo-you-only-look-oncehttps://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b</a><br>7] <a target="_blank" rel="noopener" href="https://github.com/qqwweee/keras-yolo3">https://github.com/qqwweee/keras-yolo3</a><br>8] <a target="_blank" rel="noopener" href="https://enterprise.arcgis.com/en/geoevent/latest/get-started/a-quick-tour-of-geoevent-server.htm">https://enterprise.arcgis.com/en/geoevent/latest/get-started/a-quick-tour-of-geoevent-server.htm</a><br>9] <a target="_blank" rel="noopener" href="https://enterprise.arcgis.com/en/geoevent/latest/administer/managing-big-data-stores.htm">https://enterprise.arcgis.com/en/geoevent/latest/administer/managing-big-data-stores.htm</a><br>10] <a target="_blank" rel="noopener" href="http://desktop.arcgis.com/en/arcmap/10.3/analyze/modelbuilder/what-is-modelbuilder.htm">http://desktop.arcgis.com/en/arcmap/10.3/analyze/modelbuilder/what-is-modelbuilder.htm</a><br>11] <a target="_blank" rel="noopener" href="https://www.esri.com/en-us/arcgis/products/operations-dashboard/overview">https://www.esri.com/en-us/arcgis/products/operations-dashboard/overview</a></h1><p>原文链接：<a target="_blank" rel="noopener" href="https://medium.com/geoai/an-end-to-end-solution-on-the-cloud-to-monitor-traffic-flow-using-deep-learning-9dfdfd00b621">https://medium.com/geoai/an-end-to-end-solution-on-the-cloud-to-monitor-traffic-flow-using-deep-learning-9dfdfd00b621</a></p>
<p>本文将大致介绍如何结合监控视频流，ArcGIS，ArcGIS API for API，AWS等技术来监测车流量。</p>
<p>目录：</p>
<p>交通治理以及研究问题描述<br>实时视频流以及数据标注<br>目标检测：在AWS上训练YOLO3模型<br>流程架构<br>使用Dashboard应用实时监测路况<br>基于历史数据的异常行为监测<br>结论以及展望<br>致谢以及参考文献<br>交通治理以及问题研究描述<br>车流量是监测城市环境状态的一个重要要素。控制道路上车流量是一个非常基本的需求。在一些大城市，通常使用监控相机监测繁忙的道路，高速公路，以及十字路口。交通局工作人员通常对事故，路面覆盖物（雨，冰，雪），路面犯罪，抛锚，超速，拥堵，行人数量等信息感兴趣。监控相机可以帮助更好治理路况维持公共安全。国家高速公路安全管理局的一项研究表明，36%的碰撞事故都发生在道路交汇处。因此，十字路口时城市交通拥堵的罪魁祸首，也是交管中心重点监测对象。为了监测和管理路况，交通十字路口通常安装了许多相机。监测相机可以时固定，也可以是可遥控的PTZ相机。</p>
<p>监控抓拍图-昼</p>
<p>监控抓拍图-夜<br>于是，华盛顿区域的交通部门需要Esri定制一个云解决方案，方案需要满足以下需求：1）监测110个交通路口的路况（小汽车，公交，卡车，摩托车，行人），并且使用GIS将其可视化。2）监测路口流量异常。3）监测处在危险区域的行人。这个解决方案不仅需要监控相机，还需要将空间数据和深度学习框架结合。</p>
<p>本文将介绍，如何使用ArcGIS，ArcGIS API for Pyhon，AWS以及Keras深度学习框架实现这个解决方案。解决方案是使用AWS环境中的GPU来加速实时处理视频流，从而进行模型训练和推断预测。ArcGIS API for Python将空间信息如视频流的位置与深度学习框架结合，并且使用ArcGIS Enterprise将时间信息一同保存。</p>
<p>实时视频流以及数据标注<br>深度学习模型需要大量的训练数据。作者通过Traffic Land的REST API服务获取华盛顿111个路口的实视监控视频。作者使用Python代码，从TrafficLand服务上面获取了这111个监控相机的1000多张日夜抓拍图。作者将这些训练数据图片放到一个文件夹里面，然后使用LabelImg的工具人工标注图片中的目标物。最后将标注信息导出为txt文件，txt文件可以被绝大多数的目标检测算法使用。</p>
<p>使用LabelImg软件标注<br>目标检测：在AWS上训练YOLO3模型<br>我们的目的是从实时的视频中识别目标物。YOLO是一个目标检测很火的算法，该算法在实时应用中的精准度十分的高。YOLO可以生成目标物在图片中的位置，并且告诉用户该目标物的类别。YOLO只需要在网络中进行一次前向衍生就能够提供预测。早些版本的YOLO如YOLO2无法识别细小的目标物，因为YOLO2的计算层降低了输入图片的分辨率。除此之外，YOLO2还缺少一些牛叉的技术，如residual blocks，skip connections 以及 upsampling。YOLO3增加计算层以及YOLO2中那些没有的牛叉的功能。YOLO3算法在模型里面3个不同的位置，对尺寸不同的要素图运用1*1的识别窗口来实现目标检测。关于YOLO的原理有很多的博客和资源，这里不不做赘述。你可以在这些参考文献里了解YOLO的原理。</p>
<p>YOLO模型<br>来自111个监控相机的1000多张带有标签的日夜抓拍图将被用作训练数据，在AWS上面训练YOLO3模型。笔者使用了AWS的EC2实例，EC2实例提供专门用来深度学习的镜像，这些镜像通常预装自带了Tensorflow，PyTorch，Keras等框架，可以用于训练复杂的深度学习模型。笔者使用了预训练的YOLO3模型以及转移学习技术。随后作者对比了预测结果和实际结果。训练模型的IOU达95%。笔者使用了现成的开源Github代码训练YOLO3 模型。</p>
<p>流程架构<br>为了在AWS上面搭建一个实时流程，我们使用了如下架构来实现路况监测：1）我们使用并行处理来加速从TrafficLand REST API取视频流程的过程。2）紧接着，抓拍图被传到AWS EC2实例上的YOLO3模型里面。YOLO3对每一个片中的目标识别并且分类。总体上使用一个NVIDIA Tesla K80 GPU，我们可以在10内完成111张图片的抓取以及预测。3）最后我们将YOLO3的预测结果传到AWS的GeoEvent中的大数据库中，从而可以在大屏幕上对每一类数据进行可视化。每一个监控相机的相关照片都保存在S3 bucket云存储中。</p>
<p>流程架构设计图<br>我们的IT团队在AWS配置好了深度学习EC2实例以及ArcGIS GeoEvent Server。GeoEvent Server将实时的流数据与带有位置数据的要素类或大数据库相结合。为了将GeoEvent Server和EC2实例上的YOLO3深度学习模型相连接，笔者在GeoEvent服务中配置了输入连接器，处理器，输出连接器。GeoEvent服务可以通过用户图形界面创建，类似Model Builder的创建方法。</p>
<p>GeoEvent输入连接器定义了来自YOLO3模型的事件数据结构，并且把事件数据传送给GeoEvent处理器。如果你的数据结构有差异，GeoEvent将无法读取事件数据。GeoEvent中有好几种常用格式（文本，RSS，ESRI要素JSON，JSON）和协议（系统文件，HTTP，TCP，UDP，WebSocket，ESRI要素服务）的输入连接器。建立输入连接器是，用户要新建一个GeoEvent Definition，GeoEvent Definition里面定义了事件数据的数据结构。下图分别展示了GeoJSON格式的GeoEvent Definition，和RestAPI的数据通信渠道。因此，每一个相机的YOLO3模型的输出结果都会使用相机位置信息转换成GeoJSON。</p>
<p>GeoEvent Definition</p>
<p>GeoEvent Input Connector<br>GeoEvent处理器是GeoEvent服务里面的一个可配置元素。GeoEvent服务提供基于事件数据的分析，比如对事件数据进行识别，对事件数据进行扩充。由于我们的流程没有对事件数据做任何处理，因此我们的流程中将不会使用任何GeoEvent处理器。</p>
<p>GeoEvent输出连接器的作用是将GeoEvent数据重新转成符合各种协议的流数据。我们配置了两个GeoEvent服务：1）一个实时GeoEvent服务，用于获取实时数据以及大屏可视化。2）一个历史GeoEvent服务，用于保存历史数据要素类可以后续用于异常分析。这两个服务的不同之处在于，实时服务只保存最新的111条记录，然而历史服务会保存之前所有生成的记录。我们可以算一下按每秒111条记录的速率，一天数据量将达到9.6百万（111相机<em>24小时</em>60分*60秒）。</p>
<p>Update a Feature Ouput Connector</p>
<p>Add a Feature Output Connector<br>使用Dashboard应用实时监测路况<br>我们使用了Dashboard应用来实现自动化监测实时交通路况。Dashboard应用展示了111个实时视频流的位置，以及每个路口各种车型的计数。Dashboard的数据来自GeoEvent服务生成的要素类。用户通过Dashboard可以判断华盛顿区域行人或者车辆拥堵的具体位置。用户要可以看到每个监控相机的实时画面。下图展示了Dashboard应用的界面。Dashboard根据用户当前浏览的地图区域更新左侧的统计数据。</p>
<p>Dashboard应用</p>
<p>查询某一个路口<br>基于历史数据的异常行为监测<br>交通部门还想知道每一种车型和行人在路口的动态流量是如何的。为了解决这个问题，我们在一个礼拜后使用历史GeoEvent服务生成的数据来计算每一种车型和行人每天每一分钟的流量状况。我们简单计算一下，会有约1百万种可能的组合（111相机<em>7天</em>24小时*60分钟）。你可以把它比作一个异常监测的查询表。我们将每一种车型的计数与历史技术做比较，如果计数高于历史计数30%，那么我们称为流量异常，并且将异常展现在地图上。我们将异常事件写进一个单独的要素类里面。下图展示了某个路口的行人和车辆的异常状况。</p>
<p>交通部门还特别在意行人路口行为。他们主要想知道是否有行人不使用人行道。解决这个问题有很多方法。一半方法是检测处图像中的人行道，然后将人行道范围外的行人标为异常。</p>
<p>某路口异常行为<br>笔者使用了另外一种方法。笔者找了一个路口连续运行YOLO3识别目标物五个小时，然后将所有行人类别的图片坐标（矩形框四个角的像素坐标）提取出，计算每一个矩形框右下，左下像素坐标的平均值，然后将每一个矩形框转换成一个像素点，我们之所以使用左下，右下的坐标，是因为这两个坐标更加贴近地面，可以更好的代表地面上的人行道。下图中每一个红色点代表这个小时内所有行人的位置。这份数据可以揭示行人过马路的规律。由于大多数人都使用人行道过马路，图中可以看到红点都聚集在人行道附近。反之，图片中的其他位置没有红点。</p>
<p>原始路口抓拍图</p>
<p>红点代表行人的历史位置<br>为了将图像中高密度和低密度的红点分开，笔者使用了DBSCAN分析算法。DBSCAN更具点的空间分布以及每个点周围的噪声情况来识别高密度点聚类。DBSCAN还会将一些距离点聚类区域较远的点标为outlier。下图就是使用DBSCAN标记出了不在人行道区域的行人。</p>
<p>未使用人行道的行人 1</p>
<p>未使用人行道的行人 2<br>结论以及展望<br>本文，笔者介绍了GeoAI团队开发的交通路况监测云解决方案。该解决方案可以1）访问获取实时视频数据，2）使用AWS上的YOLO3模型实时识别小汽车，巴士，卡车，摩托车，以及行人，3）将YOLO3的结果发送到AWS上面的GeoEvent服务，在Dashboard应用上会展示路况，并且使用大数据库中的历史数据进行分析，4）根据目标物的数量来分析异常事件，例如监测处于危险位置的行人。GIS在展示相机地理位置以及实时路况起到了关键的作用。未来我们还会基于此方案进行目标追踪，测速，统计车道流量等。</p>
<p>致谢以及参考文献<br>感谢Daniel Wilson配置AWS以及S3图片云存储，让整个流程快了10倍。感谢Joel McCune配置的Dashboard应用。感谢RJ Sunderman 在ArcGIS GeoEvent Server上的帮助，感谢Alberto Nieto联系交通部门启动了这个项目，使得我们可以将YOLO3，ArcGIS GeoEvent Server，AWS添加到Alberto之前的成果中去，实现实时云处理。最后感谢Mark Carlson配置了ArcGIS GeoEvent Server，以及AWS的深度学习镜像。我们还将这个解决方案复制到了Azure上面。如果你有疑问，或者有意向合作，欢迎联系我们。</p>
<p>1] <a target="_blank" rel="noopener" href="https://developers.arcgis.com/python">https://developers.arcgis.com/python</a><br>2] <a target="_blank" rel="noopener" href="https://aws.amazon.com/machine-learning/amis">https://aws.amazon.com/machine-learning/amis</a><br>3] <a target="_blank" rel="noopener" href="http://www.arcgis.com/index.html">http://www.arcgis.com/index.html</a><br>4] <a target="_blank" rel="noopener" href="http://www.trafficland.com/">http://www.trafficland.com</a><br>5] <a target="_blank" rel="noopener" href="https://github.com/tzutalin/labelImg">https://github.com/tzutalin/labelImg</a><br>6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.02640https://arxiv.org/abs/1612.08242https://arxiv.org/abs/1804.02767https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html#yolo-you-only-look-oncehttps://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b">https://arxiv.org/abs/1506.02640https://arxiv.org/abs/1612.08242https://arxiv.org/abs/1804.02767https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html#yolo-you-only-look-oncehttps://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b</a><br>7] <a target="_blank" rel="noopener" href="https://github.com/qqwweee/keras-yolo3">https://github.com/qqwweee/keras-yolo3</a><br>8] <a target="_blank" rel="noopener" href="https://enterprise.arcgis.com/en/geoevent/latest/get-started/a-quick-tour-of-geoevent-server.htm">https://enterprise.arcgis.com/en/geoevent/latest/get-started/a-quick-tour-of-geoevent-server.htm</a><br>9] <a target="_blank" rel="noopener" href="https://enterprise.arcgis.com/en/geoevent/latest/administer/managing-big-data-stores.htm">https://enterprise.arcgis.com/en/geoevent/latest/administer/managing-big-data-stores.htm</a><br>10] <a target="_blank" rel="noopener" href="http://desktop.arcgis.com/en/arcmap/10.3/analyze/modelbuilder/what-is-modelbuilder.htm">http://desktop.arcgis.com/en/arcmap/10.3/analyze/modelbuilder/what-is-modelbuilder.htm</a><br>11] <a target="_blank" rel="noopener" href="https://www.esri.com/en-us/arcgis/products/operations-dashboard/overview">https://www.esri.com/en-us/arcgis/products/operations-dashboard/overview</a></p>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>7700261 (first commit)<br>12] <a target="_blank" rel="noopener" href="https://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/densitybasedclustering.htm">https://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/densitybasedclustering.htm</a></p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/2022-06-15-trafficReg/" data-id="clkqazu8l001fdlbiae1fc4uo" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-2022-06-15-yolov1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/2022-06-15-yolov1/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>当我们谈起计算机视觉时，首先想到的就是图像分类，没错，图像分类是计算机视觉最基本的任务之一，但是在图像分类的基础上，还有更复杂和有意思的任务，如目标检测，物体定位，图像分割等，见图1所示。其中目标检测是一件比较实际的且具有挑战性的计算机视觉任务，其可以看成图像分类与定位的结合，给定一张图片，目标检测系统要能够识别出图片的目标并给出其位置，由于图片中目标数是不定的，且要给出目标的精确位置，目标检测相比分类任务更复杂。目标检测的一个实际应用场景就是无人驾驶，如果能够在无人车上装载一个有效的目标检测系统，那么无人车将和人一样有了眼睛，可以快速地检测出前面的行人与车辆，从而作出实时决策。</p>
<p><img src="https://pic3.zhimg.com/80/v2-4a8db9d67ca78afe04d5610e31e6061a_720w.jpg" alt="img">图1 计算机视觉任务（来源: cs231n）</p>
<p>近几年来，目标检测算法取得了很大的突破。比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN），它们是two-stage的，需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归。而另一类是Yolo，SSD这类one-stage算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置。第一类方法是准确度高一些，但是速度慢，但是第二类算法是速度快，但是准确性要低一些。这可以在图2中看到。本文介绍的是Yolo算法，其全称是You Only Look Once: Unified, Real-Time Object Detection，其实个人觉得这个题目取得非常好，基本上把Yolo算法的特点概括全了：You Only Look Once说的是只需要一次CNN运算，Unified指的是这是一个统一的框架，提供end-to-end的预测，而Real-Time体现是Yolo算法速度快。这里我们谈的是Yolo-v1版本算法，其性能是差于后来的SSD算法的，但是Yolo后来也继续进行改进，产生了Yolo9000算法。本文主要讲述Yolo-v1算法的原理，特别是算法的训练与预测中详细细节，最后将给出如何使用TensorFlow实现Yolo算法。</p>
<p><img src="https://pic3.zhimg.com/80/v2-497f2c3efd752f31601e26d0523f70c2_720w.jpg" alt="img">图2 目标检测算法进展与对比</p>
<h2 id="滑动窗口与CNN"><a href="#滑动窗口与CNN" class="headerlink" title="滑动窗口与CNN"></a>滑动窗口与CNN</h2><p>在介绍Yolo算法之前，首先先介绍一下滑动窗口技术，这对我们理解Yolo算法是有帮助的。采用滑动窗口的目标检测算法思路非常简单，它将检测问题转化为了图像分类问题。其基本原理就是采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类，这样就可以实现对整张图片的检测了，如下图3所示，如DPM就是采用这种思路。但是这个方法有致命的缺点，就是你并不知道要检测的目标大小是什么规模，所以你要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量，所以你的分类器不能太复杂，因为要保证速度。解决思路之一就是减少要分类的子区域，这就是R-CNN的一个改进策略，其采用了selective search方法来找到最有可能包含目标的子区域（Region Proposal），其实可以看成采用启发式方法过滤掉很多子区域，这会提升效率。</p>
<p><img src="https://pic1.zhimg.com/80/v2-1a6f0fc27b69040b0c16ca480444edf8_720w.jpg" alt="img">图3 采用滑动窗口进行目标检测（来源：deeplearning.ai）</p>
<p>如果你使用的是CNN分类器，那么滑动窗口是非常耗时的。但是结合卷积运算的特点，我们可以使用CNN实现更高效的滑动窗口方法。这里要介绍的是一种全卷积的方法，简单来说就是网络中用卷积层代替了全连接层，如图4所示。输入图片大小是16x16，经过一系列卷积操作，提取了2x2的特征图，但是这个2x2的图上每个元素都是和原图是一一对应的，如图上蓝色的格子对应蓝色的区域，这不就是相当于在原图上做大小为14x14的窗口滑动，且步长为2，共产生4个字区域。最终输出的通道数为4，可以看成4个类别的预测概率值，这样一次CNN计算就可以实现窗口滑动的所有子区域的分类预测。这其实是overfeat算法的思路。之所可以CNN可以实现这样的效果是因为卷积操作的特性，就是图片的空间位置信息的不变性，尽管卷积过程中图片大小减少，但是位置对应关系还是保存的。说点题外话，这个思路也被R-CNN借鉴，从而诞生了Fast R-cNN算法。</p>
<p><img src="https://pic3.zhimg.com/80/v2-657d39aaeb56bf586cb18f733244c8e6_720w.jpg" alt="img">图4 滑动窗口的CNN实现（来源：deeplearning.ai）</p>
<p>上面尽管可以减少滑动窗口的计算量，但是只是针对一个固定大小与步长的窗口，这是远远不够的。Yolo算法很好的解决了这个问题，它不再是窗口滑动了，而是直接将原始图片分割成互不重合的小方块，然后通过卷积最后生产这样大小的特征图，基于上面的分析，可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标，这就是Yolo算法的朴素思想。下面将详细介绍Yolo算法的设计理念。</p>
<h2 id="设计理念"><a href="#设计理念" class="headerlink" title="设计理念"></a>设计理念</h2><p>整体来看，Yolo算法采用一个单独的CNN模型实现end-to-end的目标检测，整个系统如图5所示：首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。相比R-CNN算法，其是一个统一的框架，其速度更快，而且Yolo的训练过程也是end-to-end的。</p>
<p><img src="https://pic4.zhimg.com/80/v2-d37bcff4e377a514aabfb0e371ccdf7b_720w.jpg" alt="img">图5 Yolo检测系统</p>
<p>具体来说，Yolo的CNN网络将输入的图片分割成 <img src="https://www.zhihu.com/equation?tex=S%5Ctimes+S" alt="[公式]"> 网格，然后每个单元格负责去检测那些中心点落在该格子内的目标，如图6所示，可以看到狗这个目标的中心落在左下角一个单元格内，那么该单元格负责预测这个狗。每个单元格会预测 <img src="https://www.zhihu.com/equation?tex=B" alt="[公式]"> 个边界框（bounding box）以及边界框的置信度（confidence score）。所谓置信度其实包含两个方面，一是这个边界框含有目标的可能性大小，二是这个边界框的准确度。前者记为 <img src="https://www.zhihu.com/equation?tex=Pr(object)" alt="[公式]"> ，当该边界框是背景时（即不包含目标），此时 <img src="https://www.zhihu.com/equation?tex=Pr(object)=0" alt="[公式]"> 。而当该边界框包含目标时， <img src="https://www.zhihu.com/equation?tex=Pr(object)=1" alt="[公式]"> 。边界框的准确度可以用预测框与实际框（ground truth）的IOU（intersection over union，交并比）来表征，记为 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D" alt="[公式]"> 。因此置信度可以定义为 <img src="https://www.zhihu.com/equation?tex=Pr(object)*%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D" alt="[公式]"> 。很多人可能将Yolo的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的乘积，预测框的准确度也反映在里面。边界框的大小与位置可以用4个值来表征： <img src="https://www.zhihu.com/equation?tex=(x,+y,w,h)" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=(x,y)" alt="[公式]"> 是边界框的中心坐标，而 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 是边界框的宽与高。还有一点要注意，中心坐标的预测值 <img src="https://www.zhihu.com/equation?tex=(x,y)" alt="[公式]"> 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的，单元格的坐标定义如图6所示。而边界框的 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=h" alt="[公式]"> 预测值是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在 <img src="https://www.zhihu.com/equation?tex=%5B0,1%5D" alt="[公式]"> 范围。这样，每个边界框的预测值实际上包含5个元素： <img src="https://www.zhihu.com/equation?tex=(x,y,w,h,c)" alt="[公式]"> ，其中前4个表征边界框的大小与位置，而最后一个值是置信度。</p>
<p><img src="https://pic2.zhimg.com/80/v2-fdfea5fcb4ff3ecc327758878e4ad6e1_720w.jpg" alt="img">图6 网格划分</p>
<p>还有分类问题，对于每一个单元格其还要给出预测出 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]"> 个类别概率值，其表征的是由该单元格负责预测的边界框其目标属于各个类别的概率。但是这些概率值其实是在各个边界框置信度下的条件概率，即 <img src="https://www.zhihu.com/equation?tex=Pr(class_%7Bi%7D%7Cobject)" alt="[公式]"> 。值得注意的是，不管一个单元格预测多少个边界框，其只预测一组类别概率值，这是Yolo算法的一个缺点，在后来的改进版本中，Yolo9000是把类别概率预测值与边界框是绑定在一起的。同时，我们可以计算出各个边界框类别置信度（class-specific confidence scores）: <img src="https://www.zhihu.com/equation?tex=Pr(class_%7Bi%7D%7Cobject)*Pr(object)*%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D=Pr(class_%7Bi%7D)*%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D" alt="[公式]"> 。</p>
<p>边界框类别置信度表征的是该边界框中目标属于各个类别的可能性大小以及边界框匹配目标的好坏。后面会说，一般会根据类别置信度来过滤网络的预测框。</p>
<p>总结一下，每个单元格需要预测 <img src="https://www.zhihu.com/equation?tex=(B*5+C)" alt="[公式]"> 个值。如果将输入图片划分为 <img src="https://www.zhihu.com/equation?tex=S%5Ctimes+S" alt="[公式]"> 网格，那么最终预测值为 <img src="https://www.zhihu.com/equation?tex=S%5Ctimes+S%5Ctimes+(B*5+C)" alt="[公式]"> 大小的张量。整个模型的预测值结构如下图所示。对于PASCAL VOC数据，其共有20个类别，如果使用 <img src="https://www.zhihu.com/equation?tex=S=7,B=2" alt="[公式]"> ，那么最终的预测结果就是 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes+7%5Ctimes+30" alt="[公式]"> 大小的张量。在下面的网络结构中我们会详细讲述每个单元格的预测值的分布位置。</p>
<p><img src="https://pic3.zhimg.com/80/v2-258df167ee37b5594c72562b4ae61d1a_720w.jpg" alt="img">图7 模型预测值结构</p>
<h2 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h2><p>Yolo采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层，如图8所示。对于卷积层，主要使用1x1卷积来做channle reduction，然后紧跟3x3卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数： <img src="https://www.zhihu.com/equation?tex=max(x,+0.1x)" alt="[公式]"> 。但是最后一层却采用线性激活函数。</p>
<p><img src="https://pic4.zhimg.com/80/v2-5d099287b1237fa975b1c19bacdfc07f_720w.jpg" alt="img">图8 网络结构</p>
<p>可以看到网络的最后输出为 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes+7%5Ctimes+30" alt="[公式]"> 大小的张量。这和前面的讨论是一致的。这个张量所代表的具体含义如图9所示。对于每一个单元格，前20个元素是类别概率值，然后2个元素是边界框置信度，两者相乘可以得到类别置信度，最后8个元素是边界框的 <img src="https://www.zhihu.com/equation?tex=(x,+y,w,h)" alt="[公式]"> 。大家可能会感到奇怪，对于边界框为什么把置信度 <img src="https://www.zhihu.com/equation?tex=c" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=(x,+y,w,h)" alt="[公式]"> 都分开排列，而不是按照 <img src="https://www.zhihu.com/equation?tex=(x,+y,w,h,c)" alt="[公式]"> 这样排列，其实纯粹是为了计算方便，因为实际上这30个元素都是对应一个单元格，其排列是可以任意的。但是分离排布，可以方便地提取每一个部分。这里来解释一下，首先网络的预测值是一个二维张量 <img src="https://www.zhihu.com/equation?tex=P" alt="[公式]"> ，其shape为 <img src="https://www.zhihu.com/equation?tex=%5Bbatch,+7%5Ctimes+7%5Ctimes+30%5D" alt="[公式]"> 。采用切片，那么 <img src="https://www.zhihu.com/equation?tex=P_%7B%5B:,0:7*7*20%5D%7D" alt="[公式]"> 就是类别概率部分，而 <img src="https://www.zhihu.com/equation?tex=P_%7B%5B:,7*7*20:7*7*(20+2)%5D%7D" alt="[公式]"> 是置信度部分，最后剩余部分 <img src="https://www.zhihu.com/equation?tex=P_%7B%5B:,7*7*(20+2):%5D%7D" alt="[公式]"> 是边界框的预测结果。这样，提取每个部分是非常方便的，这会方面后面的训练及预测时的计算。</p>
<p><img src="https://pic1.zhimg.com/80/v2-8630f8d3dbe3634f124eaf82f222ca94_720w.jpg" alt="img">图9 预测张量的解析</p>
<h2 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h2><p>在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用图8中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。整个网络的流程如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-40c8cbed60aba0fe2faa38e240b8563b_720w.jpg" alt="img">图10 Yolo网络流程</p>
<p>下面是训练损失函数的分析，Yolo算法将目标检测看成回归问题，所以采用的是均方差损失函数。但是对不同的部分采用了不同的权重值。首先区分定位误差和分类误差。对于定位误差，即边界框坐标预测误差，采用较大的权重 <img src="https://www.zhihu.com/equation?tex=%5Clambda+_%7Bcoord%7D=5" alt="[公式]"> 。然后其区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值 <img src="https://www.zhihu.com/equation?tex=%5Clambda+_%7Bnoobj%7D=0.5" alt="[公式]"> 。其它权重值均设为1。然后采用均方误差，其同等对待大小不同的边界框，但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为 <img src="https://www.zhihu.com/equation?tex=(x,y,%5Csqrt%7Bw%7D,+%5Csqrt%7Bh%7D)" alt="[公式]"> 。</p>
<p>另外一点时，由于每个单元格预测多个边界框。但是其对应类别只有一个。那么在训练时，如果该单元格内确实存在目标，那么只选择与ground truth的IOU最大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能。大家可能会想如果一个单元格内存在多个目标怎么办，其实这时候Yolo算法就只能选择其中一个来训练，这也是Yolo算法的缺点之一。要注意的一点时，对于不存在对应目标的边界框，其误差项就是只有置信度，坐标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。</p>
<p>综上讨论，最终的损失函数计算如下：</p>
<p><img src="https://pic3.zhimg.com/80/v2-45795a63cdbaac8c05d875dfb6fcfb5a_720w.jpg" alt="img"></p>
<p>其中第一项是边界框中心坐标的误差项， <img src="https://www.zhihu.com/equation?tex=1%5E%7Bobj%7D_%7Bij%7D" alt="[公式]"> 指的是第 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 个单元格存在目标，且该单元格中的第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 个边界框负责预测该目标。第二项是边界框的高与宽的误差项。第三项是包含目标的边界框的置信度误差项。第四项是不包含目标的边界框的置信度误差项。而最后一项是包含目标的单元格的分类误差项， <img src="https://www.zhihu.com/equation?tex=1%5E%7Bobj%7D_%7Bi%7D" alt="[公式]"> 指的是第 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]"> 个单元格存在目标。这里特别说一下置信度的target值 <img src="https://www.zhihu.com/equation?tex=C_i" alt="[公式]"> ，如果是不存在目标，此时由于 <img src="https://www.zhihu.com/equation?tex=Pr(object)=0" alt="[公式]">，那么 <img src="https://www.zhihu.com/equation?tex=C_i=0" alt="[公式]"> 。如果存在目标， <img src="https://www.zhihu.com/equation?tex=Pr(object)=1" alt="[公式]"> ，此时需要确定 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7BIOU%7D%5E%7Btruth%7D_%7Bpred%7D" alt="[公式]"> ，当然你希望最好的话，可以将IOU取1，这样 <img src="https://www.zhihu.com/equation?tex=C_i=1" alt="[公式]"> ，但是在YOLO实现中，使用了一个控制参数rescore（默认为1），当其为1时，IOU不是设置为1，而就是计算truth和pred之间的真实IOU。不过很多复现YOLO的项目还是取 <img src="https://www.zhihu.com/equation?tex=C_i=1" alt="[公式]"> ，这个差异应该不会太影响结果吧。</p>
<h2 id="网络预测"><a href="#网络预测" class="headerlink" title="网络预测"></a>网络预测</h2><p>在说明Yolo算法的预测过程之前，这里先介绍一下非极大值抑制算法（non maximum suppression, NMS），这个算法不单单是针对Yolo算法的，而是所有的检测算法中都会用到。NMS算法主要解决的是一个目标被多次检测的问题，如图11中人脸检测，可以看到人脸被多次检测，但是其实我们希望最后仅仅输出其中一个最好的预测框，比如对于美女，只想要红色那个检测结果。那么可以采用NMS算法来实现这样的效果：首先从所有的检测框中找到置信度最大的那个框，然后挨个计算其与剩余框的IOU，如果其值大于一定阈值（重合度过高），那么就将该框剔除；然后对剩余的检测框重复上述过程，直到处理完所有的检测框。Yolo预测过程也需要用到NMS算法。</p>
<p><img src="https://pic3.zhimg.com/80/v2-21f14aaa1cd64ab69fc7389044e1bd3a_720w.jpg" alt="img">图11 NMS应用在人脸检测</p>
<p>下面就来分析Yolo的预测过程，这里我们不考虑batch，认为只是预测一张输入图片。根据前面的分析，最终的网络输出是 <img src="https://www.zhihu.com/equation?tex=7%5Ctimes+7+%5Ctimes+30" alt="[公式]"> ，但是我们可以将其分割成三个部分：类别概率部分为 <img src="https://www.zhihu.com/equation?tex=%5B7,+7,+20%5D" alt="[公式]"> ，置信度部分为 <img src="https://www.zhihu.com/equation?tex=%5B7,7,2%5D" alt="[公式]"> ，而边界框部分为 <img src="https://www.zhihu.com/equation?tex=%5B7,7,2,4%5D" alt="[公式]"> （对于这部分不要忘记根据原始图片计算出其真实值）。然后将前两项相乘（矩阵 <img src="https://www.zhihu.com/equation?tex=%5B7,+7,+20%5D" alt="[公式]"> 乘以 <img src="https://www.zhihu.com/equation?tex=%5B7,7,2%5D" alt="[公式]"> 可以各补一个维度来完成 <img src="https://www.zhihu.com/equation?tex=%5B7,7,1,20%5D%5Ctimes+%5B7,7,2,1%5D" alt="[公式]"> ）可以得到类别置信度值为 <img src="https://www.zhihu.com/equation?tex=%5B7,+7,2,20%5D" alt="[公式]"> ，这里总共预测了 <img src="https://www.zhihu.com/equation?tex=7*7*2=98" alt="[公式]"> 个边界框。</p>
<p>所有的准备数据已经得到了，那么我们先说第一种策略来得到检测框的结果，我认为这是最正常与自然的处理。首先，对于每个预测框根据类别置信度选取置信度最大的那个类别作为其预测标签，经过这层处理我们得到各个预测框的预测类别及对应的置信度值，其大小都是 <img src="https://www.zhihu.com/equation?tex=%5B7,7,2%5D" alt="[公式]"> 。一般情况下，会设置置信度阈值，就是将置信度小于该阈值的box过滤掉，所以经过这层处理，剩余的是置信度比较高的预测框。最后再对这些预测框使用NMS算法，最后留下来的就是检测结果。一个值得注意的点是NMS是对所有预测框一视同仁，还是区分每个类别，分别使用NMS。Ng在deeplearning.ai中讲应该区分每个类别分别使用NMS，但是看了很多实现，其实还是同等对待所有的框，我觉得可能是不同类别的目标出现在相同位置这种概率很低吧。</p>
<p>上面的预测方法应该非常简单明了，但是对于Yolo算法，其却采用了另外一个不同的处理思路（至少从C源码看是这样的），其区别就是先使用NMS，然后再确定各个box的类别。其基本过程如图12所示。对于98个boxes，首先将小于置信度阈值的值归0，然后分类别地对置信度值采用NMS，这里NMS处理结果不是剔除，而是将其置信度值归为0。最后才是确定各个box的类别，当其置信度值不为0时才做出检测结果输出。这个策略不是很直接，但是貌似Yolo源码就是这样做的。Yolo论文里面说NMS算法对Yolo的性能是影响很大的，所以可能这种策略对Yolo更好。但是我测试了普通的图片检测，两种策略结果是一样的。</p>
<p><img src="https://pic3.zhimg.com/80/v2-47651d0a677009d58c899c215d342e06_720w.jpg" alt="img">图12 Yolo的预测处理流程</p>
<h2 id="算法性能分析"><a href="#算法性能分析" class="headerlink" title="算法性能分析"></a>算法性能分析</h2><p>这里看一下Yolo算法在PASCAL VOC 2007数据集上的性能，这里Yolo与其它检测算法做了对比，包括DPM，R-CNN，Fast R-CNN以及Faster R-CNN。其对比结果如表1所示。与实时性检测方法DPM对比，可以看到Yolo算法可以在较高的mAP上达到较快的检测速度，其中Fast Yolo算法比快速DPM还快，而且mAP是远高于DPM。但是相比Faster R-CNN，Yolo的mAP稍低，但是速度更快。所以。Yolo算法算是在速度与准确度上做了折中。</p>
<p><img src="https://pic4.zhimg.com/80/v2-3da4029640307f737719843b389fc0ff_720w.jpg" alt="img">表1 Yolo在PASCAL VOC 2007上与其他算法的对比</p>
<p>为了进一步分析Yolo算法，文章还做了误差分析，将预测结果按照分类与定位准确性分成以下5类：</p>
<ul>
<li>Correct：类别正确，IOU&gt;0.5；（准确度）</li>
<li>Localization：类别正确，0.1 &lt; IOU&lt;0.5（定位不准）；</li>
<li>Similar：类别相似，IOU&gt;0.1；</li>
<li>Other：类别错误，IOU&gt;0.1；</li>
<li>Background：对任何目标其IOU&lt;0.1。（误把背景当物体）</li>
</ul>
<p>Yolo与Fast R-CNN的误差对比分析如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-cb8bb72fad701466328b386da6a93943_720w.jpg" alt="img">图13 Yolo与Fast R-CNN的误差对比分析</p>
<p>可以看到，Yolo的Correct的是低于Fast R-CNN。另外Yolo的Localization误差偏高，即定位不是很准确。但是Yolo的Background误差很低，说明其对背景的误判率较低。Yolo的那篇文章中还有更多性能对比，感兴趣可以看看。</p>
<p>现在来总结一下Yolo的优缺点。首先是优点，Yolo采用一个CNN网络来实现检测，是单管道策略，其训练与预测都是end-to-end，所以Yolo算法比较简洁且速度快。第二点由于Yolo是对整张图片做卷积，所以其在检测目标有更大的视野，它不容易对背景误判。其实我觉得全连接层也是对这个有贡献的，因为全连接起到了attention的作用。另外，Yolo的泛化能力强，在做迁移时，模型鲁棒性高。</p>
<p>最后不得不谈一下Yolo的缺点，首先Yolo各个单元格仅仅预测两个边界框，而且属于一个类别。对于小物体，Yolo的表现会不如人意。这方面的改进可以看SSD，其采用多尺度单元格。也可以看Faster R-CNN，其采用了anchor boxes。Yolo对于在物体的宽高比方面泛化率低，就是无法定位不寻常比例的物体。当然Yolo的定位不准确也是很大的问题。</p>
<h2 id="算法的TF实现"><a href="#算法的TF实现" class="headerlink" title="算法的TF实现"></a>算法的TF实现</h2><p>Yolo的源码是用C实现的，但是好在Github上有很多开源的TF复现。这里我们参考gliese581gg的<a href="https://link.zhihu.com/?target=https://github.com/gliese581gg/YOLO_tensorflow">YOLO_tensorflow</a>的实现来分析Yolo的Inference实现细节。我们的代码将构建一个end-to-end的Yolo的预测模型，利用的已经训练好的权重文件，你将可以用自然的图片去测试检测效果。</p>
<p>首先，我们定义Yolo的模型参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Yolo</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weights_file, verbose=<span class="literal">True</span></span>):</span><br><span class="line">        self.verbose = verbose</span><br><span class="line">        <span class="comment"># detection params</span></span><br><span class="line">        self.S = <span class="number">7</span>  <span class="comment"># cell size</span></span><br><span class="line">        self.B = <span class="number">2</span>  <span class="comment"># boxes_per_cell</span></span><br><span class="line">        self.classes = [<span class="string">&quot;aeroplane&quot;</span>, <span class="string">&quot;bicycle&quot;</span>, <span class="string">&quot;bird&quot;</span>, <span class="string">&quot;boat&quot;</span>, <span class="string">&quot;bottle&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;bus&quot;</span>, <span class="string">&quot;car&quot;</span>, <span class="string">&quot;cat&quot;</span>, <span class="string">&quot;chair&quot;</span>, <span class="string">&quot;cow&quot;</span>, <span class="string">&quot;diningtable&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;dog&quot;</span>, <span class="string">&quot;horse&quot;</span>, <span class="string">&quot;motorbike&quot;</span>, <span class="string">&quot;person&quot;</span>, <span class="string">&quot;pottedplant&quot;</span>,</span><br><span class="line">                        <span class="string">&quot;sheep&quot;</span>, <span class="string">&quot;sofa&quot;</span>, <span class="string">&quot;train&quot;</span>,<span class="string">&quot;tvmonitor&quot;</span>]</span><br><span class="line">        self.C = <span class="built_in">len</span>(self.classes) <span class="comment"># number of classes</span></span><br><span class="line">        <span class="comment"># offset for box center (top left point of each cell)</span></span><br><span class="line">        self.x_offset = np.transpose(np.reshape(np.array([np.arange(self.S)]*self.S*self.B),</span><br><span class="line">                                              [self.B, self.S, self.S]), [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line">        self.y_offset = np.transpose(self.x_offset, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        self.threshold = <span class="number">0.2</span>  <span class="comment"># confidence scores threhold</span></span><br><span class="line">        self.iou_threshold = <span class="number">0.4</span></span><br><span class="line">        <span class="comment">#  the maximum number of boxes to be selected by non max suppression</span></span><br><span class="line">        self.max_output_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        self._build_net()</span><br><span class="line">        self._build_detector()</span><br><span class="line">        self._load_weights(weights_file)</span><br></pre></td></tr></table></figure>

<p>然后是我们模型的主体网络部分，这个网络将输出[batch,7<em>7</em>30]的张量:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_build_net</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;build the network&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.verbose:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Start to build the network ...&quot;</span>)</span><br><span class="line">        self.images = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">448</span>, <span class="number">448</span>, <span class="number">3</span>])</span><br><span class="line">        net = self._conv_layer(self.images, <span class="number">1</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>)</span><br><span class="line">        net = self._maxpool_layer(net, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">2</span>, <span class="number">192</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._maxpool_layer(net, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">3</span>, <span class="number">128</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">4</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">5</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">6</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._maxpool_layer(net, <span class="number">6</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">7</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">8</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">9</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">10</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">11</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">12</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">13</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">14</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">15</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">16</span>, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._maxpool_layer(net, <span class="number">16</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">17</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">18</span>, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">19</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">20</span>, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">21</span>, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">22</span>, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">23</span>, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._conv_layer(net, <span class="number">24</span>, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        net = self._flatten(net)</span><br><span class="line">        net = self._fc_layer(net, <span class="number">25</span>, <span class="number">512</span>, activation=leak_relu)</span><br><span class="line">        net = self._fc_layer(net, <span class="number">26</span>, <span class="number">4096</span>, activation=leak_relu)</span><br><span class="line">        net = self._fc_layer(net, <span class="number">27</span>, self.S*self.S*(self.C+<span class="number">5</span>*self.B))</span><br><span class="line">        self.predicts = net</span><br></pre></td></tr></table></figure>

<p>接下来，我们要去解析网络的预测结果，这里采用了第一种预测策略，即判断预测框类别，再NMS，多亏了TF提供了NMS的函数<a href="https://link.zhihu.com/?target=https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression">tf.image.non_max_suppression</a>，其实实现起来很简单，所有的细节前面已经交代了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_build_detector</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Interpret the net output and get the predicted boxes&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># the width and height of orignal image</span></span><br><span class="line">        self.width = tf.placeholder(tf.float32, name=<span class="string">&quot;img_w&quot;</span>)</span><br><span class="line">        self.height = tf.placeholder(tf.float32, name=<span class="string">&quot;img_h&quot;</span>)</span><br><span class="line">        <span class="comment"># get class prob, confidence, boxes from net output</span></span><br><span class="line">        idx1 = self.S * self.S * self.C</span><br><span class="line">        idx2 = idx1 + self.S * self.S * self.B</span><br><span class="line">        <span class="comment"># class prediction</span></span><br><span class="line">        class_probs = tf.reshape(self.predicts[<span class="number">0</span>, :idx1], [self.S, self.S, self.C])</span><br><span class="line">        <span class="comment"># confidence</span></span><br><span class="line">        confs = tf.reshape(self.predicts[<span class="number">0</span>, idx1:idx2], [self.S, self.S, self.B])</span><br><span class="line">        <span class="comment"># boxes -&gt; (x, y, w, h)</span></span><br><span class="line">        boxes = tf.reshape(self.predicts[<span class="number">0</span>, idx2:], [self.S, self.S, self.B, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># convert the x, y to the coordinates relative to the top left point of the image</span></span><br><span class="line">        <span class="comment"># the predictions of w, h are the square root</span></span><br><span class="line">        <span class="comment"># multiply the width and height of image</span></span><br><span class="line">        boxes = tf.stack([(boxes[:, :, :, <span class="number">0</span>] + tf.constant(self.x_offset, dtype=tf.float32)) / self.S * self.width,</span><br><span class="line">                          (boxes[:, :, :, <span class="number">1</span>] + tf.constant(self.y_offset, dtype=tf.float32)) / self.S * self.height,</span><br><span class="line">                          tf.square(boxes[:, :, :, <span class="number">2</span>]) * self.width,</span><br><span class="line">                          tf.square(boxes[:, :, :, <span class="number">3</span>]) * self.height], axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># class-specific confidence scores [S, S, B, C]</span></span><br><span class="line">        scores = tf.expand_dims(confs, -<span class="number">1</span>) * tf.expand_dims(class_probs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        scores = tf.reshape(scores, [-<span class="number">1</span>, self.C])  <span class="comment"># [S*S*B, C]</span></span><br><span class="line">        boxes = tf.reshape(boxes, [-<span class="number">1</span>, <span class="number">4</span>])  <span class="comment"># [S*S*B, 4]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># find each box class, only select the max score</span></span><br><span class="line">        box_classes = tf.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line">        box_class_scores = tf.reduce_max(scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># filter the boxes by the score threshold</span></span><br><span class="line">        filter_mask = box_class_scores &gt;= self.threshold</span><br><span class="line">        scores = tf.boolean_mask(box_class_scores, filter_mask)</span><br><span class="line">        boxes = tf.boolean_mask(boxes, filter_mask)</span><br><span class="line">        box_classes = tf.boolean_mask(box_classes, filter_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># non max suppression (do not distinguish different classes)</span></span><br><span class="line">        <span class="comment"># ref: https://tensorflow.google.cn/api_docs/python/tf/image/non_max_suppression</span></span><br><span class="line">        <span class="comment"># box (x, y, w, h) -&gt; box (x1, y1, x2, y2)</span></span><br><span class="line">        _boxes = tf.stack([boxes[:, <span class="number">0</span>] - <span class="number">0.5</span> * boxes[:, <span class="number">2</span>], boxes[:, <span class="number">1</span>] - <span class="number">0.5</span> * boxes[:, <span class="number">3</span>],</span><br><span class="line">                           boxes[:, <span class="number">0</span>] + <span class="number">0.5</span> * boxes[:, <span class="number">2</span>], boxes[:, <span class="number">1</span>] + <span class="number">0.5</span> * boxes[:, <span class="number">3</span>]], axis=<span class="number">1</span>)</span><br><span class="line">        nms_indices = tf.image.non_max_suppression(_boxes, scores,</span><br><span class="line">                                                   self.max_output_size, self.iou_threshold)</span><br><span class="line">        self.scores = tf.gather(scores, nms_indices)</span><br><span class="line">        self.boxes = tf.gather(boxes, nms_indices)</span><br><span class="line">        self.box_classes = tf.gather(box_classes, nms_indices)</span><br></pre></td></tr></table></figure>

<p>其他的就比较容易了，详细代码附在<a href="https://link.zhihu.com/?target=https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/ObjectDetections/yolo/yolo_tf.py">xiaohu2015&#x2F;DeepLearning_tutorials</a>上了，欢迎给点个赞，权重文件在<a href="https://link.zhihu.com/?target=https://pan.baidu.com/s/1mhE0WL6">这里下载</a>。</p>
<p>最后就是愉快地测试你自己的图片了：</p>
<p><img src="https://pic2.zhimg.com/80/v2-cade4c6a5fef395d21cc2a663b403b15_720w.jpg" alt="img"></p>
<p>当然，如果你对训练过程感兴趣，你可以参考这里的实现<a href="https://link.zhihu.com/?target=https://github.com/thtrieu/darkflow">thtrieu&#x2F;darkflow</a>，如果你看懂了预测过程的代码，这里也会很容易阅读。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇长文详细介绍了Yolo算法的原理及实现，当然Yolo-v1还是有很多问题的，所以后续可以读读Yolo9000算法，看看其如何改进的。Ng说Yolo的paper是比较难读的，其实是很多实现细节，如果不看代码是很难理解的。所以，文章中如果有错误也可能是难免的，欢迎交流指正。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a>.</li>
<li><a href="https://link.zhihu.com/?target=https://pjreddie.com/darknet/yolo/">Yolo官网</a>.</li>
<li><a href="https://link.zhihu.com/?target=https://github.com/gliese581gg/YOLO_tensorflow">Yolo的TF实现</a>.</li>
<li><a href="https://link.zhihu.com/?target=https://www.youtube.com/watch?v=L0tzmv--CGY">YOLO: You only look once (How it works)</a>.（注：很多实现细节，需要墙）</li>
<li>Ng的<a href="https://link.zhihu.com/?target=https://www.deeplearning.ai/">deeplearning.ai</a>课程.</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/2022-06-15-yolov1/" data-id="clkqazu8m001gdlbi1bed34w1" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-HM_005_不定积分、定积分与反常积分" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/HM_005_%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86%E3%80%81%E5%AE%9A%E7%A7%AF%E5%88%86%E4%B8%8E%E5%8F%8D%E5%B8%B8%E7%A7%AF%E5%88%86/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="不定积分、定积分与反常积分"><a href="#不定积分、定积分与反常积分" class="headerlink" title="不定积分、定积分与反常积分"></a>不定积分、定积分与反常积分</h1><h2 id="不定积分"><a href="#不定积分" class="headerlink" title="不定积分"></a>不定积分</h2><h3 id="一、不定积分概念"><a href="#一、不定积分概念" class="headerlink" title="一、不定积分概念"></a>一、不定积分概念</h3><h4 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h4><p>$$<br>\begin{align}<br>&amp;原函数：设对于区间I上的任意一点x均有F’(x)&#x3D;f(x),则称F(x)为f(x)在区间I上的一个原函数\<br>&amp;不定积分：设函数f(x)于区间I上有原函数,则其余原函数的全体称为f(x)于区间I上的不定积分,记为\int{f(x)dx}\<br>&amp;线性：\int[\alpha f(x)+\beta g(x)]dx&#x3D;\alpha\int f(x)dx+\beta\int g(x)dx\<br>\end{align}<br>$$</p>
<h4 id="2-计算"><a href="#2-计算" class="headerlink" title="2.计算"></a>2.计算</h4><p>$$<br>\begin{align}<br>&amp;计算方法\begin{cases}&amp;1.基本公式\&amp;2.线性\&amp;3.积分法\begin{cases}&amp;1.换元法\&amp;2.分部积分法\\end{cases}\\end{cases}\<br>\end{align}<br>$$</p>
<h5 id="1-第一换元法-凑微分"><a href="#1-第一换元法-凑微分" class="headerlink" title="(1)第一换元法(凑微分)"></a>(1)第一换元法(凑微分)</h5><p>$$<br>\begin{align}<br>&amp;设F’(u)&#x3D;f(u),则\int{f(\Phi(x))\Phi’(x)}dx&#x3D;\int{f(\Phi(x))d(\Phi(x))}&#x3D;F(\Phi(x))+C\<br>&amp;注解：找到合适的凑微分\Phi’(x)dx&#x3D;d(\Phi(x))<br>\end{align}<br>$$</p>
<p>常见凑微分：<br>$$<br>\begin{align}<br>&amp;1.\int{f(ax+b)dx&#x3D;\frac{1}{a}\int{f(ax+b)d(ax+b)}}(a\neq0)\<br>&amp;eg1.\int{\sin (2x+3)}dx&#x3D;\frac{1}{2}\int\sin (2x+3)d(2x+3)&#x3D;\frac{1}{2}\cos{(2x+3)}+C\<br>&amp;2.\int{f(ax^n+b)x^{n-1}dx}&#x3D;\frac{1}{na}\int{f(ax^n+b)d(ax^n+b)}\<br>&amp;eg2.\int{\cos(2x^4+3)x^3dx}&#x3D;\frac{1}{4*2}\int{\cos(2x^4+3)d(2x^4+3)}&#x3D;\frac{1}{8}\cos{(2x^4+3)}+C\<br>&amp;3.\int{f(a^x+c)a^xdx}&#x3D;\frac{1}{\ln{a}}\int{f(a^x+c)}d(a^x+c)\<br>&amp;eg3.\int{\sin(2^x+3)2^xdx}&#x3D;\frac{1}{\ln2}\int{\sin{(2^x+3)}d(2^x+3)}&#x3D;\frac{1}{\ln 2}\cos{(2^x+3)}\<br>&amp;4.\int{f(\frac{1}{x})\frac{1}{x^2}}dx&#x3D;-\int{f(\frac{1}{x})}d(\frac{1}{x})\<br>&amp;eg4.\int{\ln(\frac{1}{x})}\frac{1}{x^2}dx&#x3D;-\int\ln (\frac{1}{x})d({\frac{1}{x}})+C\<br>&amp;5.\int{f(\ln |x|})\frac{1}{x}d(x)&#x3D;\int{f(\ln{|x|)}}{d(\ln|x|)}\<br>&amp;eg5.\int{\sin ({\ln{|x|}}})\frac{1}{x}dx&#x3D;\int{\sin(\ln(|x|)d(\ln{|x|})}&#x3D;\cos(\ln x)+C\<br>&amp;6.\int{f(\sqrt x)\frac{1}{\sqrt x}}dx&#x3D;2\int{f(\sqrt x)}d(\sqrt x)\<br>&amp;7.\int f(\sin x)\cos xdx&#x3D;-\int{(\sin x)}d(\sin x)\<br>&amp;8.\int{f(\cos x)\sin dx}&#x3D;\int{f(\cos x)d(\cos x)}\<br>&amp;9.\int{f(\tan x)\sec^2 xdx}&#x3D;\int{f(\tan x)d(\tan x)}\<br>&amp;10.\int{f(\cot x)\csc^2xdx}&#x3D;-\int{f(\cot x)d{(\cot x)}}\<br>&amp;11.\int{f{(\arcsin x)\frac{1}{\sqrt{1-x^2}}}}dx&#x3D;\int{f(\arcsin x)d({\arcsin x})}\<br>&amp;12.\int{f(\arccos x)(-\frac{1}{\sqrt{1-x^2}}})dx&#x3D;\int{f(\arccos x)d(\arccos x)}\<br>&amp;13.\int{f(\arctan x)\frac{1}{1+x^2}dx}&#x3D;\int{f(\arctan x)d(\arctan x)}\<br>&amp;14.\int{f(\sqrt{x^2+a})}\frac{x}{\sqrt{x^2+a}}dx&#x3D;\int{f(\sqrt{x^2+a})}d(\sqrt{x^2+a})\<br>&amp;注解：(\sqrt{x^2\pm a})’&#x3D;\frac{x}{\sqrt{x^2+a}},(\sqrt{a^2-x^2})’&#x3D;\frac{-x}{\sqrt{a^2-x^2}}\<br>\end{align}<br>$$</p>
<h5 id="2-第二换元法"><a href="#2-第二换元法" class="headerlink" title="(2)第二换元法"></a>(2)第二换元法</h5><p>$$<br>\begin{align}<br>&amp;设F’(u)&#x3D;f(\Phi(u))\Phi’(u),则\<br>&amp;\int{f(x)dx}\overset{x&#x3D;\Phi(u)}{&#x3D;}\int{f(\Phi(u))\Phi’(u)du}&#x3D;F(u)+C&#x3D;F(\Phi^{-1}(x))+C\<br>&amp;注解：找到合适的x&#x3D;\Phi(u)\<br>\end{align}<br>$$</p>
<p>1)三角换元<br>$$<br>\begin{align}<br>&amp;x&#x3D;a\sin u,x&#x3D;a\tan u,x&#x3D;a \sec u\<br>&amp;\sqrt{a^2-x^2}\overset{x&#x3D;a\sin u}{&#x3D;}a\cos u,u\in[-\frac{\pi}{2},\frac{\pi}{2}],x\in[-a,a]\<br>&amp;\sqrt{a^2+x^2}\overset{x&#x3D;a\tan u}{&#x3D;}a\sec u,u\in{(-\frac{\pi}{2},\frac{\pi}{2})},x\in{(-\infty,\infty)}\<br>&amp;\sqrt{x^2-a^2}\overset{x&#x3D;a\sec u}{&#x3D;}a\tan u,u\in(\frac{\pi}{2},\pi]\cup(0,\frac{\pi}{2}]\<br>\end{align}<br>$$<br>2)倒变换<br>$$<br>\begin{align}<br>&amp;x&#x3D;\frac{1}{u}常用于含\frac{1}{x}的函数\<br>\end{align}<br>$$<br>3）指数(或对数)变换<br>$$<br>\begin{align}<br>&amp;a^x&#x3D;u或x&#x3D;\frac{\ln u}{\ln a}常用于含a^x的函数\<br>\end{align}<br>$$<br>4）用于有理化的变换<br>$$<br>\begin{align}<br>&amp;\frac{1}{\sqrt{x}+\sqrt[3]{x}}用x&#x3D;u^6\<br>&amp;\sqrt[n]{\frac{ax+b}{cx+d}}用u&#x3D;\sqrt[n]{\frac{ax+b}{cx+d}}或x&#x3D;-\frac{du^n-b}{cu^n-a}\<br>\end{align}<br>$$</p>
<h5 id="3-分部积分法"><a href="#3-分部积分法" class="headerlink" title="(3)分部积分法"></a>(3)分部积分法</h5><p>$$<br>\begin{align}<br>&amp;\int{u(x)v’(x)dx}&#x3D;\int{u(x)d(v(x))}&#x3D;u(x)v(x)-\int{v(x)u’(x)dx}\<br>&amp;注解：找到合适的u(x),v(x)\<br>\end{align}<br>$$</p>
<p>1)降幂法<br>$$<br>\begin{align}<br>&amp;\int{x^ne^{ax}dx},\int{x^n\sin axdx},\int{x^n\cos ax dx}\<br>&amp;取u(x)&#x3D;x^n\<br>\end{align}<br>$$<br>2)升幂法<br>$$<br>\begin{align}<br>&amp;\int{x^a\ln xdx},\int{x^a\arcsin xdx},\int{x^a\arccos x dx},\int{x^a\arctan x dx}\<br>&amp;取u(x)&#x3D;\ln x\<br>\end{align}<br>$$<br>3)循环法<br>$$<br>\begin{align}<br>&amp;\int{e^{ax}\sin ax dx},\int{e^{ax}\cos {ax} dx}\<br>&amp;取u(x)&#x3D;e^{ax}或\sin{ax}<br>\end{align}<br>$$<br>4)递推公式法<br>$$<br>\begin{align}<br>&amp;与n有关的结果I_n，建立递推关系I_n&#x3D;f(I_{n-1})或f(I_{n-2})\<br>\end{align}<br>$$</p>
<h2 id="定积分"><a href="#定积分" class="headerlink" title="定积分"></a>定积分</h2><h3 id="一、定积分概念"><a href="#一、定积分概念" class="headerlink" title="一、定积分概念"></a>一、定积分概念</h3><h4 id="1-定义-1"><a href="#1-定义-1" class="headerlink" title="1.定义"></a>1.定义</h4><p>$$<br>\begin{align}<br>&amp;定义:设函数f(x)在区间[a,b]上有定义且有界\<br>&amp;(1)分割：将[a,b]分成n个[x_{i-1},x_{i}]小区间\<br>&amp;(2)求和：[x_{i-1},x_{i}]上取一点\xi_{i},\sum_{i&#x3D;1}^{n}{f(\xi_{i})\Delta x_i},\lambda&#x3D;\max{\Delta x_{1},\Delta x_{2},…,\Delta x_{n}}\<br>&amp;(3)取极限：若\lim_{\lambda \rightarrow 0}{\sum_{i&#x3D;1}^{n}f(\xi_{i})\Delta x}\exist,且极值不依赖区间[a,b]分发以及点\xi_{i}的取法,则称f(x)在区间[a,b]上可积,\<br>&amp;\int^{b}<em>{a}{f(x)dx}&#x3D;\lim</em>{\lambda \rightarrow 0}{f(\xi)\Delta x_{i}}<br>&amp;\<br>&amp;注解：\<br>&amp;(1)\lambda \rightarrow0 \rightarrow \nleftarrow n\rightarrow \infty\<br>&amp;(2)定积分表示一个值,与积分区间[a,b]有关,与积分变化量x无关\<br>&amp;\int_{a}^{b}{f(x)dx}&#x3D;\int_{a}^{b}{f(t)dt}\<br>&amp;(3)如果积分\int_{0}^{1}{f(x)dx}\exist,将[0,1]n等分，此时\Delta{x_{i}}&#x3D;\frac{1}{n},取\xi_{i}&#x3D;\frac{i}{n},\<br>&amp;\int_{0}^{1}f(x)dx&#x3D;\lim_{\lambda \rightarrow 0}{\sum_{i&#x3D;1}{n}{f(\xi_{i})\Delta x_{i}}}&#x3D;\lim_{n\rightarrow \infty}\sum_{i&#x3D;1}^{n}f(\frac{i}{n})\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;\int^{b}<em>{a}{f(x)dx}&#x3D;\lim</em>{\lambda \rightarrow 0}\sum^{n}<em>{i&#x3D;1}f(\xi_i)\Delta_i&#x3D;\begin{cases}&amp;\lim</em>{n\rightarrow \infty}{\sum_{i&#x3D;1}^{n}{f(a+(i-1)\frac{b-a}{n})\frac{b-a}{n}}},左侧\&amp;\lim_{n\rightarrow \infty}{\sum_{i&#x3D;1}^{n}{f(a+i\frac{b-a}{n})\frac{b-a}{n}}},右侧\\end{cases}\<br>&amp;中点：\Phi_i&#x3D;a+(i-1)\frac{b-a}{n}+\frac{b-a}{2n}\<br>\end{align}<br>$$</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20210613172601984.jpg" alt="image-20210613172601984"></p>
<p>定理：(线性)<br>$$<br>\begin{align}<br>&amp;\int[\alpha f(x)+\beta g(x)]dx&#x3D;\alpha\int f(x)dx+\beta\int g(x)dx\<br>\end{align}<br>$$<br>注解：积分无小事<br>$$<br>\begin{align}<br>&amp;\int{e^{\pm x^2}dx,\int{\frac{\sin x}{x}}}积不出来\<br>&amp;F’(x)&#x3D;f(x),x\in I,连续函数一定存在原函数，无穷多个\<br>&amp;[F(x)+C]’&#x3D;f(x)<br>\end{align}<br>$$</p>
<h4 id="2-定积分存在的充分条件"><a href="#2-定积分存在的充分条件" class="headerlink" title="2.定积分存在的充分条件"></a>2.定积分存在的充分条件</h4><p>$$<br>\begin{align}<br>&amp;若f(x)在[a,b]上连续,则\int^{b}<em>{a}{f(x)dx}必定存在\<br>&amp;若f(x)在[a,b]上有上界,且只有有限个间断点,则\int^{b}</em>{a}{f(x)dx}必定存在\<br>&amp;若f(x)在[a,b]上只有有限个第一类间断点,则\int^{b}_{a}{f(x)dx}必定存在\<br>\end{align}<br>$$</p>
<h4 id="3-定积分的几何意义"><a href="#3-定积分的几何意义" class="headerlink" title="3.定积分的几何意义"></a>3.定积分的几何意义</h4><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20210405155729433.jpg" alt="image-20210405155729433"><br>$$<br>\begin{align}<br>&amp;(1)f(x)\geqslant{0},\int_{a}^{b}{f(x)dx}&#x3D;S\<br>\end{align}<br>$$<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20210405155859329.jpg" alt="image-20210405155859329"><br>$$<br>\begin{align}<br>&amp;(2)f(x)\leqslant{0},\int_{a}^{b}{f(x)dx}&#x3D;-S\<br>\end{align}<br>$$</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20210405155556537.jpg" alt="image-20210405155556537"><br>$$<br>\begin{align}<br>&amp;(3)f(x)\geqslant{0}\cup f(x)\leqslant{0},\int_{a}^{b}{f(x)dx}&#x3D;S_1+S_3-S_2\</p>
<p>\end{align}<br>$$</p>
<p>注解：<br>$$<br>\begin{align}<br>&amp;（1）当f(x)\geq0时,定积分的几何意义是,以区间[a,b]为底,y&#x3D;f(x)为曲边的曲边梯形面积\<br>&amp;（2）定积分是一个常数，只与f和区间[a,b]有关,与积分变量用什么字母无关\<br>&amp;\int_a^b{f(x)}dx&#x3D;\int_a^b{f(t)dt}\<br>&amp;（3）\int_a^bdx&#x3D;b-a\<br>&amp;（4）\int_{a}^{a}f(x)&#x3D;0,\int_a^bf(x)dx&#x3D;-\int_b^a{f(t)}dt<br>\end{align}<br>$$</p>
<h3 id="二、定积分的性质"><a href="#二、定积分的性质" class="headerlink" title="二、定积分的性质"></a>二、定积分的性质</h3><h4 id="1-不等式性质"><a href="#1-不等式性质" class="headerlink" title="1.不等式性质"></a>1.不等式性质</h4><p>$$<br>\begin{align}<br>&amp;(1)保序性：若在区间[a,b]上f(x)\leqslant{g(x)},则\int_a^{b}{f(x)dx}\leqslant{\int_a^{b}{g(x)dx}}\<br>&amp;推论：\<br>&amp;(1)f(x)\geq0,\forall x\in[a,b],则\int_a^b{f(x)dx}\geq0\<br>&amp;(2)f(x)\geq0,\forall x\in[a,b],且[c,d]\subset[a,b],则\int_a^b{f(x)dx}\geq\int_c^d{f(x)dx}\<br>&amp;(3)|\int_a^bf(x)dx|\leq\int_a^b{|f(x)|dx}\<br>&amp;-|f|\leq f\leq |f|\Rightarrow \int_a^b-|f|\leq \int_a^bf\leq \int_a^b|f|\Rightarrow |\int_a^bf|\leq\int_a^b|f|\<br>&amp;如：x^2\leq x^3,x\in[0,1],则\int_0^1{x^3dx}\leq\int_0^1{x^2dx}\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;(4)(估值不等式)若M及m分别是f(x)在[a,b]上的最大值和最小值,\<br>&amp;则m(b-a)\leqslant{\int_a^{b}{f(x)dx}\leqslant{M(b-a)}}\<br>\end{align}<br>$$</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/geogebra-export.jpg" alt="geogebra-export"><br>$$<br>\begin{align}<br>&amp;证明：M(b-a)&#x3D;S_{AFDC}&#x3D;S_1+S_2+S_3\<br>&amp;m(b-a)&#x3D;S_{EBDC}&#x3D;S_3\<br>&amp;\int_a^{b}{f(x)dx}&#x3D;S_{ADBC}&#x3D;S_2+S_3\<br>&amp;S_3\leqslant{S_2+S_3\leqslant{S_1+S_2+S_3}}\<br>&amp;\Leftrightarrow{m(b-a)\leqslant{\int_a^{b}{f(x)dx}\leqslant{M(b-a)}}}\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;(3)|\int_a^{b}{f(x)dx}|\leqslant{\int_a^{b}{|f(x)|dx}}\<br>\end{align}<br>$$</p>
<h4 id="2-中值定理"><a href="#2-中值定理" class="headerlink" title="2.中值定理"></a>2.中值定理</h4><p>$$<br>\begin{align}<br>&amp;(1)若f(x)在[a,b]上连续,则\int_a^{b}{f(x)dx}&#x3D;f(\xi)(b-a),(a&lt;\xi&lt;b)\<br>&amp;称\frac{1}{b-a}{\int_{a}^{b}{f(x)dx}为函数y&#x3D;f(x)在区间[a,b]上的平均值}\<br>&amp;注解：F’(x)&#x3D;f(x),F(b)-F(a)&#x3D;\int_a^b{f(x)dx},f(\xi)(b-a)&#x3D;F’(\xi)(b-a)\<br>&amp;(2)若f(x),g(x)在[a,b]上连续，g(x)不变号,则\int_{a}^{b}{f(x)g(x)dx}&#x3D;f(\xi)\int_a^b{g(x)dx}\<br>\end{align}<br>$$</p>
<p>注解：<br>$$<br>\begin{align}<br>&amp;\int_0^1{\frac{x}{\sin x}}dx\<br>&amp;f(x)&#x3D;\begin{cases}&amp;\frac{x}{\sin x},x\in[0,1]\&amp;1,x&#x3D;0\\end{cases}\<br>&amp;结论：有限处点的函数不影响定积分\<br>&amp;f(x)&#x3D;{\begin{cases}&amp;x+1,[1,2]\&amp;x,[0,1]\\end{cases}}\<br>&amp;\int_0^2{f(x)dx}&#x3D;\int_0^1{xdx}+\int_1^2{(x+1)dx}\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;证明：\frac{1}{2}\leq\int_0^{\frac{1}{2}}\frac{1}{\sqrt{1-x^n}}dx\leq\frac{\pi}{6}\<br>&amp;估值积分：x\in[0,\frac{1}{2}]\<br>&amp;\<br>\end{align}<br>$$</p>
<p>例题：<br>$$<br>\begin{align}<br>&amp;1.求极限\lim_{n\rightarrow \infty}\int_0^1{\frac{x^ne^x}{1+e^x}dx}\<br>&amp;根据积分容易知道0\leq\frac{x^ne^x}{1+e^x}\leq x^n,x\in[0,1],n\in N^*\<br>&amp;用积分的保号性\<br>&amp;0\leq\int_0^1{\frac{x^ne^x}{1+e^x}dx}\leq \int_0^1{x^n}dx&#x3D;\frac{1}{n+1}\<br>&amp;用夹逼定理\<br>&amp;\lim_{n\rightarrow\infty}\frac{1}{n+1}&#x3D;0\<br>&amp;\lim_{n\rightarrow \infty}\int_0^1{\frac{x^ne^x}{1+e^x}dx}&#x3D;0\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;2.设I_1&#x3D;\int_0^{\frac{4}{\pi}}\frac{\tan x}{x}dx,I_2&#x3D;\int_0^{\frac{4}{\pi}}\frac{x}{\tan x}dx则\<br>&amp;(A)I_1&gt;I_2&gt;1(B)1&gt;I_1&gt;I_2(C)I_2&gt;I_1&gt;1(D)1&gt;I_2&gt;I_1\<br>&amp;解：用保序性a&lt;b,f(x)\leq g(x),\int_a^b f(x)\leq \int_a^b g(x)\<br>&amp;\tan x&gt;x,x\in[0,\frac{\pi}{2}]\<br>&amp;\frac{\tan x}{x}&gt;1&gt;\frac{x}{\tan x},x\in[0,\frac{\pi}{4}]\<br>&amp;根据保序性\<br>&amp;\int_0^{\frac{\pi}{4}}\frac{\tan x}{x}dx&gt;\int_0^{\frac{\pi}{4}}1dx&#x3D;\frac{\pi}{4}&gt;\int_0^{\frac{\pi}{4}}\frac{x}{\tan x},x\in[0,\frac{\pi}{4}]\<br>&amp;证：\int_0^{\frac{\pi}{4}}\frac{\tan x}{x}与1的关系\<br>&amp;积分中值定理\<br>&amp;\int_0^{\frac{\pi}{4}}\frac{\tan x}{x}&#x3D;f(\xi)(\frac{\pi}{4}-0)&#x3D;\frac{\tan \xi}{\xi}*\frac{\pi}{4},\xi\in{[0,\frac{\pi}{4}]}\<br>&amp;根据\frac{\tan x}{x}在x\in[0,\frac{\pi}{4}]上单调递增\<br>&amp;0&lt;f(\xi)&lt;\frac{4}{\pi},0&lt;\int_0^{\frac{\pi}{4}}\frac{\tan x}{x}&lt;1\<br>&amp;选(B)\<br>\end{align}<br>$$</p>
<h3 id="三、积分上限函数"><a href="#三、积分上限函数" class="headerlink" title="三、积分上限函数"></a>三、积分上限函数</h3><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20210405152647772.jpg" alt="image-20210405152647772"><br>$$<br>\begin{align}<br>&amp;如果f(x)在区间[a,b]上连续,则\Phi(x)&#x3D;\int_a^b{f(t)dt}在[a,b]上可导,且\int_a^b{f(t)dt})\<br>&amp;(\int_a^xf(t)dt)’&#x3D;f(x),(\int_a^{x^2}f(t)dt)’&#x3D;f(x^2)*2x\<br>&amp;如果f(x)在区间[a,b]上连续,\phi_1(x),\phi_2(x)为可导函数,则\Phi(x)&#x3D;\int_a^b{f(t)dt}在[a,b]上可导,且(\int_{\phi_1(x)}^{\phi_2(x)}{f(t)dt})’\<br>&amp;&#x3D;f[\phi_2(x)]*\phi_2’(x)-f[\phi_1(x)]*\phi_1’(x)&#x3D;(\int_{\phi_1(x)}^0{f(t)dt}+\int_{\phi_2(x)}^0{f(t)dt})’\<br>&amp;设函数f(x)在[-l,l]上连续,则\<br>&amp;如果f(x)为奇函数,那么\int_0^xf(t)dt必为偶函数\<br>&amp;如果f(x)为偶函数,那么\int_0^xf(t)dt必为奇函数\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;任取x\in[a,b),取\Delta x&gt;0,使x+\Delta x\in[a,b)\<br>&amp;\frac{\Delta F}{\Delta x}&#x3D;\frac{F(x+\Delta x)-F(x)}{\Delta x}&#x3D;\frac{1}{\Delta x}[\int_a^{x+\Delta x}f(t)dt-\int_a^xf(t)dt]&#x3D;\frac{1}{\Delta x}\int_x^{x+\Delta x}f(t)dt&#x3D;f(x+\sigma\Delta x)\rightarrow f(x)(\Delta x\rightarrow 0^+)\<br>\end{align}<br>$$<br>推论：<br>$$<br>\begin{align}<br>&amp;若f(x)、\phi’(x)、\psi(x)于[a,b]上连续,则\<br>&amp;(1)(\int_a^{\phi(x)}f(t)dt)’&#x3D;f(\phi(x))\phi’(x)\<br>&amp;(2)(\int_b^{\psi(x)}f(t)dt)’&#x3D;-f(\psi(x))\psi’(x)\<br>&amp;(3)(\int_{\psi(x)}^{\phi(x)}f(t)dt)’&#x3D;f(\phi(x))\phi’(x)-f(\psi(x))\psi’(x)\<br>\end{align}<br>$$<br>例题<br>$$<br>\begin{align}<br>&amp;1.设函数f(x)在R上连续,且是奇函数,则其原函数均是偶函数.当f(x)是偶函数时？是周期函数？\<br>&amp;证：\<br>&amp;令F_0(x)\int_0^xf(t)dt,x\in R\<br>&amp;F_0(-x)&#x3D;\int_0^{-x}f(t)dt\overset{t&#x3D;-u}{&#x3D;}\int_0^xf(-u)d(u)&#x3D;\int_0^xf(u)du&#x3D;F_0(x)\Rightarrow F_0(x)为偶函数\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;求变现积分导数\<br>&amp;(1)F(x)&#x3D;\int_x^{e^{-x}}f(t)dt\<br>&amp;(2)F(x)&#x3D;\int_0^{x^2}(x^2-t)f(t)dt\<br>&amp;(3)F(x)&#x3D;\int_0^{x}f(x^2-t)dt\<br>&amp;(4)设函数y&#x3D;y(x)由参数方程\begin{cases}&amp;x&#x3D;1+2t^2\&amp;y&#x3D;\int_1^{1+2\ln t}\frac{e^u}{u}du\\end{cases}(t&gt;1),求\frac{d^2y}{dx^2}|_{x&#x3D;9}\<br>&amp;解:\<br>&amp;(1)F(x)’&#x3D;(\int_x^{e^{-x}}f(t)dt)’&#x3D;f(e^{-x})(-e^{-x})-f(x)\<br>&amp;(2)F(x)’&#x3D;(\int_0^{x^2}(x^2-t)f(t)dt)’&#x3D;(\int_0^{x^2}x^2f(t)dt-\int_0^{x^2}tf(t)dt)’\<br>&amp;&#x3D;2x\int_0^{x^2}f(t)dt+x^2f(x^2)2x-x^2f(x^2)2x&#x3D;2x\int_0^{x^2}f(t)dt\<br>&amp;(3)F(x)&#x3D;\int_0^{x}f(x^2-t)dt&#x3D;-\frac{1}{2}\int_0^xf(x^2-t^2)d(x^2-t^2)\overset{u&#x3D;x^2-t^2}{&#x3D;}-\frac{1}{2}\int_0^xf(u)du\<br>&amp;F(x)’&#x3D;\frac{1}{2}f(x^2)2x&#x3D;xf(x^2)\<br>&amp;(4)\frac{dy}{dx}&#x3D;\frac{\frac{e^{1+2\ln t}}{1+2\ln t}\frac{2}{t}}{4t^2}&#x3D;\frac{e}{2(1+2\ln t)}\<br>&amp;\frac{d^2y}{dx^2}&#x3D;\frac{d(\frac{dy}{dx})}{dx}&#x3D;\frac{e}{2}(-\frac{\frac{2}{t}}{(1+2\ln t)^2})\frac{1}{4t}\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;2.求变现积分的积分:\<br>&amp;(1)设f(x)&#x3D;\int_0^x{\frac{\sin t}{\pi -t}dt},求\int_0^\pi{f(x)}dx\<br>&amp;解:\<br>&amp;\int_0^\pi{f(x)}dx&#x3D;\int_0^{\pi}\int_0^x\frac{\sin t}{\pi -t}dt\space dx\<br>&amp;&#x3D;x\int_0^x\frac{\sin t}{\pi t}|<em>0^{\pi}-\int_0^{\pi}x\frac{\sin x}{\pi -x}dx\<br>&amp;&#x3D;\pi\int_0^{\pi}\frac{\sin x}{\pi t}+\int_0^{\pi}\frac{[(\pi-x)-\pi]\sin x}{\pi-x}dx&#x3D;\int_0^{\pi}\sin xdx&#x3D;2\<br>&amp;(2)\lim</em>{x\rightarrow\infty}{\frac{(\int_0^x{e^{t^2}}dt)^2}{\int_0^xe^{2t^2}dt}}&#x3D;\lim_{x\rightarrow\infty}{\frac{(2\int_0^{x}e^{t^2}dt)e^{x^2}}{e^{2x^2}}}&#x3D;\lim_{x\rightarrow\infty}\frac{2\int_0^{x}e^{t^2}}{e^{x^2}}&#x3D;\lim_{x\rightarrow\infty}\frac{1}{2x}&#x3D;0\<br>\end{align}<br>$$ { }</p>
<p>$$<br>\begin{align}<br>&amp;(3)设f(x)连续,\phi(x)&#x3D;\int_0^1{f(tx)dt},且\lim_{x\rightarrow0}\frac{f(x)}{x}&#x3D;A(常数),求\phi’(x)并讨论\phi’(x)在x&#x3D;0处的连续性\<br>&amp;当x\neq0时\<br>&amp;令u&#x3D;tx,t\in[0,1],u&#x3D;tx\in[0,x],\phi(x)&#x3D;\int_0^1f(tx)dt\overset{tx&#x3D;u}{&#x3D;}\int_0^x{f(u)d(\frac{u}{x})}&#x3D;\frac{\int_0^xf(u)du}{x}\<br>&amp;\phi’(x)&#x3D;\frac{xf(x)-\int_0^xf(u)du}{x^2}\<br>&amp;当x&#x3D;0时,f(0)&#x3D;0,\phi(0)&#x3D;f(0)&#x3D;0,\phi’(0)&#x3D;\lim_{x\rightarrow0}\frac{\phi(x)\phi(0)}{x-0}&#x3D;\lim_{x\rightarrow0}\frac{\int_0^xf(u)du}{x^2}&#x3D;\lim_{x\rightarrow 0}\frac{f(x)}{2x}&#x3D;\frac{1}{2}A\<br>&amp;\lim_{x\rightarrow0}\phi’(x)&#x3D;\lim_{x\rightarrow 0}{\frac{xf(x)-\int_0^xf(u)du}{x^2}}&#x3D;A-\frac{1}{2}A&#x3D;\frac{1}{2}A&#x3D;\phi’(0)\Leftrightarrow\phi’(x)在x&#x3D;0处连续\<br>\end{align}<br>$$</p>
<p>注解：<br>$$<br>\begin{align}<br>&amp;注意变限积分进行正逆运算时上下限的映射\<br>&amp;例如F(x)&#x3D;\int_0^x{f(t)dt}\overset{t&#x3D;-u}{&#x3D;}\int_{-a}^{x}f(-u)d(-u)\<br>\end{align}<br>$$</p>
<h3 id="四、定积分的计算"><a href="#四、定积分的计算" class="headerlink" title="四、定积分的计算"></a>四、定积分的计算</h3><h4 id="1-牛顿莱布尼茨公式"><a href="#1-牛顿莱布尼茨公式" class="headerlink" title="1.牛顿莱布尼茨公式"></a>1.牛顿莱布尼茨公式</h4><p>$$<br>\int_a^bf(x)dx&#x3D;F(x)|_a^b&#x3D;F(b)-F(a)<br>$$</p>
<h4 id="2-换元积分法"><a href="#2-换元积分法" class="headerlink" title="2.换元积分法"></a>2.换元积分法</h4><p>$$<br>\int_a^bf(x)dx&#x3D;\int_\alpha^\beta{f(\Phi(t))\Phi’(t)dt}<br>$$</p>
<h4 id="3-分部积分法-1"><a href="#3-分部积分法-1" class="headerlink" title="3.分部积分法"></a>3.分部积分法</h4><p>$$<br>\int_a^budv&#x3D;uv|_a^b-\int_a^bvdu<br>$$</p>
<h4 id="4-奇偶性和周期性"><a href="#4-奇偶性和周期性" class="headerlink" title="4.奇偶性和周期性"></a>4.奇偶性和周期性</h4><p>$$<br>\begin{align}<br>&amp;直接使用奇偶性周期性定义证明\<br>&amp;(1)设f(x)为[-a,a]上的连续函数(a&gt;0),则\<br>&amp;\int_{-a}{a}f(x)dx&#x3D;\begin{cases}0,&amp;f(x)奇函数\2\int_0^af(x)dx,&amp;f(x)偶函数\end{cases}\<br>&amp;证：\int_{-a}^0{f(x)dx}\overset{x&#x3D;-t}{&#x3D;}\int_0^a{f(-t)d(-t)}&#x3D;-\int_{0}^{a}f(t)d(t)&#x3D;-\int_0^a{f(x)dx}\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;(2)设f(x)是以T为周期的连续函数,则对\forall A，有\int_a^{a+T}f(x)&#x3D;\int_0^T{f(x)dx}\<br>&amp;\int_a^{a+T}f(x)dx\overset{x&#x3D;a+t}{&#x3D;}\int_0^T{f(a+t)d(a+t)}&#x3D;\int_0^{a+t}f(a+t)dt\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;\Phi:x\in[a,b]\rightarrow y\in[c,d],令\frac{x-a}{b-a}&#x3D;\frac{y-c}{d-c},y&#x3D;c+\frac{d-c}{b-a}(x-a)\<br>\end{align}\<br>$$</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20210617160041903.jpg" alt="image-20210617160041903"></p>
<h4 id="5-奇偶函数积分后的奇偶性-奇偶函数求导后的奇偶性"><a href="#5-奇偶函数积分后的奇偶性-奇偶函数求导后的奇偶性" class="headerlink" title="5.奇偶函数积分后的奇偶性(奇偶函数求导后的奇偶性)"></a>5.奇偶函数积分后的奇偶性(奇偶函数求导后的奇偶性)</h4><h5 id="1-奇偶函数求导后的奇偶性"><a href="#1-奇偶函数求导后的奇偶性" class="headerlink" title="1.奇偶函数求导后的奇偶性"></a>1.奇偶函数求导后的奇偶性</h5><p>$$<br>\begin{align}<br>&amp;(1)f(x)为奇函数:\<br>&amp;f(-x)&#x3D;-f(x)\<br>&amp;\Leftrightarrow f’(-x)(-1)&#x3D;-f’(x)\<br>&amp;\Leftrightarrow f’(-x)&#x3D;f’(x)\<br>&amp;\Leftrightarrow f’(x)为偶函数\<br>&amp;(2)f(x)为偶函数:\<br>&amp;f(-x)&#x3D;f(x)\<br>&amp;\Leftrightarrow f’(-x)&#x3D;f’(x)\<br>&amp;\Leftrightarrow f’(-x)(-1)&#x3D;f’(x)\<br>&amp;\Leftrightarrow f’(-x)&#x3D;-f’(x)\<br>&amp;\Leftrightarrow f’(x)为奇函数\<br>\end{align}<br>$$</p>
<h5 id="2-奇偶函数求积分后的奇偶性"><a href="#2-奇偶函数求积分后的奇偶性" class="headerlink" title="2.奇偶函数求积分后的奇偶性"></a>2.奇偶函数求积分后的奇偶性</h5><p>$$<br>\begin{align}<br>&amp;设F(x)为f(x)的原函数\<br>&amp;(1)f(x)为奇函数:\<br>&amp;f(-x)&#x3D;-f(x)\<br>&amp;\Leftrightarrow \int f(-x)dx&#x3D;-\int f(x)dx\<br>&amp;\Leftrightarrow -\int f(-x)d(-x)&#x3D;-\int f(x)dx\<br>&amp;\Leftrightarrow F(-x)&#x3D;F(x)\<br>&amp;\Leftrightarrow F(x)为偶函数\<br>&amp;(2)f(x)为偶函数:\<br>&amp;f(-x)&#x3D;f(x)\<br>&amp;\Leftrightarrow \int f(-x)dx&#x3D;\int f(x)dx\<br>&amp;\Leftrightarrow -\int f(-x)d(-x)&#x3D;\int f(x)dx\<br>&amp;\Leftrightarrow F(-x)&#x3D;-F(x)\<br>&amp;\Leftrightarrow F(x)为奇函数\<br>\end{align}<br>$$</p>
<h5 id="3-奇偶函数复合后的奇偶性"><a href="#3-奇偶函数复合后的奇偶性" class="headerlink" title="3.奇偶函数复合后的奇偶性"></a>3.奇偶函数复合后的奇偶性</h5><p>$$<br>\begin{align}<br>&amp;\exist f(x),g(x),F(x)&#x3D;f(g(x))\<br>&amp;设f(x)为奇函数\<br>&amp;(1)g(x)为偶函数\<br>&amp;F(-x)&#x3D;f(g(-x))&#x3D;f(g(x))&#x3D;F(x),F(x)为偶函数\<br>&amp;(2)g(x)为奇函数\<br>&amp;F(-x)&#x3D;f(g(-x))&#x3D;f(-g(x))&#x3D;-f(g(x))&#x3D;-F(x),F(x)为奇函数\<br>&amp;设f(x)为偶函数\<br>&amp;(1)g(x)为奇函数\<br>&amp;F(-x)&#x3D;f(g(-x))&#x3D;f(g(x))&#x3D;F(x),F(x)为偶函数\<br>&amp;(2)g(x)为偶函数\<br>&amp;F(-x)&#x3D;f(g(-x))&#x3D;f(g(x))&#x3D;F(x),F(x)为偶函数\<br>&amp;注解:外偶全偶,外奇奇偶\<br>\end{align}<br>$$</p>
<p>例题：<br>$$<br>\begin{align}<br>&amp;1.设M&#x3D;\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}{\frac{\sin x}{1+x^2}\cos^4xdx},N&#x3D;\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}{(\sin x^3+\cos^4x)dx},P&#x3D;\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}(x^2\sin^3x-\cos^4x)dx,则\<br>&amp;(A)N&lt;P&lt;M(B)M&lt;P&lt;N(C)N&lt;M&lt;P(D)P&lt;M&lt;N\<br>&amp;根据对称性判断\<br>&amp;M:f_M(x)为奇函数，F_M(x)为偶函数\<br>&amp;N:N&#x3D;\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}{(\sin x^3+\cos^4x)dx}&#x3D;\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\sin ^3xdx+\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos ^4xdx\<br>&amp;\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\sin ^3xdx&#x3D;0,\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos ^4xdx\geq 0,\Rightarrow N\geq 0\<br>&amp;P:P&#x3D;\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}(x^2\sin^3x-\cos^4x)dx&#x3D;\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}x^2\sin^3xdx-\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos^4xdx\<br>&amp;\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}x^2\sin^3xdx&#x3D;0,\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos^4xdx\geq0,\Rightarrow P\leq0\<br>&amp;\Leftrightarrow P&lt;M&lt;N,\space\space选(D)\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;2.设f(x)&#x3D;\begin{cases}&amp;kx,0\leq x\leq \frac{1}{2}a\&amp;c,\frac{1}{2}a&lt;x\leq a\\end{cases},求F(x)&#x3D;\int_0^xf(t)dt,x\in[0,a]\<br>&amp;F(x)&#x3D;\begin{cases}&amp;\int_0^xktdt&#x3D;\frac{1}{2}kt^2|<em>0^x&#x3D;\frac{1}{2}kx^2,0\leq x\leq \frac{1}{2}a\&amp;\int_0^{\frac{1}{2}a}ktdt+\int</em>{\frac{1}{2}a}^c cdt&#x3D;\frac{1}{8}ka^2+c^2-\frac{1}{2}ac,\frac{1}{2}a&lt;x\leq a\\end{cases}\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>&amp;3.证明：\int_0^{2\pi}f(|\cos x|)dx&#x3D;4\int_0^{\frac{\pi}{2}}f(|\cos x|)dx\<br>\end{align}<br>$$</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/e0e1f27ff16b0cf00a8f3d155bfc3423.jpg" alt="1111"></p>
<h4 id="6-已有公式"><a href="#6-已有公式" class="headerlink" title="6.已有公式"></a>6.已有公式</h4><p>$$<br>\begin{align}<br>&amp;(1)\int_0^{\frac{\pi}{2}}{\sin^nxdx&#x3D;\int_0^{\frac{\pi}{2}}\cos^n xdx&#x3D;\begin{cases}\frac{n-1}{n}<em>\frac{n-3}{n-2}</em>…*\frac{1}{2}*\frac{\pi}{2},&amp;n为偶数\\frac{n-1}{n}<em>\frac{n-3}{n-2}</em>…*\frac{2}{3},&amp;n为大于1的奇数\\end{cases}}\<br>&amp;(2)\int_0^{\pi}xf(\sin x)dx&#x3D;\frac{\pi}{2}\int_0^{\pi}f(\sin x)dx(f(x)为连续函数)\<br>\end{align}<br>$$</p>
<h4 id="7-与定积分有关的证明"><a href="#7-与定积分有关的证明" class="headerlink" title="7.与定积分有关的证明"></a>7.与定积分有关的证明</h4><h4 id="8-经典例题："><a href="#8-经典例题：" class="headerlink" title="8.经典例题："></a>8.经典例题：</h4><h5 id="例题1"><a href="#例题1" class="headerlink" title="例题1:"></a>例题1:</h5><p>$$<br>\begin{align}<br>&amp;\lim_{n\rightarrow \infty}{(\frac{1}{n+1}+\frac{1}{n+2}+…+\frac{1}{n+n})}\<br>&amp;法1：夹逼定理+基本不等式\<br>&amp;\frac{1}{1+x}&lt;\ln(x+1)&lt;x\<br>&amp;令x&#x3D;\frac{1}{n}\<br>&amp;得\frac{1}{n+1}&#x3D;\frac{\frac{1}{n}}{\frac{1}{n}+1}&lt;\ln(\frac{1}{n}+1)&#x3D;\ln(n+1)-\ln(n)&lt;\frac{1}{n}\<br>&amp;得\frac{1}{n+2}&lt;ln(n+2)-ln(n+1)&lt;\frac{1}{n+1}\<br>&amp;得\frac{1}{n+n}&lt;\ln(n+n)-\ln(n+n-1)&lt;\frac{1}{n+n-1}\<br>&amp;得\frac{1}{n+1}+\frac{1}{n+2}+…+\frac{1}{n+n}&lt;ln(2n)-ln(n)&#x3D;ln2\<br>&amp;法2：\lim_{n\rightarrow \infty}{(\frac{1}{n+1}+\frac{1}{n+2}+…+\frac{1}{n+n})}中\<br>&amp;\frac{1}{n+1}中n为主体，1为变体\<br>&amp;\frac{变体}{主体}\rightarrow^{n \rightarrow{\infty}}\begin{cases}0,次(夹逼定理)\A\neq 0,同(定积分)\end{cases}\<br>&amp;\lim_{\lambda \rightarrow 0}{\sum_{i&#x3D;1}^{n}{f(\xi_i)\Delta x_i}&#x3D;\lim_{n\rightarrow \infty}\frac{1}{n}\sum_{i&#x3D;1}^{n}f(\xi_i)(b-a)}&#x3D;\int_0^1\frac{1}{1+x}&#x3D;\ln(1+x)|_{0}^{1}&#x3D;\ln2\<br>\end{align}<br>$$</p>
<h5 id="例题2"><a href="#例题2" class="headerlink" title="例题2"></a>例题2</h5><p>$$<br>\begin{align}<br>&amp;设f(x)&#x3D;\int_0^{\pi}{\frac{\sin x}{\pi-t}dt},计算\int_0^{\pi}f(x)dx.\<br>&amp;法1：分部积分+换元法\<br>&amp;原式&#x3D;xf(x)|_0^{\pi}-\int_0^{\pi}{\frac{x\sin x}{\pi-x}dx}\<br>&amp;&#x3D;\pi{\int_0^{\pi}{\frac{\sin{t}}{\pi-t}dt}-\int_0^{\pi}{\frac{x\sin x}{\pi-x}}dx}\<br>&amp;&#x3D;\int_0^{\pi}{\frac{(\pi-x)\sin x}{\pi-x}dx}&#x3D;2\<br>&amp;法2：\<br>&amp;原式&#x3D;\int_0^\pi{f(x)d(x-{\pi})}&#x3D;(x-\pi)f(x)|_0^{\pi}-\int_0^{\pi}{\frac{(x-\pi)\sin x}{\pi-x}dx}&#x3D;2\<br>&amp;法3：二重积分转化为累次积分\<br>&amp;原式&#x3D;\int_0^{\pi}{\int_0^{\pi}\frac{x\sin t}{\pi-t}dt}dx\<br>\end{align}<br>$$</p>
<h5 id="例题3"><a href="#例题3" class="headerlink" title="例题3"></a>例题3</h5><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/AN%L6IJ6TF[%1UB3OUWMRCR.jpg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/123.jpg" alt="123"><br>$$<br>\begin{align}<br>&amp;法1：构造辅助函数\<br>&amp;根据题意f(1)&#x3D;f(-1)&#x3D;1,f(0)&#x3D;-1\Rightarrow f(x)为偶函数,f最低点函数值为-1\<br>&amp;可以构造符合题意的辅助函数f(x)&#x3D;2x^2-1\<br>&amp;法2：根据函数的性质直接判断<br>\end{align}<br>$$</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20210408160543049.jpg" alt="image-20210408160543049"></p>
<h5 id="例题4"><a href="#例题4" class="headerlink" title="例题4"></a>例题4</h5><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/Q8%7DOT_25(HC79%5BS_21)AZZK.jpg" alt="img"></p>
<p>$$<br>\begin{align}<br>&amp;因为\lim_{x\rightarrow 0}{\frac{ax-\sin x}{\int_b^x{\frac{\ln{1+t^3}}{t}dt}}}&#x3D;c(c\neq 0)\<br>&amp;所以\lim_{x\rightarrow 0}{ax-\sin x}&#x3D;0并且\lim_{x \rightarrow 0}{\int_b^x{\frac{\ln{1+t^3}}{t}dt}}&#x3D;0\<br>&amp;化简,使用洛必达法则上下求导\<br>&amp;\lim_{x\rightarrow 0}{\frac{ax-\sin x}{\int_b^x{\frac{\ln{1+t^3}}{t}dt}}}&#x3D;\lim_{x\rightarrow 0}{\frac{a-\cos x}{\frac{\ln{1+x^3}}{x}}}&#x3D;\lim_{x\rightarrow 0}{\frac{a-\cos x}{x^2}}\<br>&amp;\Rightarrow a&#x3D;1,c&#x3D;\frac{1}{2},b&#x3D;0\<br>\end{align}<br>$$</p>
<h2 id="反常积分"><a href="#反常积分" class="headerlink" title="反常积分"></a>反常积分</h2><h3 id="一、无穷区间上的反常积分"><a href="#一、无穷区间上的反常积分" class="headerlink" title="一、无穷区间上的反常积分"></a>一、无穷区间上的反常积分</h3><p>$$<br>\begin{align}<br>&amp;(1)\int_a^{+\infty}{f(x)}dx&#x3D;\lim_{t\rightarrow +\infty}{\int_{a}^{t}f(x)dx}\<br>&amp;(2)\int_{-\infty}^{b}{f(x)}dx&#x3D;\lim_{t\rightarrow -\infty}{\int_{t}^{b}f(x)dx}\<br>&amp;(3)\int_{-\infty}^{0}{f(x)}dx和{\int_{0}^{+\infty}f(x)dx}都收敛,则{\int_{-\infty}^{+\infty}f(x)dx}收敛\<br>&amp;且{\int_{-\infty}^{+\infty}f(x)dx}&#x3D;\int_{-\infty}^{0}{f(x)}dx+{\int_{0}^{+\infty}f(x)dx}\<br>&amp;如果其中一个发散,结果也发散\<br>&amp;常用结论：\int_a^{+\infty}{\frac{1}{x^p}dx}\begin{cases}&amp;p&gt;1,收敛\&amp;p\leq1 ,发散\\end{cases},(a&gt;0)\<br>\end{align}<br>$$</p>
<h3 id="二、无界函数的反常积分"><a href="#二、无界函数的反常积分" class="headerlink" title="二、无界函数的反常积分"></a>二、无界函数的反常积分</h3><p>$$<br>\begin{align}<br>&amp;如果函数f(x)在点a的任一领域内都无界,那么点a为函数f(x)的瑕点(也称为无界点).无界函数的反常积分也成为瑕积分\<br>&amp;(1)设函数f(x)在(a,b]上连续,点a为f(x)的瑕点.如果极限\lim_{t\rightarrow a^+}{\int_{t}^{b}{f(x)dx}}\exist,\<br>&amp;则称此极限为函数f(x)在区间[a,b]上的反常区间,记作\int_{a}^{b}f(x)dx,即\int_{a}^{b}f(x)dx&#x3D;\lim_{t\rightarrow a^+}{\int_{t}^{b}{f(x)dx}}\<br>&amp;这时也称反常积分\int_a^b{f(x)dx}收敛,如果上述极限不存在，则反常积分\int_a^b{f(x)dx}发散\<br>&amp;(2)设函数f(x)在[a,b)上连续,点b为函数f(x)的瑕点,则可以类似定义函数f(x)在区间[a,b]上的反常积分\int_a^bf(x)dx&#x3D;\lim_{t\rightarrow b^-}{\int_a^tf(x)dx}\<br>&amp;设函数f(x)在[a,b]上除点c(a&lt;c&lt;b)外连续,点c为函数f(x)的瑕点,如果反常积分\int_a^c{f(x)dx}和\int_c^b{f(x)dx}都收敛\<br>&amp;则称反常积分\int_a^b{f(x)dx}收敛,且\int_a^b{f(x)dx}&#x3D;\int_a^c{f(x)dx}+\int_c^b{f(x)dx}\<br>&amp;如果至少一个发散,则称\int_a^b{f(x)dx}发散\<br>&amp;常用结论：\<br>&amp;\int_a^b{\frac{1}{(x-a)^p}}\begin{cases}&amp;p&lt;1,收敛\&amp;p\geq 1,发散\\end{cases}\<br>&amp;\int_a^b{\frac{1}{(x-a)^p}}\begin{cases}&amp;p&lt;1,收敛\&amp;p\geq 1,发散\\end{cases}\<br>\end{align}<br>$$</p>
<h3 id="三、例题"><a href="#三、例题" class="headerlink" title="三、例题"></a>三、例题</h3><h5 id="例题1-1"><a href="#例题1-1" class="headerlink" title="例题1"></a>例题1</h5><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/12edsadada.jpg" alt="12edsadada"><br>$$<br>\begin{align}<br>&amp;\int\frac{1}{\ln^{\alpha}x}d(\ln x)\rightarrow^{\ln x&#x3D;u}\int{\frac{du}{u^{\alpha+1}}}\begin{cases}&amp;{\alpha-1&lt; 1}\&amp;{\alpha+1&gt;1}\\end{cases}\Rightarrow 0&lt;\alpha&lt;2\<br>\end{align}<br>$$</p>
<h2 id="定积分的应用"><a href="#定积分的应用" class="headerlink" title="定积分的应用"></a>定积分的应用</h2><h4 id="微元法"><a href="#微元法" class="headerlink" title="微元法"></a>微元法</h4><p>$$<br>\begin{align}<br>&amp;\<br>\end{align}<br>$$</p>
<h3 id="一、几何应用"><a href="#一、几何应用" class="headerlink" title="一、几何应用"></a>一、几何应用</h3><h4 id="1-平面图形的面积"><a href="#1-平面图形的面积" class="headerlink" title="1.平面图形的面积"></a>1.平面图形的面积</h4><p>$$<br>\begin{align}<br>&amp;(1)若平面域D由曲线y&#x3D;f(x),y&#x3D;g(x)(f(x)\geq g(x)),x&#x3D;a,x&#x3D;b(a&lt;b)所围成,则平面域D的面积为\<br>&amp;S&#x3D;\int_a^b{[f(x)-g(x)]dx}\<br>&amp;(2)若平面域D由曲线由\rho&#x3D;\rho(\theta),\theta&#x3D;\alpha,\theta&#x3D;\beta(\alpha&lt;\beta)所围成,则其面积为S&#x3D;\frac{1}{2}\int_{\alpha}^{\beta}{\rho^2(\theta)d\theta}<br>\end{align}<br>$$</p>
<h4 id="2-旋转体的体积"><a href="#2-旋转体的体积" class="headerlink" title="2.旋转体的体积"></a>2.旋转体的体积</h4><p>$$<br>\begin{align}<br>&amp;若区域D由曲线y&#x3D;f(x)(f(x)\geq 0)和直线x&#x3D;a,x&#x3D;b(0\leq a&lt;b)及x轴所围成,则\<br>&amp;(1)区域D绕x轴旋转一周所得到的旋转体体积为V_x&#x3D;\pi\int_a^b{f^2(x)dx}\<br>&amp;(2)区域D绕y轴旋转一周所得到的旋转体体积为V_y&#x3D;2\pi\int_a^b{xf(x)dx}\<br>&amp;(3)区域D绕y&#x3D;kx+b轴旋转一周所得到的旋转体体积为V&#x3D;2\pi\int_D\int{r(x,y)d\sigma}\<br>&amp;例如：求y&#x3D;x,y&#x3D;x^2在第一象限的封闭图形绕转轴的体积\<br>\end{align}<br>$$</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/U1%7D97(ZE)HIN4FCVUKI$%5DZB.jpg" alt="img"><br>$$<br>\begin{align}<br>&amp;V_x&#x3D;2\pi\int_D\int yd\sigma&#x3D;2\pi\int_0^1{dx}\int_{x^2}^{x}ydy\<br>&amp;V_y&#x3D;2\pi\int_D\int xd\sigma&#x3D;2\pi\int_0^1{dx}\int_{x^2}^{x}xdy\<br>&amp;V_{x&#x3D;1}&#x3D;2\pi\int_D\int (1-x)d\sigma\<br>&amp;V_{y&#x3D;2}&#x3D;2\pi\int_D\int (2-y)d\sigma\<br>\end{align}<br>$$</p>
<h4 id="3-曲线弧长"><a href="#3-曲线弧长" class="headerlink" title="3.曲线弧长"></a>3.曲线弧长</h4><p>$$<br>\begin{align}<br>&amp;(1)C:y&#x3D;y(x),a\leq x\leq b,s&#x3D;\int_a^b{\sqrt{1+y’^2}dx}\<br>&amp;(2)C:\begin{cases}&amp;x&#x3D;x(t)\&amp;y&#x3D;y(t)\\end{cases},\alpha \leq t\leq  \beta,s&#x3D;\int_{\alpha}^{\beta}{\sqrt{x’^2+y’^2}dx}\<br>&amp;(3)C:\rho&#x3D;\rho(\theta),\alpha \leq \theta\leq  \beta,s&#x3D;\int_{\alpha}^{\beta}{\sqrt{\rho^2+\rho’^2}dx}\<br>\end{align}<br>$$</p>
<h4 id="4-旋转体侧面积"><a href="#4-旋转体侧面积" class="headerlink" title="4.旋转体侧面积"></a>4.旋转体侧面积</h4><p>$$<br>\begin{align}<br>&amp;曲线y&#x3D;f(x)(f(x)\geq 0)和直线x&#x3D;a,x&#x3D;b(0\leq a&lt;b)及x轴所围成的区域绕x轴旋转所得到的旋转体的侧面积为\<br>&amp;S&#x3D;2\pi\int_a^b{f(x)\sqrt{1+f’^2(x)}dx}\<br>\end{align}<br>$$</p>
<h3 id="二、物理应用"><a href="#二、物理应用" class="headerlink" title="二、物理应用"></a>二、物理应用</h3><h4 id="1-压力"><a href="#1-压力" class="headerlink" title="1.压力"></a>1.压力</h4><h4 id="2-变力做功"><a href="#2-变力做功" class="headerlink" title="2.变力做功"></a>2.变力做功</h4><h4 id="3-引力（较少考）"><a href="#3-引力（较少考）" class="headerlink" title="3.引力（较少考）"></a>3.引力（较少考）</h4><h4 id="例题1-2"><a href="#例题1-2" class="headerlink" title="例题1"></a>例题1<img src="https://raw.githubusercontent.com/ebxeax/images/main/X2PPU~%@L@NM4Y%7DW6GZTT_R.jpg" alt="img"></h4><p>$$<br>\begin{align}<br>&amp;分析题意可知,该容器由x^2+y^2&#x3D;1的圆和x^2+(y-1)^2&#x3D;1的偏心圆组成\<br>&amp;根据图像的对称性可以避免不同表达式带来的困难\<br>&amp;对圆的小带子进行积分，带子长度为x，积分区间为-1到\frac{1}{2}，\int_{-1}^{\frac{1}{2}}{\pi x^2dy}\<br>&amp;由于图像的对称性，将积分结果乘二\<br>&amp;(1)V&#x3D;2\pi\int_{-1}^{\frac{1}{2}}{x^2}dy&#x3D;2\pi\int_{-1}^{\frac{1}{2}}{(1-y^2)dy}&#x3D;\frac{9\pi}{4}\<br>\end{align}<br>$$</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202021-04-19%20203327.jpg" alt="屏幕截图 2021-04-19 203327"><br>$$<br>\begin{align}<br>&amp;(2)W&#x3D;F<em>S&#x3D;G</em>S&#x3D;mg<em>S&#x3D;\rho VSg\<br>&amp;上部为W_1&#x3D;\int_{\frac{1}{2}}^{2}(2y-y^2)(2-y)dy</em>\rho g\<br>&amp;下部为W_2&#x3D;\int^{\frac{1}{2}}_{-1}(1-y^2)(2-y)dy*\rho g\<br>&amp;W&#x3D;W_1+W_2\<br>\end{align}<br>$$<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202021-04-19%20204534.jpg" alt="屏幕截图 2021-04-19 204534"></p>
<h4 id="例题2-1"><a href="#例题2-1" class="headerlink" title="例题2"></a>例题2</h4><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/image-20210419211039410.jpg" alt="image-20210419211039410"><br>$$<br>\begin{align}<br>&amp;F_p&#x3D;P<em>A&#x3D;\rho gh</em>A\<br>&amp;将图像分为上部和下部，上部为矩形区域和下部的抛物线围成的面积区域，对其进行依次求解\<br>&amp;P_1&#x3D;2\rho gh\int_1^{h+1}{h+1-y}dy&#x3D;\rho gh^2\<br>&amp;P_2&#x3D;2\rho gh\int_0^1{(h+1-y)\sqrt{y}dy&#x3D;4\rho g(\frac{1}{3}h+\frac{2}{15})}\<br>&amp;\frac{P_1}{P_2}&#x3D;\frac{4}{5}\Rightarrow h&#x3D;2,h&#x3D;-\frac{1}{3}(舍去)<br>\end{align}<br>$$</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/1618837868(1).jpg" alt="1618837868(1)"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/HM_005_%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86%E3%80%81%E5%AE%9A%E7%A7%AF%E5%88%86%E4%B8%8E%E5%8F%8D%E5%B8%B8%E7%A7%AF%E5%88%86/" data-id="clkqazu9a001idlbi8b6wcmmm" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-OS_001_introduction-Operator-System" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/OS_001_introduction-Operator-System/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="第一章-计算机系统概述"><a href="#第一章-计算机系统概述" class="headerlink" title="第一章 计算机系统概述"></a>第一章 计算机系统概述</h1><h2 id="并发、共享、虚拟、异步"><a href="#并发、共享、虚拟、异步" class="headerlink" title="并发、共享、虚拟、异步"></a>并发、共享、虚拟、异步</h2><p>没有并发和共享，就没有虚拟和异步</p>
<p>并发和共享互为存在条件</p>
<p>只有系统有并发性，才能导致异步性</p>
<h2 id="命令接口"><a href="#命令接口" class="headerlink" title="命令接口"></a>命令接口</h2><p>联机：交互式</p>
<p>脱机：批处理</p>
<h2 id="程序接口"><a href="#程序接口" class="headerlink" title="程序接口"></a>程序接口</h2><p>系统调用（广义指令）</p>
<h2 id="操作系统用作扩充机器"><a href="#操作系统用作扩充机器" class="headerlink" title="操作系统用作扩充机器"></a>操作系统用作扩充机器</h2><p>没有任何软件支持的计算机称为裸机</p>
<p>覆盖了软件的机器称为<strong>扩充机器</strong>或<strong>虚拟机</strong></p>
<h2 id="操作系统发展"><a href="#操作系统发展" class="headerlink" title="操作系统发展"></a>操作系统发展</h2><p>手工操作系统：独占全机、CPU等待手工操作</p>
<p>单道批处理操作系统：解决人机矛盾、CPU和I&#x2F;O设备速率不匹配、每次主机内存仅存放一道作业</p>
<p>多道批处理操作系统：资源利用率高、用户响应时间长、不提供人机交互</p>
<p>分时操作系统：同时、交互、独立、及时</p>
<p>实时操作系统：及时、可靠</p>
<p>网络操作系统</p>
<p>分布式计算机系统</p>
<p>个人操作系统</p>
<h2 id="操作系统运行机制"><a href="#操作系统运行机制" class="headerlink" title="操作系统运行机制"></a>操作系统运行机制</h2><p>CPU的状态划分为用户态、核心态</p>
<h2 id="中断和异常"><a href="#中断和异常" class="headerlink" title="中断和异常"></a>中断和异常</h2><p>中断（外中断）：CPU执行指令意外的事件【设备发出的I&#x2F;O结束中断】</p>
<p>异常（内中断、陷入）：CPU执行指令内部的事件【非法操作码、地址越界、算术溢出、虚拟存储系统的缺页、陷入指令】</p>
<h2 id="中断处理过程"><a href="#中断处理过程" class="headerlink" title="中断处理过程"></a>中断处理过程</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">关中断1--&gt;保存断点</span><br><span class="line">保存断点--&gt;中断服务程序寻址</span><br><span class="line">中断服务程序寻址--&gt;保存现场和屏蔽字</span><br><span class="line">保存现场和屏蔽字--&gt;开中断1</span><br><span class="line">开中断1--&gt;执行中断服务程序</span><br><span class="line">执行中断服务程序--&gt;关中断2</span><br><span class="line">关中断2--&gt;恢复现场和屏蔽字</span><br><span class="line">恢复现场和屏蔽字--&gt;开中断2</span><br><span class="line">开中断2--&gt;中断返回</span><br></pre></td></tr></table></figure>

<p><img src="/../imgs/OS/os_1.png" alt="os_1"></p>
<h2 id="系统调用"><a href="#系统调用" class="headerlink" title="系统调用"></a>系统调用</h2><p>用户程序中调用操作系统提供的子功能</p>
<p>程序员可以使用高级语言，估计又要可以调用库函数，有的库函数封装了系统调用</p>
<p>用户进程执行 调用系统调用 <strong>进入核心态</strong> 执行系统调用 <strong>退出核心态</strong> 从系统调用返回</p>
<h2 id="大内核-微内核"><a href="#大内核-微内核" class="headerlink" title="大内核 微内核"></a>大内核 微内核</h2><p>大内核：内核代码庞大、结构混乱、难以维护、提供高性能系统服务</p>
<p>微内核：分离内核与服务、频繁切换用户核心态、操作系统执行开销大、内核内容少、方便维护</p>
<h1 id="第二章-进程管理"><a href="#第二章-进程管理" class="headerlink" title="第二章 进程管理"></a>第二章 进程管理</h1><p>进程:程序段、数据段、PCB进程控制块</p>
<p>进程映像是静态的，进程是动态的</p>
<p>动态、并发、独立、异步、结构</p>
<h2 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h2><p>运行、就绪、阻塞、创建、结束</p>
<h2 id="进程通信"><a href="#进程通信" class="headerlink" title="进程通信"></a>进程通信</h2><p>共享存储（需要同步互斥工具PV操作、低级：基于共享数据结构、高级：基于存储区）</p>
<p>消息传递（进程间数据交换以格式化消息为单位、直接&#x2F;间接）</p>
<p>管道通信（pipe文件用于连接一个读进程和写进程通信、半双工、全双工需要两条管道）</p>
<h2 id="线程-多线程"><a href="#线程-多线程" class="headerlink" title="线程 多线程"></a>线程 多线程</h2><p>引入进程目的：更好的使多道程序<strong>并发</strong>执行</p>
<p>引入线程目的：减小程序在并发执行付出的时空开销，提高并发性能</p>
<p>线程：线程ID、程序计数器、寄存器结合、堆栈组合、三态【就绪、阻塞、运行】</p>
<p>轻量实体，无系统资源，唯一ID和线程控制块</p>
<p>用户级线程ULT：操作系统意识不到ULT的存在，有关线程管理由应用程序完成</p>
<p>内核级线程KLT：有关线程管理由内核完成</p>
<p>多线程模型：多对一、一对一、多对多（m&lt;&#x3D;n）</p>
<h2 id="处理机调度"><a href="#处理机调度" class="headerlink" title="处理机调度"></a>处理机调度</h2><p>调度层次：作业调度（高）、内存调度（中）、进程调度（低）</p>
<p>不能进行进程调度与切换：处理中断、进程处于内核态、其他需要完全屏蔽中断的原子操作</p>
<p>调度方式：非剥夺调度（非抢占方式、适用于大多批处理系统、不能用于分时系统和大多数的实时系统）、剥夺调度（抢占方式、有优先权、短进程优先、时间片原则）</p>
<p>调度基本原则：CPU利用率、系统吞吐量、周转时间、等待时间、响应时间</p>
<p>系统吞吐量：单位时间内CPU完成作业的数量</p>
<p>周转时间：从作业提交到作业完成所用时间</p>
<p>$$<br>T&#x3D;t_{等待}+t_{就绪队列排队}+t_{上处理机运行及输入输出}\<br>t_{周转时间}&#x3D;t_{作业完成时间}-t_{作业提交时间}\<br>t_{平均周转时间}&#x3D;\frac{\sum_i^n{t_i}}{n}(作业i的周转时间：t_i)\<br>t_{带权周转时间}&#x3D;\frac{t_{作业周转时间}}{t_{作业实际运转}}\geq{1}\<br>t_{带权平均周转时间}&#x3D;\frac{\sum_i^n{t_i}}{n}(作业i的带权周转时间：t_i)\<br>$$</p>
<p>等待时间：进程处于等处理机状态的时间和</p>
<p>响应时间：从用户提交请求到系统首次产生响应所用时间</p>
<p><strong>调度算法</strong>：先来先服务（FCFS、非抢占）、短作业优先（SJF、SPF）、抢占式短作业优先（SRTN）、优先级调度、高响应比优先（HRRN）、时间片轮转调度（RR）、多级反馈队列调度</p>
<p>1.先来先服务（FCFS）：简单，效率低；对长作业有利，对短作业不利；有利于CPU繁忙型作业，不利于I&#x2F;O繁忙型作业；不会导致饥饿；多用于早期批处理系统</p>
<p>2.短作业优先（SJF、SPF）：当前已到达的最短作业先上处理机；有优先权、短进程优先、时间片原则；适用于实时&#x2F;分时操作系统；调度机制导致长作业长时间不被调度（饥饿）；多用于早期批处理系统</p>
<p>3.抢占式短作业优先（SRTN）：最短剩余时间算法</p>
<p>4.优先级调度：适用于实时操作系统；剥夺、非剥夺、静态优先级、动态优先级；系统进程&gt;用户进程 交互型进程&gt;非交互型进程 I&#x2F;O型进程&gt;计算型进程;不导致饥饿</p>
<p>5.高响应比优先（HRRN）：多用于早期批处理系统</p>
<p>$$<br>响应比R_p&#x3D;\frac{t_{等待时间}+t_{要求服务时间}}{t_{要求服务时间}}\geq{1}\<br>$$</p>
<p>6.时间片轮转调度（RR）：时间片大小设置对系统性能影响很大，时间片足够大，以至于所有进程都能在一个时间片完成，退化为先来先服务算法；时间片太小，处理机频繁在进程间切换，增加处理机开销；时间片长短由系统的响应时间、就绪队列的进程数目、系统处理能力决定；不会导致饥饿</p>
<p>7.多级反馈队列调度：设置多个就绪队列，各个队列赋予不同优先级，赋予各个队列中进程执行时间片的大小各个不同，一个进程进入内存后挂在一级队列队尾，时间片内未完成进入第二级队列队尾，第一级队列为空下一级运行；课本认为是抢占式算法；</p>
<h2 id="进程同步"><a href="#进程同步" class="headerlink" title="进程同步"></a>进程同步</h2><p>临界资源：进入区、临界区、退出区、剩余区</p>
<p>同步：完成某种任务而建立的两个或多个进程，需要协调制约关系</p>
<p>互斥：一个进程使用临界资源另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程允许访问临界资源</p>
<p>为禁止两个进程进入同一个临界区，指定准则：</p>
<p>（1）空闲让进</p>
<p>（2）忙则等待</p>
<p>（3）有限等待</p>
<p>（4）让权等待</p>
<p>实现互斥基本方法：</p>
<p>（1）软件：单标志法（违背空闲让进）、双标志法先检查（违法忙则等待）、双标志法后检查（导致饥饿、违背空闲让进、有限等待）、皮特森算法Peterson’s Algorithm（违法让权等待）</p>
<p>（2）硬件：中断屏蔽方法（禁止一切中断发生、优：简单高效 &#x2F; 缺：不适用多处理机、用户进程，只适用于系统内核进程）、硬件指令方法TS&#x2F;TSL（优：适用于任意数目的进程，无论是单处理机还是多处理机，简单容易验证其正确性 &#x2F; 缺：不能实现让权等待，从等待中随机选择一个进临界区，可能导致饥饿）</p>
<p><strong>信号量</strong></p>
<p>整型信号量：用于表示资源数目的整型量S，只要信号量S&lt;&#x3D;0，就会不断测试，违背让权等待</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">wait</span>(S)&#123;</span><br><span class="line">    <span class="keyword">while</span>(S &lt;= <span class="number">0</span>)</span><br><span class="line">        S = S - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">signal</span>(S)&#123;</span><br><span class="line">    S = S + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>记录型信号量：一个用于记录资源数目的整型量value，一个进程链表L，链接等待资源的进程,S.L解决了让权等待的问题</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    <span class="type">int</span> value;</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">process</span> *L;</span><br><span class="line">&#125;semaphore;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">wait</span><span class="params">(semaphore S)</span></span>&#123;</span><br><span class="line">    S.value --;</span><br><span class="line">    <span class="keyword">if</span>(S.value &lt; <span class="number">0</span>)&#123;</span><br><span class="line">        add <span class="keyword">this</span> process P to S.L;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">block</span>(S.L);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">signal</span><span class="params">(semaphore S)</span></span>&#123;</span><br><span class="line">    S.value ++;</span><br><span class="line">    <span class="keyword">if</span>(S.value &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">        remove a process P from S.L;</span><br><span class="line">        <span class="built_in">wakeup</span>(P);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>利用信号量实现同步：前V后P，必须保证一前一后，实现同步关系</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">semaphore S = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">P1</span>()&#123;</span><br><span class="line">    x;</span><br><span class="line">    <span class="built_in">V</span>(S);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">P2</span>()&#123;</span><br><span class="line">    <span class="built_in">P</span>(S);</span><br><span class="line">    y;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>利用信号量实现互斥：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">seamphore S = <span class="number">1</span>;</span><br><span class="line"><span class="built_in">P1</span>()&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">P</span>(S);</span><br><span class="line">    进程P1的临界区;</span><br><span class="line">    <span class="built_in">V</span>(S);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">P2</span>()&#123;</span><br><span class="line">   ...</span><br><span class="line">   <span class="built_in">P</span>(S);</span><br><span class="line">   进程P2的临界区</span><br><span class="line">   <span class="built_in">V</span>(S);</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>利用信号量实现前驱关系：</p>
<p>对不同的临界资源设置不同的互斥信号量，PV必须成对出现</p>
<p><img src="/../imgs/OS/OS_mutex.png" alt="OS_mutex"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">semaphore a1 = a2 = b1 = b2 = c = d = e = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">S1</span>()&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">V</span>(a1);</span><br><span class="line">    <span class="built_in">V</span>(a2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">S2</span>()&#123;</span><br><span class="line">    <span class="built_in">P</span>(a1);</span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">V</span>(b1);</span><br><span class="line">    <span class="built_in">V</span>(b2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">S3</span>()&#123;</span><br><span class="line">    <span class="built_in">P</span>(a2);</span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">V</span>(c);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">S4</span>()&#123;</span><br><span class="line">    <span class="built_in">P</span>(b1);</span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">V</span>(d);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">S5</span>()&#123;</span><br><span class="line">    <span class="built_in">P</span>(b2);</span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">V</span>(e);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">S6</span>()&#123;</span><br><span class="line">    <span class="built_in">P</span>(c);</span><br><span class="line">    <span class="built_in">P</span>(d);</span><br><span class="line">    <span class="built_in">P</span>(e);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>另一角度：图论出度（P）入度（V）</p>
<p><strong>管程</strong></p>
<p>名称、共享结构数据、一组过程（函数）、设置共享结构数据初值</p>
<p>互斥特性由编译器负责实现</p>
<p>各外部进程&#x2F;线程，只能从管程提供的特定入口才能访问共享数据</p>
<p>每次只允许一个进程在管程内执行某个内部过程</p>
<p><strong>经典同步问题</strong></p>
<p><em>1.生产者消费者问题</em></p>
<ul>
<li><p>一组生产者进程(Producer)</p>
</li>
<li><p>一组消费者进程(Consumer)</p>
</li>
<li><p>共享初始为空 大小为n的缓冲区(Buffer)</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">semaphore mutex = 1;  //mutex</span><br><span class="line">semaphore empty = n;  //buffer</span><br><span class="line">semaphore full = 0;   //full</span><br><span class="line"></span><br><span class="line">Producer()&#123;</span><br><span class="line">	while(1)&#123;</span><br><span class="line">		Produce();</span><br><span class="line">		P(mutex);</span><br><span class="line">		add2Buffer();</span><br><span class="line">		V(mutex);</span><br><span class="line">		V(full);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Consumer()&#123;</span><br><span class="line">	while(1)&#123;</span><br><span class="line">		P(full);</span><br><span class="line">		P(mutex);</span><br><span class="line">		getFromBuffer();</span><br><span class="line">		V(mutex);</span><br><span class="line">		Consume();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>2.读者写者问题</em></p>
<ul>
<li>读者进程(Reader)</li>
<li>写者进程(Writer)</li>
<li>共享一个文档(Document)</li>
<li>多进程读，不可多进程写</li>
<li>写进程写，不可读</li>
<li>写进程检查是否有读进程读</li>
</ul>
<p><strong>读进程优先</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">int count = 0;</span><br><span class="line">semaphore mutex = 1;</span><br><span class="line">semaphore rw = 1;</span><br><span class="line">Reader()&#123;</span><br><span class="line">	while(1)&#123;</span><br><span class="line">		P(mutex);</span><br><span class="line">		if(count == 0)</span><br><span class="line">			P(rw);</span><br><span class="line">		count++;</span><br><span class="line">		V(mutex);</span><br><span class="line">		Read();</span><br><span class="line">		P(mutex);</span><br><span class="line">		count--;</span><br><span class="line">		if(count == 0)</span><br><span class="line">			V(rw);</span><br><span class="line">		V(mutex);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">Writer()&#123;</span><br><span class="line">	while(1)&#123;</span><br><span class="line">		P(rw);</span><br><span class="line">		write();</span><br><span class="line">		v(rw);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>写进程优先</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">int count = 0;</span><br><span class="line">semaphore mutex = 1;</span><br><span class="line">semaphore rw = 1;</span><br><span class="line">semaphore w = 1;</span><br><span class="line">Writer()&#123;</span><br><span class="line">	while(1)&#123;</span><br><span class="line">		P(w);</span><br><span class="line">		P(rw);</span><br><span class="line">		Write();</span><br><span class="line">		V(rw);</span><br><span class="line">		V(w);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">Reader()&#123;</span><br><span class="line">	while(1)&#123;</span><br><span class="line">		P(w);</span><br><span class="line">		P(mutex);</span><br><span class="line">		if(count == 0)</span><br><span class="line">			P(rw);</span><br><span class="line">		count++;</span><br><span class="line">		V(mutex);</span><br><span class="line">		V(w);</span><br><span class="line">		Read();</span><br><span class="line">		P(mutex);</span><br><span class="line">		count--;</span><br><span class="line">		if(count == 0)</span><br><span class="line">			V(rw);</span><br><span class="line">		V(mutex);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>3.哲学家进餐问题</em></p>
<ul>
<li>5名哲学家(Philosopher)</li>
<li>每两名之间有一根筷子(Chopstick)</li>
<li>每名有一碗饭</li>
<li>吃完饭思考</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">semaphore Chopsticks[5] = &#123;1, 1, 1, 1, 1&#125;;</span><br><span class="line">semaphore mutex = 1;</span><br><span class="line">Philosopher()&#123;</span><br><span class="line">	do&#123;</span><br><span class="line">		P(mutex);</span><br><span class="line">		P(Chopsticks[i]);</span><br><span class="line">		P(Chopsticks[(i+1)%5]);</span><br><span class="line">		V(mutex);</span><br><span class="line">		eat();</span><br><span class="line">		V(Chopsticks[i]);</span><br><span class="line">		V(Chopsticks[(i+1)%5]);</span><br><span class="line">		think();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>4.吸烟者问题</strong></p>
<ul>
<li>3个吸烟者进程(Smoker)</li>
<li>1个提供者进程(Offer)</li>
<li>Smoker1(paper, glue)</li>
<li>Smoker2(tobacco, glue)</li>
<li>Smoker3(paper, tobacco)</li>
<li>Offer(offer1) return paper, glue</li>
<li>Offer(offer2) return tobacco, glue</li>
<li>Offer(offer3) return paper, tobacco</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">int num = 0;	//store random num</span><br><span class="line">semaphore offer1 = 0;</span><br><span class="line">semaphore offer2 = 0;</span><br><span class="line">semaphore offer3 = 0;</span><br><span class="line">semaphore end = 0;</span><br><span class="line">Offer()&#123;</span><br><span class="line">	whlie(1)&#123;</span><br><span class="line">		num++;</span><br><span class="line">		num = num % 3;</span><br><span class="line">		if(num == 0)</span><br><span class="line">			V(offer1);</span><br><span class="line">		else if(num == 1)</span><br><span class="line">			V(offer2);</span><br><span class="line">		else</span><br><span class="line">			V(offer3);</span><br><span class="line">		P(end);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">Smoker1()&#123;</span><br><span class="line">	while(1)&#123;</span><br><span class="line">		P(offer3);</span><br><span class="line">		smoke();</span><br><span class="line">		V(end);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">Smoker2()&#123;</span><br><span class="line">	while(1)&#123;</span><br><span class="line">		P(offer2);</span><br><span class="line">		smoke();</span><br><span class="line">		V(end);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">Smoker3()&#123;</span><br><span class="line">	while(1)&#123;</span><br><span class="line">		P(offer1);</span><br><span class="line">		smoke();</span><br><span class="line">		V(end);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>eg1</em></p>
<ul>
<li>3个进程P1 P2 P3</li>
<li>互斥使用N个单元的缓冲区(Buffer)</li>
<li>P1 produce() return (int num) put() @Buffer</li>
<li>P2</li>
</ul>
<p><strong>死锁</strong></p>
<p>多个进程因竞争资源造成的一种互相等待，若无外力作用，这些进程都将无法向前推进</p>
<p>死锁产生的原因：（1）系统资源的竞争（2）进程推进顺序非法（3）死锁产生的必要条件：互斥条件、不剥夺条件（已经至少保持了一个资源）、请求并保持条件、循环等待条件</p>
<p><strong>发生死锁一定有循环等待，但发生循环等待未必死锁</strong></p>
<p><strong>死锁的处理策略</strong></p>
<p><strong>死锁预防</strong>：破坏四个必要条件之一即可</p>
<ul>
<li><p>破坏互斥条件</p>
</li>
<li><p>破坏不剥夺条件</p>
</li>
<li><p>破坏请求并保持条件</p>
</li>
<li><p>破坏循环等待条件</p>
</li>
</ul>
<p>死锁避免：在资源分配过程，防止进入不安全状态</p>
<ul>
<li>银行家算法</li>
</ul>
<p>$$Need &#x3D; Max -Allocation$$</p>
<p>死锁检测和解除：系统分配进程时不做措施，应该提供死锁检测和解除的手段</p>
<ul>
<li>资源分配图</li>
</ul>
<p><img src="/../imgs/OS/source_allocation.png" alt="source_allocation"></p>
<p>请求边：从进程到资源<br>分配边：从资源到进程</p>
<ul>
<li>死锁定理：依次消除与不阻塞进程相连接的边，直到无边可消除</li>
<li>死锁解除：资源剥夺法、撤销进程法、进程回退法</li>
</ul>
<h1 id="第三章-内存管理"><a href="#第三章-内存管理" class="headerlink" title="第三章 内存管理"></a>第三章 内存管理</h1><p><strong>程序装入和链接</strong></p>
<ul>
<li>编译：编译程序将源代码编译成若干目标模块</li>
<li>链接：链接程序将编译后形成一组目标模块及所需的库函数链接在一起，形成一个完整的装入模块</li>
<li>装入：装入程序装入模块装入内存运行</li>
</ul>
<p>链接方式</p>
<ul>
<li>静态：程序运行前，链接为一个完整可执行的程序</li>
<li>装入时动态：装入过程中，边装入边链接</li>
<li>运行时动态：程序执行过程需要该模块才进行</li>
</ul>
<p>装入方式：</p>
<ul>
<li>绝对装入：仅适用于单道程序，装入程序按照装入模块中的地址，将程序数据装入内存，逻辑地址与物理地址完全相同</li>
<li>可重定位装入（静态重定位）：一个作业装入内存，<strong>必须给它分配要求的全部内存空间</strong>，若没有足够的内存，则无法装入。一旦运行，作业进入内存，整个运行期间不能在内存中移动，也不能再申请内存空间</li>
<li>动态运行时装入（动态重定位）：程序在内存中发生移动，则需要采用动态的装入方式。装入程序把装入内存后，并不立即将装入模块中的相对地址转换为绝对地址，而是将<strong>地址转换推迟到程序真正要执行时才进行</strong>，需要重定位寄存器的支持</li>
</ul>
<p>逻辑地址和物理地址</p>
<ul>
<li>地址重定位：当装入程序将可执行代码装入内存时，必须通过地址转换将逻辑地址转换成为物理地址</li>
</ul>
<p>内存保护</p>
<ul>
<li>在CPU中设置一堆上、下限寄存器，CPU要访问一个地址，分别和两个寄存器值相比，判断有无越界</li>
<li>采用重定位寄存器（基址寄存器）和界地址寄存器（限长寄存器）来实现这种保护</li>
</ul>
<p><strong>连续分配管理方式</strong></p>
<p>单一连续分配</p>
<p>内存分为系统区、用户区，无需进行内存保护，内存中只允许有有一道程序</p>
<ul>
<li>优 简单无外部碎片</li>
<li>缺 只能用于单用户、单任务的操作系统中，有内部碎片，存储器利用率极低</li>
</ul>
<p><strong>固定分区分配</strong></p>
<p>将用户内存空间划分为若干固定大小的区域，每个分区只装入一道作业。当有空闲分区时，可从外存的后备作业队列中选择适当大小的作业装入</p>
<ul>
<li>划分分区方式：分区大小相等、分区大小不等</li>
</ul>
<p>问题</p>
<ul>
<li>程序可能太大放不进任何一块分区，用户不得不使用覆盖技术使用内存空间</li>
<li>主存利用率低，程序小也要占用一个分区，现象称为<strong>内部碎片</strong></li>
</ul>
<p><strong>动态分区分配</strong></p>
<p>又称可变分区分配，不预先划分内存，在进程装入内存时，根据进程大小动态地建立分区，并使分区的大小正好适合进程的需要</p>
<p>问题：所有分区外的存储空间会产生越来越多的碎片，克服外部碎片可以通过紧凑技术来解决</p>
<p>动态分区策略</p>
<ul>
<li><p>首次适应Fiist Fit：空闲分区以地址递增的次序链接，找到大小能满足要求的第一个空间。会使得内存的低地址部分出现很多小的空闲分区，每次分配查找时，都要经过这些分区，增加了查找的开销</p>
</li>
<li><p>最佳适应Best Fit：空闲分区按容量递增的方式形成分区链，找到第一个能满足要求的空间。性能通常很差，因为每次最佳的分配会留下很小难以利用的内存块，会产生最多的外部碎片。</p>
</li>
<li><p>最坏适应Worst Fit（最大适应Lasgest Fit）:以容量递减的次序链接，找到第一个能满足要求的空闲分区。选择最大的块，但却把最大的连续内存的划分开，会很快导致没有可用的大内存块，导致性能很差</p>
</li>
<li><p>邻近适用Next Fit（循环首次适应算法）：分配内存时从上次查询结束的位置开始继续查找。在一次扫描中，内存前面部分使用后再释放时，不会参与分配，导致在内存的末尾分配空间分裂为小碎片。</p>
</li>
</ul>
<p><em><strong>非连续分配</strong></em> </p>
<p><strong>基本分页存储管理方式</strong>：主存空间划分为大小相等且固定的块，块相对较小，作为主存的基本单位。每个进程以块为单位进行划分，进程在执行时，以块为单位逐个申请贮存中的空间</p>
<p>分页管理不会产生外部碎片，块的大小相对分区要小，进程按块划分，进程运行时按块申请主存可用空间并执行，进程只会在最后一个不完整的块，不产生主存碎片，，每个进程平均只产生半个块大小的内部碎片</p>
<ul>
<li><p>页面和页面大小：进程中的块称为页（page），内存中的块称为页框（page frame、或页帧）。外存也以同样的单位进行划分，称为块（block）。进程在执行时需要申请主存空间，要为每个页面分配主存中的可用页框，页和页框一一对应</p>
</li>
<li><p>地址结构</p>
</li>
</ul>
<p><img src="/../imgs/OS/single_page.png" alt="single_page"></p>
<ul>
<li>页表：通过查找页表即可找到相应的物理块</li>
</ul>
<p>基本地址变换：</p>
<p>将逻辑地址变换为内存中的物理地址，在系统中设置页表寄存器（PTR）存放内存起始地址F和页表长度M。</p>
<p><img src="/../imgs/OS/page_allocation_transform.png" alt="page_allocation_transform"></p>
<p>$$<br>\begin{aligned}<br>&amp;（1）页号P&#x3D;\frac{A}{L}\<br>&amp;（2）页内偏移量W&#x3D;A%L\<br>&amp;（3）比较页号P和页表长度M，P\geq M产生越界中断\<br>&amp;（4）页表中页号P对应的页表项地址A&#x3D;F+P<em>M\<br>&amp;（5）物理地址E&#x3D;b</em>L+W\<br>\end{aligned}<br>$$<br><em>页式管理中空间地址是一维的</em></p>
<p>问题</p>
<ul>
<li>每次访存均需地址变换，地址变换必须足够快，否则访存速度会降低</li>
<li>每个进程引入页表，用于存储映射机制，页表不能过大</li>
</ul>
<p>引入快表机制：在地址变换过程中加入具有并行查找能力的高速缓冲存储器——<strong>快表</strong>（相联存储器TLB，主存中的页表为<strong>慢表</strong>）</p>
<p><img src="/../imgs/OS/add_TLB_page_allocation_transform.png" alt="add_TLB_page_allocation_transform"></p>
<p>一次地址变换流程</p>
<p>（1）CPU给出逻辑地址，将页号送入高速缓冲寄存器，查询此页号是否存在于快表内</p>
<p>（2）若匹配到，直接取出对应页的页框号，与页内偏移拼接为物理地址，访存</p>
<p>（3）若未匹配到，访问慢表查询，读取页表项后复制到快表中，进行地址变换后访存</p>
<p><em>值得注意的是，题中是否说明快表初始为空以及快表慢表的查询机制，是否同时查询！</em></p>
<p>两级页表</p>
<ul>
<li>引入页表，执行时不需要调入所有内存页框，为了压缩页表，采取多级映射</li>
</ul>
<p><img src="/../imgs/OS/2_dim_page.png" alt="2_dim_page"></p>
<ul>
<li>多级页表大小不能超过一个页面</li>
</ul>
<p><img src="/../imgs/OS/2_dim_page_arch.png" alt="2_dim_page_arch"></p>
<p>$$<br>\begin{aligned}<br>&amp;逻辑地址：32bit\<br>&amp;以字节编址\<br>&amp;页表项：4B\<br>&amp;页面大小：4KB&#x3D;2^{12}B （页内偏移量）\<br>&amp;页号：32-12&#x3D;20\<br>&amp;全映射需要2^{20}个页表项\<br>&amp;共需4B<em>2^{20}&#x3D;4MB大小空间存储页表\<br>&amp;\<br>&amp;以40MB进程为例\<br>&amp;页表项：\frac{40MB}{4B</em>4KB}&#x3D;40KB\<br>&amp;需要\frac{40KB}{4KB}&#x3D;10页面\<br>&amp;整个进程需要\frac{40MB}{4KB}&#x3D;10*2^{10}个页面\<br>&amp;\<br>&amp;为了压缩页表，采取二级页表机制\<br>&amp;页表10页进行映射只需要10个页表项\<br>&amp;上一级页表只需要1页就已经足够2^{10}&#x3D;1024个页表项\<br>&amp;进程执行时，仅需将这一页的上级页表调入即可\<br>&amp;页面大小：4KB&#x3D;2^{12}B （页内偏移量）\<br>&amp;页号：32-12&#x3D;20\<br>&amp;顶级（一级）页表为1个页面\<br>&amp;一级页表项\frac{4KB}{4B}&#x3D;1K\<br>&amp;一级页表占用log_2{1K}&#x3D;10位\<br>&amp;二级页表占用20-10&#x3D;10位\<br>&amp;二级页表大小2^{10}*4B&#x3D;4KB\leq页面大小4KB\<br>\end{aligned}<br>$$</p>
<p><strong>基本分段存储管理方式</strong></p>
<p>分页管理是从计算机角度考虑设计，提高内存利用率，分页通过硬件机制实现</p>
<p>分段管理是从用户和程序员出发，方便编程、信息保护和共享、动态增长以及动态链接等方面的需要</p>
<p><img src="/../imgs/OS/single_seg.png" alt="single_seg"></p>
<p>段号决定每个进程最多可以分几个段，段内地址决定每个段内的最大长度</p>
<p>段表：每个进程都有一张逻辑空间与内存空间映射的段表</p>
<p><img src="/../imgs/OS/seg.png" alt="seg"></p>
<p>地址变换机构</p>
<p><img src="/../imgs/OS/segment_address_translation_mechanism.png" alt="分段地址变换机构"></p>
<p>$$<br>从逻辑地址A中取出前几位为段号S，后几位为段内偏移量W\<br>比较段号S和段表长度M，S\geq M,产生越界中断，否则继续执行\<br>段表中段号S对应段表项地址&#x3D;段表始址F+段号S*段表项长度\<br>段内偏移量\geq C，产生越界中断，否则继续执行\<br>取出段表项中该段的起始地址b，E&#x3D;b+W，得到物理地址E去访问内存\<br>$$</p>
<p>段的共享与保护：分段系统，通过两个作业的段表中相应表项指向被共享的段同一个物理副本，当地一个作业从共享段读取数据，必须防止另一个修改数据，不能修改的的代码称为<strong>纯代码、可重入代码</strong></p>
<p>段号和段内偏移需要显示给出，地址空间为二维</p>
<p><strong>段页式管理方式</strong></p>
<p>作业的地址空间被分为若干逻辑段，每段都有自己的段号，将每段分为若干大小的固定的页，内存空间分为若干和页面大小相同的存储块，对内存的分配以块为单位。</p>
<ul>
<li><p>段号位数决定每个进程最多可以分几个段</p>
</li>
<li><p>页号位数决定最多有几个页</p>
</li>
<li><p>页内偏移量决定页面大小、内存块的大小</p>
</li>
</ul>
<p>段页式系统的逻辑地址结构：</p>
<p><img src="/../imgs/OS/OS_page_seg_add.png" alt="OS_page_seg_add"></p>
<p>地址变换</p>
<p>需要三次访存，可以使用快表机制</p>
<p><img src="/../imgs/OS/segment_page_address_translation_mechanism.png" alt="segment_page_address_translation_mechanism"></p>
<p><strong>虚拟内存管理</strong></p>
<p>传统存储管理方式</p>
<ul>
<li>一次性：一次全部装入，才能开始</li>
<li>驻留性：装入后，常驻内存，任何部分都不会被换出，直至结束</li>
</ul>
<p>局部性原理</p>
<p>一个程序，一段时间内，只有一部分会被访问</p>
<ul>
<li>空间</li>
<li>时间</li>
</ul>
<p>虚拟存储器</p>
<p>将程序的一部分装入内存，其余部分留在外存，当所访问的部分不在内存，操作系统将需要的部分调入内存，将暂时不需要的内容换到外存</p>
<ul>
<li>多次性：无需一次全部装入，允许分为多次调入</li>
<li>对换性：无需常驻内存</li>
<li>虚拟性：从逻辑上扩充内存容量，使用户看到的内存容量远大于内存容量</li>
</ul>
<p>本质：用时间换空间</p>
<p>实现：请求分页、请求分段、请求段页式</p>
<p>支持：内存、外存、页表机制、段表机制、中断机构、地址变换机构</p>
<p><strong>请求分页管理方式</strong></p>
<p>访问不存在内存中的页面，通过调页将其调入，通过置换算法将暂时不需要的页面调到外存上</p>
<p>页表机制</p>
<p><img src="/../imgs/OS/Request_paging_page_table_mechanism.png" alt="Request_paging_page_table_mechanism"></p>
<p>$$<br>状态位P：指示是否调入内存\<br>访问字段A：记录一段时间内被访问的次数\<br>修改位M：标识页面调入内存后是否被修改过\<br>外存地址：指出该页在外存上的地址，通常是物理块号\<br>$$</p>
<p>缺页中断机制</p>
<p>访问页面不在内存中时，产生一个缺页中断，请求操作系统将缺页调入内存，将缺页的进程阻塞，若内存有闲置的空闲块，则分配一个块，将页面装入，并修改页表相应的页表项，若内存中无空闲块，则淘汰某页，淘汰页若在内存中修改过需要同步，写回外存</p>
<p>地址变换机构</p>
<p><img src="/../imgs/OS/request_paging_address_translation_mechanism.png" alt="request_paging_address_translation_mechanism"></p>
<p>页面置换算法</p>
<ul>
<li>最佳置换算法OPT</li>
</ul>
<p>选择的被淘汰页是以后永不使用的页面，或是最长时间内不再被访问的页面</p>
<ul>
<li>先进先出算法FIFO</li>
</ul>
<p>优先淘汰最早进入内存的页面，即内存中驻留时间最久的页面</p>
<p>会产生所分配的物理块增大页故障数不减反增的异常现象，<strong>Belady异常</strong></p>
<ul>
<li>最近最久未使用算法LRU</li>
</ul>
<p>选择最近最长时间未访问过的页面予以淘汰，为每个页面设置一个访问字段记录上次被访问所经历的时间</p>
<p>性能较好，需要寄存器和栈的硬件支持</p>
<ul>
<li>时钟置换算法CLOCK</li>
</ul>
<p>简单的CLOCK算法：每帧关联一个附加位，使用位<strong>u</strong>，连成一个循环队列。某页装入时，使用位置为1；被访问时，使用位置为1；置换时，操作系统扫描缓冲区，每当遇到一个使用位为1的帧，置为0；最后停留在第一个使用位为0的帧</p>
<p>CLOCK算法性能比较接近LRU算法</p>
<p>改进的CLOCK算法：再增加一个修改位<strong>m</strong>，<strong>P(u,m)</strong></p>
<p>第一轮扫描，指针扫描过的页面使用位<strong>u</strong>置为0</p>
<p>第一轮扫描中，未找到使用位<strong>u</strong>为0的页面进行第二轮扫描</p>
<p>第二轮扫描，第一个页面置换出，换入页面<strong>m</strong>修改位置为1，并将指针后移</p>
<p><strong>页面分配策略</strong></p>
<p>一个进程分配的物理页框的集合，分配给一个进程的存储量越小，任何时候驻留在主存中的进程数就越多，从而提高处理机的效率；一个进程页数过少，基于局部性原理，页错误率会相对较高；页数过多，基于局部性原理，给特定的进程分配更多主存空间对该进程的错误率没有明显改善。</p>
<p>策略：固定分配局部置换、可变分配全局置换、可变分配局部置换（没有固定分配全局分配）</p>
<p>调入时机：预调页策略、请求调页策略</p>
<p>从何处调页：系统拥有足够的对换区间、系统缺少足够的对换区间、UNIX方式</p>
<p><strong>抖动</strong></p>
<p>某进程频繁访问的页面数目高于可用的物理页帧数目</p>
<p><strong>工作集</strong></p>
<p>在某段时间间隔内，进程要访问的页面集合</p>
<p>一般，分配给进程的物理块数（驻留集大小）要大于工作集大小</p>
<h1 id="第四章-文件系统"><a href="#第四章-文件系统" class="headerlink" title="第四章 文件系统"></a>第四章 文件系统</h1><p><strong>文件</strong></p>
<p>文件结构：数据项、记录、文件</p>
<p>属性：名称、标识符、类型、位置、大小、保护、时间</p>
<p>所有文件的信息都保存在目录结构中，而目录结构保存在外存上，文件信息在需要时调入内存。</p>
<p>基本操作：创建、写、读、重定位、删除、截断</p>
<p>文件逻辑结构</p>
<ul>
<li>无结构文件（流式文件）</li>
</ul>
<p>二进制式字符流组成</p>
<ul>
<li>有结构文件（记录式文件）</li>
</ul>
<p>顺序文件</p>
<p>索引文件</p>
<p>索引顺序文件</p>
<p>直接文件或散列文件</p>
<p><strong>目录结构</strong></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/OS_001_introduction-Operator-System/" data-id="clkqazu9d001jdlbi2pqg5rrt" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-中央处理器" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/%E4%B8%AD%E5%A4%AE%E5%A4%84%E7%90%86%E5%99%A8/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="中央处理器"><a href="#中央处理器" class="headerlink" title="中央处理器"></a>中央处理器</h1><p>控制器：负责协调并控制计算机各部件执行程序的指令序列：取指令、分析指令、执行指令<br>运算器：对数据加工</p>
<ul>
<li>指令控制</li>
<li>操作控制</li>
<li>数据加工</li>
<li>时间控制</li>
<li>中断处理</li>
</ul>
<h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><p>运算器和控制器两大部分组成<br>运算器：</p>
<ul>
<li>算术逻辑单元（ALU）</li>
<li>暂存寄存器</li>
<li>累加寄存器（ACC）</li>
<li>通用寄存器组（AX&#x2F;BX&#x2F;CX&#x2F;DX&#x2F;SP）</li>
<li>程序状态字寄存器（PSW：OF&#x2F;SF&#x2F;ZF&#x2F;CF）</li>
<li>移位器</li>
<li>计数器</li>
</ul>
<p>控制器（CU）：</p>
<ul>
<li>程序计数器（PC）</li>
<li>指令寄存器（IR）</li>
<li>指令译码器</li>
<li>存储器地址寄存器</li>
<li>存储器数据寄存器</li>
<li>时序系统</li>
<li>微操作信号发生器</li>
</ul>
<h2 id="指令执行过程"><a href="#指令执行过程" class="headerlink" title="指令执行过程"></a>指令执行过程</h2><h3 id="指令周期"><a href="#指令周期" class="headerlink" title="指令周期"></a>指令周期</h3><p>CPU从主存取出并执行一条指令的时间，指令周期通常使用若干机器周期表示，每个机器周期可等长或不等，一个机器周期包含若干时钟周期（节拍），每个机器周期内的时钟周期数可以不等</p>
<ul>
<li>无条件转移指令：执行阶段不需要访存，只包含取指阶段（取指和分析）和执行阶段</li>
<li>间接寻址指令：为了取操作数，需要先访1次，取出有效地址，然后访存，取出操作数，所有需要包含间址周期，介于取指和执行之间</li>
<li>CPU采用中断方式实现主机和I&#x2F;O设备的信息交换，CPU在每条指令结束前，都要发送中断查询信号，若有中断请求，CPU进入中断响应阶段（中断周期）</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>取指周期</td>
<td>间址周期</td>
<td>执行周期</td>
<td>中断周期</td>
</tr>
</tbody></table>
<p>只有访存的目的不同，取指周期是为了取指令，间址周期是为了取有效地址，执行周期是为了取操作数，中断周期是为了保存程序断点<br>CPU内设置了4个标志触发器FE&#x2F;IND&#x2F;EX&#x2F;INT</p>
<table>
<thead>
<tr>
<th>FE</th>
<th>IND</th>
<th>EX</th>
<th>INT</th>
</tr>
</thead>
<tbody><tr>
<td>Fetch</td>
<td>Index</td>
<td>Execute</td>
<td>Interrupt</td>
</tr>
<tr>
<td>取值</td>
<td>间址</td>
<td>执行</td>
<td>中断</td>
</tr>
</tbody></table>
<h3 id="指令周期的数据流"><a href="#指令周期的数据流" class="headerlink" title="指令周期的数据流"></a>指令周期的数据流</h3><h4 id="取指周期"><a href="#取指周期" class="headerlink" title="取指周期"></a>取指周期</h4><p>根据PC中的内容从主存取出指令代码放入IR  </p>
<ul>
<li>PC $\to$ IR $\to$ AddrBus $\to$ Mem</li>
<li>CU $\to$ DataBus $\to$ MDR $\to$ Mem</li>
<li>Mem $\to$ DataBus $\to$ MDR $\to$ IR（存放指令）</li>
<li>CU $\stackrel{SignalControl::FE}{\longrightarrow}$ [(PC) + 1 $\to$ PC]</li>
</ul>
<h4 id="间址周期"><a href="#间址周期" class="headerlink" title="间址周期"></a>间址周期</h4><p>取操作数有效地址，间址为例：指令中的地址码送到MAR并送至地址总线，CU向存储器发送读命令，以获取有效地址并存在MDR  </p>
<ul>
<li>Addr(IR) &#x2F; MDR $\to$ MAR $\to$ AddrBus $\to$ Mem</li>
<li>CU $\stackrel{SignalI&#x2F;O::READ}{\longrightarrow}$ ControlBus $\to$ Mem（存放有效地址）</li>
<li>Mem $\to$ DataBus $\to$ MDR</li>
</ul>
<h4 id="执行周期"><a href="#执行周期" class="headerlink" title="执行周期"></a>执行周期</h4><p>取操作数，根据IR的指令字的操作码通过ALU操作产生执行结果</p>
<ul>
<li>无统一的数据流向</li>
</ul>
<h4 id="中断周期"><a href="#中断周期" class="headerlink" title="中断周期"></a>中断周期</h4><p>处理中断请求，假设程序断点存入堆栈，用SP指示栈顶地址，入栈操作是先修改栈顶指针，后存入数据</p>
<ul>
<li>CU $\stackrel{SignalStack::SP}{\longrightarrow}$ [(SP) - 1 $\to$ SP] $\to$ MAR $\to$ AddrBus $\to$ Mem</li>
<li>CU $\stackrel{SignalI&#x2F;O::WRITE}{\longrightarrow}$ ControlBus $\to$ Mem</li>
<li>PC $\to$ MDR $\to$ DataBus $\to$ Mem（程序断点存入主存）</li>
<li>CU $\to$ PC（中断服务程序的入口地址送至PC）</li>
</ul>
<h3 id="指令执行方案"><a href="#指令执行方案" class="headerlink" title="指令执行方案"></a>指令执行方案</h3><ul>
<li>单指令周期</li>
<li>多指令周期</li>
<li>流水线方案</li>
</ul>
<h2 id="数据通路"><a href="#数据通路" class="headerlink" title="数据通路"></a>数据通路</h2><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>数据通路：数据在功能部件之间传输的路径<br>由控制部件控制，控制部件根据每条指令功能的不同生成对数据通路的控制信号<br>数据通路功能：实现CPU内部的运算器与寄存器以及寄存器之间的数据交换</p>
<h3 id="基本结构-1"><a href="#基本结构-1" class="headerlink" title="基本结构"></a>基本结构</h3><ul>
<li>CPU内部单总线模式<br>所有寄存器的输入输出端都连接在一条公共通路上，结构简单但数据传输存在较多的冲突现象，性能较低。连接各部件的总线只有一条时，称为单总线结构，CPU中有两条或更多的总线时，构成双总线结构或多总线结构</li>
<li>CPU内部多总线模式<br>所有寄存器的输入输出端都连接在多条公共通路上，相比之下单总线在一个时钟内只允许传一个数据，因此指令执行效率很低，因此在多总线方式，同时在多总线上传输不同的数据，提高效率</li>
<li>专用数据通路方式<br>根据指令执行过程中的数据和地址的流动方向安排连接线路，避免使用共享的总线，性能较高，但硬件量大</li>
</ul>
<h4 id="寄存器之间的数据传输"><a href="#寄存器之间的数据传输" class="headerlink" title="寄存器之间的数据传输"></a>寄存器之间的数据传输</h4><p>通过内部总线完成<br>寄存器AX的输入输出由AXout和AXin控制  </p>
<ul>
<li>(PC) $\to$ MAR，PCout和MARin有效</li>
</ul>
<h4 id="主存与CPU之间的数据传输"><a href="#主存与CPU之间的数据传输" class="headerlink" title="主存与CPU之间的数据传输"></a>主存与CPU之间的数据传输</h4><p>主存与CPU之间的数据传输需要借助CPU内部总线完成<br>主存内读取</p>
<ul>
<li>(PC) $\to$ MAR，PCout和MARin有效  </li>
<li>1 $\to$ R，CU发出读命令  </li>
<li>MEM(MAR) $\to$ MDR，MDRin有效  </li>
<li>(MDR) $\to$ IR，MDRout和IRin有效</li>
</ul>
<h4 id="执行算术或逻辑算术"><a href="#执行算术或逻辑算术" class="headerlink" title="执行算术或逻辑算术"></a>执行算术或逻辑算术</h4><p>由于ALU没有内部存储功能，执行加法操作，相加的两个数必须在ALU的两个输入输出端同时有效</p>
<ul>
<li>(MDR) $\to$ MAR，MDRout和MARin有效  </li>
<li>1 $\to$ R，CU读命令</li>
<li>MEM(MAR) $\to$ 操作数从主存送至MDR</li>
<li>(MDR) $\to$ Y，MDRout和Yin有效</li>
<li>(ACC) + (Y) $\to$ Z ACCout和ALUin有效</li>
<li>(Z) $\to$ ACC，Zout和ACCin有效</li>
</ul>
<h2 id="控制器功能与原理"><a href="#控制器功能与原理" class="headerlink" title="控制器功能与原理"></a>控制器功能与原理</h2><h3 id="结构和功能"><a href="#结构和功能" class="headerlink" title="结构和功能"></a>结构和功能</h3><ul>
<li>运算器部件通过数据总线与内存储器、输入设备和输出设备传送数据</li>
<li>输入设备和输出设备通过接口电路与总线相连接</li>
<li>内存储器、输入设备通过接口电路与总线相连接</li>
<li>内存储器、输入设备和输出设备从地址总线接收地址信息，从控制总线得到控制信号，通过数据总线与其他部件传输数据</li>
<li>控制器部件从数据总线接受指令信息，从运算器部件接收指令转移地址，送出指令地址到地址总线，还要向系统中的部件通过运算所需的控制信号</li>
</ul>
<p>控制器功能：</p>
<ul>
<li>从主存取指令，并指出下一条指令在主存中的位置</li>
<li>对指令进行译码或测试，产生相应的操作控制信号，以便启动规定的动作</li>
<li>指挥并控制CPU、主存、输入输出设备之间的数据流动方向</li>
</ul>
<p>根据控制器产生微操作控制信号的方式不同，控制器可分为</p>
<ul>
<li>硬布线控制器</li>
<li>微程序控制器</li>
</ul>
<p>两类控制器的PC和IR是相同的，但确定和表示指令执行步骤的办法以及给出控制部件各部件运算所需控制信号的方案不同</p>
<h3 id="硬布线控制器"><a href="#硬布线控制器" class="headerlink" title="硬布线控制器"></a>硬布线控制器</h3><ul>
<li>根据指令的要求、当前的时序及外部和内部的状态，按时间的顺序发送一系列微操作控制信号</li>
<li>由复杂的组合逻辑门电路和一些触发器构成</li>
</ul>
<h4 id="硬布线控制单元"><a href="#硬布线控制单元" class="headerlink" title="硬布线控制单元"></a>硬布线控制单元</h4><p>指令的操作码是决定控制单元发出不同操作命令（控制信号）的关键<br>CU的输入信号来源：</p>
<ul>
<li>经指令译码器译码产生的信息指令</li>
<li>时序系统产生的机器周期信号和节拍信号</li>
<li>来自执行单元的反馈信号（标志）</li>
<li>系统总线（控制总线）控制信号（中断请求、DMA请求）</li>
</ul>
<h4 id="硬布线控制器的时序系统及微操作"><a href="#硬布线控制器的时序系统及微操作" class="headerlink" title="硬布线控制器的时序系统及微操作"></a>硬布线控制器的时序系统及微操作</h4><ul>
<li>时钟周期<br>用时钟信号控制节拍发生器，可以产生节拍，每个节拍的宽度对应一个时钟周期，每个节拍内机器可以完成一个或几个需要同时执行的操作</li>
<li>机器周期<br>是所有指令执行过程的一个基准时间，访问一次存储器的时间是固定的，因此通常以存取周期作为基准时间，即内存中读取一个指令字的最短时间作为机器周期，在存储字长等于指令字长前提下，取指周期视为机器周期</li>
<li>指令周期</li>
<li>微操作命令分析<br>控制单元具有发出各种操作命令（控制信号）序列的功能，这些命令与指令有关</li>
</ul>
<p>执行过程，一条指令分为3个工作周期：取指周期、间址周期、执行周期  </p>
<p>取指周期  </p>
<ul>
<li>(PC) $\to$ MAR  </li>
<li>1 $\to$ R  </li>
<li>M(MAR) $\to$ MDR</li>
<li>(MDR) $\to$ IR</li>
<li>OP(IR) $\to$ CU</li>
<li>(PC) + 1  $\to$ PC</li>
</ul>
<p>间址周期</p>
<ul>
<li>Addr(IR) $\to$ MAR</li>
<li>1  $\to$ R</li>
<li>M(MAR)  $\to$ MDR</li>
</ul>
<p>执行周期  </p>
<ul>
<li>非访存指令  </li>
<li>访存指令</li>
</ul>
<h4 id="CPU控制方式"><a href="#CPU控制方式" class="headerlink" title="CPU控制方式"></a>CPU控制方式</h4><ul>
<li>同步控制方式<br>具有统一的时钟，所有控制信号均来自统一的时钟信号</li>
<li>异步控制方式<br>不存在基准时标信号，各部件按自身固有的速度工作，以应答方式联络</li>
<li>联合控制方式<br>大部分采用同步控制，小部分采用异步控制</li>
</ul>
<h3 id="微程序控制器"><a href="#微程序控制器" class="headerlink" title="微程序控制器"></a>微程序控制器</h3><p>采用存储逻辑实现,把微操作信号代码化,使每条机器指令转化为一段微程序并存入一个专门的存储器（控制存储器），微操作控制信号由微指令产生</p>
<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ul>
<li><p>微操作和微命令<br>一条机器指令可以分解为一系列微操作序列，微操作是计算机中最基本、不可再分解的操作；微程序控制的计算机中，将控制部件向执行部件发出的各种控制命令称为微命令，是构成控制系列的最小单位<br>微命令有相容性和互斥性</p>
</li>
<li><p>微指令和微周期<br>微指令是若干微命令的集合，存放微指令的控制存储器的单元地址称为微地址<br>一条微地址包括：<br>（1）操作控制字段（微操作码）：用于产生某一步操作所需的各种操作控制信号<br>（2）顺序控制字段（微地址码）：用于控制产生下一条要执行的微指令地址<br>微周期是执行一条微指令所需的时间，通常为一个时钟周期  </p>
</li>
<li><p>主存储器和控制存储器<br>主存储器M，用于存放程序和数据，在CPU外部，用RAM实现<br>控制存储器CM，用于存放微程序，在CPU内部，用ROM实现</p>
</li>
<li><p>程序和微程序<br>程序是指令的有序集合，用于完成某些特定的功能<br>微程序是微指令的有序集合，一条指令的给你由一段微程序实现<br>微程序和程序是两个不同的概念，微程序由微指令组成，描述机器指令，微程序实质是机器指令的实时解释器，由计算机设计者实现编制并存放于控制存储器CM中，无需知道，而程序最终由机器指令组成，由软件设计人员事先编制并存放于主存储器或辅助存储器</p>
</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>MAR</td>
<td>存放主存的读写地址</td>
<td>CMAR</td>
<td>存放控制存储器的读写微指令地址</td>
</tr>
<tr>
<td>IR</td>
<td>存放从主存中读出的指令</td>
<td>CMDR&#x2F;$\mu$IR</td>
<td>存放控制存储器中读出的微指令</td>
</tr>
</tbody></table>
<h4 id="组成-过程"><a href="#组成-过程" class="headerlink" title="组成&amp;过程"></a>组成&amp;过程</h4><ul>
<li>控制存储器：存放各指令对应的微程序</li>
<li>微指令寄存器：用于存放从CM中取出的微指令，位数同微指令字长相等</li>
<li>微地址形成部件：产生初始微地址和后继微地址，以保证微指令的连续执行</li>
<li>微地址寄存器：接收微地址形成部件送来的微地址，为在CM中读取微指令作准备</li>
</ul>
<p>在微程序控制器的控制下计算器执行机器指令的过程：  </p>
<ul>
<li>执行取微指令：自动将取指微程序的入口地址送入CMAR，从CM中读取相应的微指令送入CMDR（取指微程序的入口地址一般为CM的0号单元，当取指微程序执行完成，从主存取出的机器指令就已经存入指令寄存器中）</li>
<li>由机器指令的操作码字段通过微地址形成部件产生该机器指令对应的微程序入口地址，并送入CMAR</li>
<li>从CM中逐条读取对应的微指令并执行</li>
<li>执行完对应于一条机器指令的一个微程序后，又回到取指微程序的入口地址，继续第一步</li>
</ul>
<p>微程序和机器指令：<br>一条机器指令对应一个微程序，由于机器指令的取指令操作都是相同的，因此可将取指令操作的微命令统一编制为一个微程序，这个微程序只负责将指令从主存单元取出送入指令寄存器，也可编制对应的间址周期和中断周期的微程序<br>控制存储器CM中的微程序个数 &#x3D; 机器指令数+取指+间址+中断</p>
<h4 id="编码方式"><a href="#编码方式" class="headerlink" title="编码方式"></a>编码方式</h4><p>又称微指令的控制方式，如何对微指令的控制字段进行编码，以形成控制信号  </p>
<ul>
<li><p>直接编码（直接控制）<br>无需进行译码，微指令的微命令字段中每位代表一个微命令，设计微指令，选用某个微命令只需将微命令对应的字段设置为1或0<br>优：简单直观速度快<br>缺：微指令字长过长，n个微命令要求微指令的操作字段有n位，造成控制存储器容量极大</p>
</li>
<li><p>字段直接编码<br>将微指令的微命令字段分成若干小字段，互斥性微命令组合在同一字段，相容性微命令组合在不同字段，每个字段独立编码，每种编码代表一个微命令且各字段编码含义单独定义，与其他字段无关<br>优:可以缩短微指令字长<br>缺：需要通过编译电路后发出微命令，较直接编译慢</p>
</li>
</ul>
<h4 id="地址形成方式"><a href="#地址形成方式" class="headerlink" title="地址形成方式"></a>地址形成方式</h4><ul>
<li>直接由微指令的下地址字段指出，微指令格式中设置一个下地址字段，由微指令的下地址字段直接指出后继微指令的地址（断定方式）</li>
<li>根据机器指令的操作码形成，机器指令取至指令寄存器后，微指令的地址由操作码经微地址形成部件形成</li>
<li>增量计算器法：(CMAR) + 1 $\to$ CMAR</li>
<li>根据各种标志决定微指令分支转移地址</li>
<li>通过测试网络形成</li>
<li>由硬件直接产生微程序入口地址</li>
</ul>
<h4 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h4><ul>
<li>水平型：直接编码、字段直接编码、字段间接编码、混合编码</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>$A_1$</td>
<td>$A_2$</td>
<td>$\dots$</td>
<td>$A_{n-1}$</td>
<td>$A_{n}$</td>
<td>判断测试字段</td>
<td>后继地址字段</td>
</tr>
<tr>
<td>操作控制</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>顺序控制</td>
<td></td>
</tr>
</tbody></table>
<p>优：微程序短，执行速快<br>缺：微指令长，编写微程序复杂  </p>
<ul>
<li>垂直型：采用类似机器指令操作码的方式，在微指令中设置微操作码字段，采用微操作码编译法，由微操作码规定微指令的功能</li>
</ul>
<table>
<thead>
<tr>
<th>$\mu$OP</th>
<th>Rd</th>
<th>Rs</th>
</tr>
</thead>
<tbody><tr>
<td>微操作码</td>
<td>目的地址</td>
<td>源地址</td>
</tr>
</tbody></table>
<p>优：微指令短、简单、规整，便于编写微程序<br>缺：微程序长，执行速度慢，工作效率低  </p>
<ul>
<li>混合型<br>在垂直型的基础上增加一些不太复杂的并行操作</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>水平型</th>
<th>垂直型</th>
</tr>
</thead>
<tbody><tr>
<td>并行能力</td>
<td>并行能力强、灵活性高、效率高</td>
<td>较差</td>
</tr>
<tr>
<td>执行时间</td>
<td>短</td>
<td>长</td>
</tr>
<tr>
<td>长度</td>
<td>微指令字较长，微程序较短</td>
<td>微指令字较短，微程序较长</td>
</tr>
<tr>
<td>难易程度</td>
<td>难</td>
<td>易</td>
</tr>
</tbody></table>
<h4 id="动态微程序设计和毫微程序设计"><a href="#动态微程序设计和毫微程序设计" class="headerlink" title="动态微程序设计和毫微程序设计"></a>动态微程序设计和毫微程序设计</h4><ul>
<li><p>动态微程序设计：根据用户的要求改变微程序，需要可写控制寄存器，使用EPROM</p>
</li>
<li><p>毫微程序设计：硬件不由微程序直接控制，通过存放在第二级控制存储器中的毫微程序来解释</p>
</li>
</ul>
<h4 id="微程序控制器和硬布线控制器比较"><a href="#微程序控制器和硬布线控制器比较" class="headerlink" title="微程序控制器和硬布线控制器比较"></a>微程序控制器和硬布线控制器比较</h4><table>
<thead>
<tr>
<th></th>
<th>微程序控制器</th>
<th>硬布线控制器</th>
</tr>
</thead>
<tbody><tr>
<td>工作原理</td>
<td>微操作控制信号以微程序的形式存放在控制存储器中，执行指令时读出即可</td>
<td>微操作控制信号由组合逻辑电路根据当前的指令码、状态和时序，即时产生</td>
</tr>
<tr>
<td>执行速度</td>
<td>慢</td>
<td>快</td>
</tr>
<tr>
<td>规整性</td>
<td>较规整</td>
<td>烦琐、不规整</td>
</tr>
<tr>
<td>应用场合</td>
<td>CISC CPU</td>
<td>RISC CPU</td>
</tr>
<tr>
<td>易扩充性</td>
<td>容易</td>
<td>困难</td>
</tr>
</tbody></table>
<h3 id="异常（内中断）和中断"><a href="#异常（内中断）和中断" class="headerlink" title="异常（内中断）和中断"></a>异常（内中断）和中断</h3><p>异常是由CPU内部产生的意外事件，分为硬故障中断和程序性异常<br>硬故障中断：是由硬连线出现异常引起（存储器校验异常、总线错误）<br>程序性异常（软件中断）：CPU内部因执行指令而引起的（整除0、溢出、断点、单步跟踪、非法指令）<br>按照异常发生的原因和返回方式，可进一步分为故障、自陷、终止</p>
<ul>
<li><p>故障（Fault）<br>引起故障的指令启动后、执行结束前被检测到的异常，因为无法通过异常处理程序恢复故障，因此不能回到原断点处执行，必须终止进程的执行  </p>
</li>
<li><p>自陷（Trap）<br>事先安排的一种异常事件，事先在程序中用一条特殊指令或通过某种方式设定特殊控制标志人为设置一个陷阱，当执行到被设置了陷阱的指令时，CPU在执行陷阱指令后，自动根据不同的陷阱类型进行相应的处理，然后返回到自陷程序下一条指令执行。当自陷指令是转移指令时，并不是返回到下一条指令执行，而不是返回到转移目标指令执行</p>
</li>
<li><p>终止（Abort）<br>如果在执行指令的过程中发生了使计算机无法继续执行的硬件故障，程序将无法继续执行，只能终止，此时调出中断服务程序来重启系统，终止异常和外中断属于硬件中断</p>
</li>
</ul>
<p>中断是来自CPU外部、与CPU执行指令无关的事件引起的中断<br>中断的分类：</p>
<ul>
<li><p>可屏蔽中断<br>通过可屏蔽中断请求线INTR向CPU发出中断请求，CPU可通过设置相应的屏蔽字来屏蔽或不屏蔽某个中断</p>
</li>
<li><p>不可屏蔽中断<br>通过专门的不可屏蔽中断请求NMI向CPU发出的中断请求，通常是非常紧急的硬件故障</p>
</li>
</ul>
<p>中断和异常的不同：  </p>
<ul>
<li><p>缺页或溢出等异常事件是由特定指令在执行过程中产生的，而中断不和任何指令相关联，也不阻止任何指令的完成  </p>
</li>
<li><p>异常的检测由CPU完成，不通过外部的某个信号通知CPU，中断CPU必须通过中断请求线获取中断源信息，才能知道哪个设备发生了何种中断</p>
</li>
</ul>
<h3 id="异常和中断响应过程"><a href="#异常和中断响应过程" class="headerlink" title="异常和中断响应过程"></a>异常和中断响应过程</h3><ul>
<li>关中断</li>
<li>保存断点和程序状态</li>
<li>识别异常和中断并转到相应的处理程序（软件识别和硬件识别）</li>
</ul>
<p>软件识别：CPU设置一个异常状态寄存器，用于记录异常原因，操作系统使用一个统一的异常和中断查询程序，按优先级顺序查询异常状态寄存器，以检查异常和中断类型，先查询到的先处理，然后转到内核中相应的处理程序</p>
<p>硬件识别（向量中断）：异常或中断处理程序的首地址称为中断向量，所有中断向量都存放于中断向量表中，每个异常或中断都被指定一个中断类型号，在中断向量表内，类型号和中断向量一一对应</p>
<h2 id="指令流水线"><a href="#指令流水线" class="headerlink" title="指令流水线"></a>指令流水线</h2><p>从两方面提高处理机的并行性：  </p>
<ul>
<li>时间并行：流水线技术</li>
<li>空间并行：超标量处理机</li>
</ul>
<h3 id="指令流水的定义"><a href="#指令流水的定义" class="headerlink" title="指令流水的定义"></a>指令流水的定义</h3><p>一条指令的执行过程分解为几个阶段，每个阶段由相应的功能部件完成</p>
<ul>
<li>取指IF</li>
<li>译码&#x2F;读寄存器ID</li>
<li>执行&#x2F;计算地址EX</li>
<li>访存MEM</li>
<li>写回WB</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/%E4%B8%AD%E5%A4%AE%E5%A4%84%E7%90%86%E5%99%A8/" data-id="clkqazuad001mdlbias8bcq8v" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LI_001_introduce-linux" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/LI_001_introduce-linux/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><p><img src="https://cdn.nlark.com/yuque/0/2021/jpeg/450565/1618455861307-31f350fc-c2e5-42f6-9b06-4dcc211ab168.jpeg" alt="image"></p>
<p><strong>Linux</strong> 是一种<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%94%B1%E5%8F%8A%E5%BC%80%E6%94%BE%E6%BA%90%E4%BB%A3%E7%A0%81%E8%BD%AF%E4%BB%B6">自由和开放源码</a>的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%B1%BBUnix%E7%B3%BB%E7%BB%9F">类UNIX</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BD%9C%E6%A5%AD%E7%B3%BB%E7%B5%B1">操作系统</a>。该操作系统的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%86%85%E6%A0%B8">内核</a>由<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%9E%97%E7%BA%B3%E6%96%AF%C2%B7%E6%89%98%E7%93%A6%E5%85%B9">林纳斯·托瓦兹</a>在1991年10月5日首次发布，在加上<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%BD%BF%E7%94%A8%E8%80%85%E7%A9%BA%E9%96%93">用户空间</a>的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%87%89%E7%94%A8%E7%A8%8B%E5%BC%8F">应用程序</a>之后，成为Linux操作系统。Linux也是<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%94%B1%E8%BD%AF%E4%BB%B6">自由软件</a>和<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%BC%80%E6%94%BE%E6%BA%90%E4%BB%A3%E7%A0%81%E8%BD%AF%E4%BB%B6">开放源代码软件</a>发展中最著名的例子。只要遵循<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GNU%E9%80%9A%E7%94%A8%E5%85%AC%E5%85%B1%E8%AE%B8%E5%8F%AF%E8%AF%81">GNU 通用公共许可证</a>（GPL），任何个人和机构都可以自由地使用Linux的所有底层<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</a>，也可以自由地修改和再发布。大多数Linux系统还包括像提供<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GUI">GUI</a>的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/X_Window">X Window</a>之类的程序。除了一部分专家之外，大多数人都是直接使用<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Linux%E7%99%BC%E8%A1%8C%E7%89%88">Linux 发行版</a>，而不是自己选择每一样组件或自行设置。</p>
<p><strong>Linux</strong>严格来说是单指操作系统的内核，因操作系统中包含了许多<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GUI">用户图形接口</a>和其他实用工具。如今Linux常用来指基于Linux的完整操作系统，内核则改以<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Linux%E5%86%85%E6%A0%B8">Linux内核</a>称之。由于这些支持用户空间的系统工具和库主要由<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%90%86%E6%9F%A5%E5%BE%B7%C2%B7%E6%96%AF%E6%89%98%E6%9B%BC">理查德·斯托曼</a>于1983年发起的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GNU%E8%A8%88%E5%8A%83">GNU计划</a>提供，<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%94%B1%E8%BD%AF%E4%BB%B6%E5%9F%BA%E9%87%91%E4%BC%9A">自由软件基金会</a>提议将其组合系统命名为<strong>GNU&#x2F;Linux</strong>，但Linux不属于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GNU%E8%A8%88%E5%8A%83">GNU计划</a>，这个名称并没有得到社区的一致认同。</p>
<p>Linux最初是作为支持<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%8B%B1%E7%89%B9%E5%B0%94">英特尔</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/X86">x86</a>架构的个人电脑的一个自由操作系统。目前Linux已经被移植到更多的计算机<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%A1%AC%E4%BB%B6">硬件</a>平台，远远超出其他任何操作系统。Linux可以运行在<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%9C%8D%E5%8A%A1%E5%99%A8">服务器</a>和其他大型平台之上，如<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%A4%A7%E5%9E%8B%E8%AE%A1%E7%AE%97%E6%9C%BA">大型计算机</a>和<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%B6%85%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA">超级计算机</a>。世界上500个最快的超级计算机已100％运行Linux发行版或变种。Linux也广泛应用在<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%B3%BB%E7%BB%9F">嵌入式系统</a>上，如<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%89%8B%E6%9C%BA">手机</a>（Mobile Phone）、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%B9%B3%E6%9D%BF%E9%9B%BB%E8%85%A6">平板电脑</a>（Tablet）、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%B7%AF%E7%94%B1%E5%99%A8">路由器</a>（Router）、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%94%B5%E8%A7%86">电视</a>（TV）和<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%94%B5%E5%AD%90%E6%B8%B8%E6%88%8F%E6%9C%BA">电子游戏机</a>等。在<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%A7%BB%E5%8A%A8%E8%AE%BE%E5%A4%87">移动设备</a>上广泛使用的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Android">Android</a>操作系统就是创建在Linux内核之上。</p>
<p>通常情况下，Linux被打包成供个人计算机和服务器使用的Linux发行版，一些流行的主流Linux发布版，包括<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Debian">Debian</a>（及其派生版本<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Ubuntu">Ubuntu</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Linux_Mint">Linux Mint</a>）、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Fedora">Fedora</a>（及其相关版本<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Red_Hat_Enterprise_Linux">Red Hat Enterprise Linux</a>、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/CentOS">CentOS</a>）和<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/OpenSUSE">openSUSE</a>等。Linux发行版包含Linux内核和支撑内核的实用<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A8%8B%E5%BA%8F">程序</a>和库，通常还带有大量可以满足各类需求的应用程序。个人计算机使用的Linux发行版通常包含<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/X_Window">X Window</a>和一个相应的桌面环境，如<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GNOME">GNOME</a>或<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/KDE">KDE</a>。桌面Linux操作系统常用的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F">应用程序</a>，包括<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Firefox">Firefox</a>网页浏览器、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/LibreOffice">LibreOffice</a>办公软件、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/GIMP">GIMP</a>图像处理工具等。由于Linux是自由软件，任何人都可以创建一个符合自己需求的Linux发行版。</p>
<h3 id="1、什么是虚拟机（virtual-machine）？"><a href="#1、什么是虚拟机（virtual-machine）？" class="headerlink" title="1、什么是虚拟机（virtual machine）？"></a>1、什么是虚拟机（virtual machine）？</h3><p>　　通过软件技术 模拟出来的一台虚拟的计算机，使用起来与真实的计算机类似。</p>
<h3 id="2、虚拟机软件"><a href="#2、虚拟机软件" class="headerlink" title="2、虚拟机软件"></a>2、虚拟机软件</h3><p>　　虚拟机软件可以生成虚拟机，且可以同时运行多个不同的操作系统。举个例子：　　现有一个装有 Windows 系统的计算机，在上面安装了一个虚拟机软件（比如 VMware），VMware 里又装有 Linux、Mac OS等操作系统，则装有 Windows 系统被称为 宿主机，而 Linux、Mac OS 被称为 虚拟机。</p>
<h3 id="3、Windows系统下-安装虚拟机-–-VMware"><a href="#3、Windows系统下-安装虚拟机-–-VMware" class="headerlink" title="3、Windows系统下 安装虚拟机 – VMware"></a>3、Windows系统下 安装虚拟机 – VMware</h3><p>（1）Step1：下载虚拟机软件。　　</p>
<p>官网地址：<a target="_blank" rel="noopener" href="https://www.vmware.com/products/workstation-pro/workstation-pro-evaluation.html">https://www.vmware.com/products/workstation-pro/workstation-pro-evaluation.html</a></p>
<p>　　使用百度云盘下载亦可：　　　　</p>
<p>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/13qT3rTAVSUGKTuDRckNPVg">https://pan.baidu.com/s/13qT3rTAVSUGKTuDRckNPVg</a>　　　　提取码：w4p4</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455214205-5e5791ba-c4a2-49e7-b4c1-ec0963fa943e.jpg" alt="image"></p>
<p>（2）Step2：下载相关系统的镜像文件。– CentOS7　　阿里云镜像地址： <a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/centos/8.3.2011/isos/x86_64/">https://mirrors.aliyun.com/centos/8.3.2011/isos/x86_64/</a></p>
<p>　　可以自行选择镜像：<a target="_blank" rel="noopener" href="https://developer.aliyun.com/mirror/">https://developer.aliyun.com/mirror/</a></p>
<h3 id="4、VMware-14-安装-Linux-系统"><a href="#4、VMware-14-安装-Linux-系统" class="headerlink" title="4、VMware 14 安装 Linux 系统"></a>4、VMware 14 安装 Linux 系统</h3><p>（1）Step1：双击运行 VMware，输入密钥 或者 试用。</p>
<p>【密钥：】</p>
<p>   CG54H-D8D0H-H8DHY-C6X7X-N2KG6</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455214188-8cc5d2ce-65db-47f9-9bc2-0d2055bb3c97.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455214169-aef05676-c5ea-42bf-a54b-c89e1f3dc07a.jpg" alt="image"></p>
<p>（2）Step2：创建新的虚拟机</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455214206-01236339-d837-4258-8649-0bca25958c58.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455214354-a1137984-5351-4063-8be5-34b636a16ad5.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455215015-f81c40ef-781d-4bbd-b722-de22d2fabd43.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455215099-1fd0745f-23d1-4ab8-ae90-44f197f294f9.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455215151-d2ebb6c8-2047-4f39-acc8-338dfb038ec4.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455215407-24110645-943d-4681-83c3-02d0d3671eaf.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455215619-c81a50cb-5287-4c6a-b083-6b479477f3ff.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455215755-24cde419-4ea5-4b44-84bb-01a0f8c6c10c.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455215965-7fc907e2-9e0b-4484-b093-ad0bc402e4a1.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455216139-1ad38d11-4cf3-4390-99ea-dacc30384a3e.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455216312-5bfc8647-7c42-4df6-85bb-a9895e6897ed.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455216494-16968ac1-e133-4619-bf2c-2f5f54052571.jpg" alt="image"></p>
<p>（3）Step3：运行虚拟机，并安装、配置 CentOS7.</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455216665-5c5463d0-34ff-4844-8a9d-692726db63a8.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455216963-a098b216-2229-4f74-ae65-9b81393aebbd.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455217416-7cb6eef2-8676-4610-a8d6-53b6228f8fb8.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455217667-3317df2a-aee4-4640-bb88-6be7f413b45d.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455218306-df3e2f0a-0c1b-4d91-8ce9-7eab7f13e7b9.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455218317-73c91c14-9418-42cf-ba9d-b2bd5821e3d2.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455218825-6396ea38-ae83-46ff-810e-f08b3f65e804.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455219006-acadffe8-bff1-4856-a94f-98ab53e97402.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455219170-d624ae37-40a1-417f-b67e-f186e902d788.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455219341-9490eaf3-3fe6-4072-9f5c-0718cc63ae4a.jpg" alt="image"></p>
<p>（4）Step4：重启后，接受协议，登录虚拟机。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455219373-0e39d2bb-1d13-4273-8d77-a2f9d13d68a1.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455219668-6a898840-ae2f-4356-8dcc-685afd83c40b.jpg" alt="image"></p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455219603-692ff53a-e740-497c-b229-de83185daf69.jpg" alt="image"></p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="Linux环境模拟："><a href="#Linux环境模拟：" class="headerlink" title="Linux环境模拟："></a>Linux环境模拟：</h3><p><a target="_blank" rel="noopener" href="http://cb.vu/">http://cb.vu/</a></p>
<p><a target="_blank" rel="noopener" href="https://bellard.org/jslinux/">https://bellard.org/jslinux/</a></p>
<h3 id="linux-为何物"><a href="#linux-为何物" class="headerlink" title="linux 为何物"></a>linux 为何物</h3><p>Linux 就是一个操作系统，就像你多少已经了解的 Windows（xp，7，8）和 Mac OS 。至于操作系统是什么，就不用过多解释了，如果你学习过前面的入门课程，应该会有个基本概念了，这里简单介绍一下操作系统在整个计算机系统中的角色。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455297583-c2b45d9e-b854-4ad9-b2be-7398a8a7a8e2.jpg" alt="image"></p>
<p>我们的 Linux 主要是系统调用和内核那两层。当然直观地看，我们使用的操作系统还包含一些在其上运行的应用程序，比如文本编辑器、浏览器、电子邮件等。</p>
<h3 id="linux历史简介"><a href="#linux历史简介" class="headerlink" title="linux历史简介"></a>linux历史简介</h3><p>操作系统始于二十世纪五十年代，当时的操作系统能运行批处理程序。批处理程序不需要用户的交互，它从文件或者穿孔卡片读取数据，然后输出到另外一个文件或者打印机。</p>
<p>二十世纪六十年代初，交互式操作系统开始流行。它不仅仅可以交互，还能使多个用户从不同的终端同时操作主机。这样的操作系统被称作分时操作系统，它的出现对批处理操作系统是个极大的挑战。许多人尝试开发分时操作系统， 其中包括一些大学的研究项目和商业项目。当时有个项目叫做 Multics ，它的技术在当时很具有创新性。 Multics 项目的开发并不顺利，它花费了远超过预计的资金，却没有在操作系统市场上占到多少份额。而参加该项目的一个开发团体——贝尔实验室退出了这个项目。他们在退出后开发了他们自己的一个操作系统—— UNIX 。</p>
<p>UNIX 最初免费发布并因此在大学里受到欢迎。后来，UNIX 实现了 TCP&#x2F;IP 协议栈，成为了早期工作站的操作系统的一个流行选择。</p>
<p>1990 年，UNIX 在服务器市场上尤其是大学校园中成为主流操作系统，许多校园都有 UNIX 主机，当然还包括一些研究它的计算机系的学生。这些学生都渴望能在自己的电脑上运行 UNIX 。不幸的是，从那时候开始，UNIX 开始变得商业化，它的价格也变得非常昂贵。而唯一低廉的选择就是 MINIX，这是一个功能有限的类似 UNIX 的操作系统，作者 Andrew Tanenbaum 开发它的目的是用于教学。</p>
<p>1991 年 10 月，Linus Torvalds（Linux 之父）在赫尔辛基大学接触 UNIX，他希望能在自己的电脑上运行一个类似的操作系统。可是 UNIX 的商业版本非常昂贵，于是他从 MINIX 开始入手，计划开发一个比 MINIX 性能更好的操作系统。很快他就开始了自己的开发工作。他第一次发行的版本迅速吸引了一些黑客。尽管最初的 Linux 并没有多少用处，但由于一些黑客的加入使它很快就具有了许多吸引人的特性，甚至一些对操作系统开发不感兴趣的人也开始关注它。</p>
<p>Linux 本身只是操作系统的内核。内核是使其它程序能够运行的基础。它实现了多任务和硬件管理，用户或者系统管理员交互运行的所有程序实际上都运行在内核之上。其中有些程序是必需的，比如说，命令行解释器（shell），它用于用户交互和编写 shell 脚本。 Linus 没有自己去开发这些应用程序，而是使用已有的自由软件。这减少了搭建开发环境所需花费的工作量。实际上，他经常改写内核，使得那些程序能够更容易地在 Linux 上运行。许多重要的软件，包括 C 编译器，都来自于自由软件基金 GNU 项目。GNU 项目开始于 1984 年，目的是为了开发一个完全类似于 UNIX 的免费操作系统。为了表扬 GNU 对 Linux 的贡献，许多人把 Linux 称为 GNU&#x2F;Linux（GNU 有自己的内核）。</p>
<p>1992－1993 年，Linux 内核具备了挑战 UNIX 的所有本质特性，包括 TCP&#x2F;IP 网络，图形界面系统（X window )，Linux 同样也吸引了许多行业的关注。一些小的公司开始开发和发行 Linux，有几十个 Linux 用户社区成立。1994 年，Linux 杂志也开始发行。</p>
<p>Linux 内核 1.0 在 1994 年 3 月发布，内核的发布要经历许多开发周期，直至达到一个稳定的版本。</p>
<p>下面列举一些 Linux 诞生大事件：</p>
<ul>
<li>1965 年，Bell 实验室、MIT、GE（通用电气公司）准备开发 Multics 系统，为了同时支持 300 个终端访问主机，但是 1969 年失败了；那时候并没有鼠标、键盘，输入设备，只有卡片机。因此，如果要测试某个程序，则需要将读卡纸插入卡片机，如果有错误，还需要重新来过；Multics：Multiplexed Information and Computing Service；</li>
<li>1969 年，Ken Thompson（C 语言之父）利用汇编语言开发了 File Server System（Unics，即 UNIX 的原型）；因为汇编语言对于硬件的依赖性，因此只能针对特定硬件； 只是为了移植一款“太空旅游”的游戏；</li>
<li>1973 年，Dennis Ritchie 和 Ken Thompson 发明了 C 语言，而后写出了 UNIX 的内核；将 B 语言改成 C 语言，由此产生了 C 语言之父；90% 的代码是 C 语言写的，10% 的代码用汇编语言写的，因此移植时只要修改那 10% 的代码即可；</li>
<li>1977 年，Berkeley 大学的 Bill Joy 针对他的机器修改了 UNIX 源码，称为 BSD（Berkeley Software Distribution）；Bill Joy 是 Sun 公司的创始人；</li>
<li>1979 年，UNIX 发布 System V，用于个人计算机；</li>
<li>1984 年，因为 UNIX 规定“不能对学生提供源码”，Tanenbaum 老师自己编写兼容于 UNIX 的 Minix，用于教学；</li>
<li>1984 年，Stallman 开始 GNU（GNU’s Not Unix）项目，创办 FSF（Free Software Foundation）基金会；产品：GCC、Emacs、Bash Shell、GLIBC；倡导“自由软件”；GNU 的软件缺乏一个开放的平台运行，只能在 UNIX 上运行；自由软件指用户可以对软件做任何修改，甚至再发行，但是始终要挂着 GPL 的版权；自由软件是可以卖的，但是不能只卖软件，而是卖服务、手册等；</li>
<li>1985 年，为了避免 GNU 开发的自由软件被其他人用作专利软件，因此创建 GPL（General Public License）版权声明；</li>
<li>1988 年，MIT 为了开发 GUI，成立了研发 XFree86 的组织；</li>
<li>1991 年，芬兰赫尔辛基大学的研究生 Linus Torvalds 基于 gcc、bash 开发了针对 386 机器的 Linux 内核；</li>
<li>1994 年，Torvalds 发布 Linux-v1.0；</li>
<li>1996 年，Torvalds 发布 Linux-v2.0，确定了 Linux 的吉祥物：企鹅。</li>
</ul>
<p>UNIX 进化史（UNIX 大家族族谱 1969-2013）：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455298848-31bac66b-173d-4b54-a4ac-80358ce22427.jpg" alt="image"></p>
<h3 id="linux与-Windows-到底有哪些不同"><a href="#linux与-Windows-到底有哪些不同" class="headerlink" title="linux与 Windows 到底有哪些不同"></a>linux与 Windows 到底有哪些不同</h3><h4 id="1-免费与收费"><a href="#1-免费与收费" class="headerlink" title="1. 免费与收费"></a>1. 免费与收费</h4><ul>
<li>最新正版 Windows 10，需要付费购买；</li>
<li>Linux 免费或少许费用。</li>
</ul>
<h4 id="2-软件与支持"><a href="#2-软件与支持" class="headerlink" title="2. 软件与支持"></a>2. 软件与支持</h4><ul>
<li>Windows 平台：数量和质量的优势，不过大部分为收费软件；由微软官方提供重要支持和服务；</li>
<li>Linux 平台：大都为开源自由软件，用户可以修改定制和再发布，由于基本免费没有资金支持，部分软件质量和体验欠缺；由全球所有的 Linux 开发者和自由软件社区提供支持。</li>
</ul>
<h4 id="3-安全性"><a href="#3-安全性" class="headerlink" title="3. 安全性"></a>3. 安全性</h4><ul>
<li>Windows 平台：三天两头打补丁安装系统安全更新，还是会中病毒木马；</li>
<li>Linux 平台：要说 Linux 没有安全问题，那当然是不可能的，这一点仁者见仁智者见智，相对来说肯定比 Windows 平台要更加安全，使用 Linux 你也不用装某杀毒、某毒霸。</li>
</ul>
<h4 id="4-使用习惯"><a href="#4-使用习惯" class="headerlink" title="4. 使用习惯"></a>4. 使用习惯</h4><ul>
<li>Windows：普通用户基本都是纯图形界面下操作使用，依靠鼠标和键盘完成一切操作，用户上手容易，入门简单；</li>
<li>Linux：兼具图形界面操作（需要使用带有桌面环境的发行版）和完全的命令行操作，可以只用键盘完成一切操作，新手入门较困难，需要一些学习和指导（这正是我们要做的事情），一旦熟练之后效率极高。</li>
</ul>
<h4 id="5-可定制性"><a href="#5-可定制性" class="headerlink" title="5. 可定制性"></a>5. 可定制性</h4><ul>
<li>Windows：这些年之前算是全封闭的，系统可定制性很差；</li>
<li>Linux：你想怎么做就怎么做，Windows 能做到得它都能，Windows 做不到的，它也能。</li>
</ul>
<h4 id="6-应用范畴"><a href="#6-应用范畴" class="headerlink" title="6. 应用范畴"></a>6. 应用范畴</h4><p>或许你之前不知道 Linux ，要知道，你之前在 Windows 使用百度、谷歌，上淘宝，聊 QQ 时，支撑这些软件和服务的，是后台成千上万的 Linux 服务器主机，它们时时刻刻都在忙碌地进行着数据处理和运算，可以说世界上大部分软件和服务都是运行在 Linux 之上的。</p>
<h4 id="7-Windows-没有的"><a href="#7-Windows-没有的" class="headerlink" title="7. Windows 没有的"></a>7. Windows 没有的</h4><ul>
<li>稳定的系统</li>
<li>安全性和漏洞的快速修补</li>
<li>多用户</li>
<li>用户和用户组的规划</li>
<li>相对较少的系统资源占用</li>
<li>可定制裁剪，移植到嵌入式平台（如安卓设备）</li>
<li>可选择的多种图形用户界面（如 GNOME，KDE）</li>
</ul>
<h4 id="8-Linux-没有的"><a href="#8-Linux-没有的" class="headerlink" title="8. Linux 没有的"></a>8. Linux 没有的</h4><ul>
<li>特定的支持厂商</li>
<li>足够的游戏娱乐支持度</li>
<li>足够的专业软件支持度</li>
</ul>
<h3 id="如何学习Linux"><a href="#如何学习Linux" class="headerlink" title="如何学习Linux"></a>如何学习Linux</h3><ul>
<li>明确目的：你是要用 Linux 来干什么，搭建服务器、做程序开发、日常办公，还是娱乐游戏；</li>
<li>面对现实：Linux 大都在命令行下操作，能否接受不用或少用图形界面；</li>
<li>是学习 Linux 操作系统本身还是某一个 Linux 发行版（<a target="_blank" rel="noopener" href="http://www.ubuntu.com/">Ubuntu</a>，<a target="_blank" rel="noopener" href="http://www.centos.org/">CentOS</a>，<a target="_blank" rel="noopener" href="http://fedoraproject.org/">Fedora</a>，<a target="_blank" rel="noopener" href="http://www.opensuse.org/">OpenSUSE</a>，<a target="_blank" rel="noopener" href="http://www.debian.org/">Debian</a>，<a target="_blank" rel="noopener" href="http://linuxmint.com/">Mint</a> 等等），如果你对发行版的概念或者它们之间的关系不明确的话可以参看 <a target="_blank" rel="noopener" href="http://baike.baidu.com/view/897468.htm">Linux 发行版</a>。</li>
<li>注重基础，从头开始，可参考菜鸟教程</li>
</ul>
<h3 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h3><p>通常在图形界面中对实际体验带来差异的不是上述的不同发行版的各种终端模拟器，而是这个 Shell（壳）。有壳就有核，这里的核就是指 UNIX&#x2F;Linux 内核，Shell 是指“提供给使用者使用界面”的软件（命令解析器），类似于 DOS 下的 command（命令行）和后来的 cmd.exe 。</p>
<p>普通意义上的 Shell 就是可以接受用户输入命令的程序。它之所以被称作 Shell 是因为它隐藏了操作系统底层的细节。同样的 UNIX&#x2F;Linux 下的图形用户界面 GNOME 和 KDE，有时也被叫做“虚拟 shell”或“图形 shell”。</p>
<p>UNIX&#x2F;Linux 操作系统下的 Shell 既是用户交互的界面，也是<strong>控制系统的脚本语言</strong>。当然这一点也有别于 Windows 下的命令行，虽然该命令行也提供了很简单的控制语句。在 Windows 操作系统下，有些用户从来都不会直接使用 Shell，然而在 UNIX 系列操作系统下，Shell 仍然是控制系统启动、X11 启动和很多其它实用工具的脚本解释程序。</p>
<p>在 UNIX&#x2F;Linux 中比较流行的常见的 Shell 有 bash、zsh、ksh、csh 等等，Ubuntu 终端默认使用的是 bash，默认的桌面环境是 GNOME 或者 Unity（基于 GNOME）</p>
<h3 id="命令行操作体验"><a href="#命令行操作体验" class="headerlink" title="命令行操作体验"></a>命令行操作体验</h3><p>在 linux 中，最最重要的就是命令，这就包含了 2 个过程，输入和输出</p>
<ul>
<li>输入：输入当然就是打开终端，然后按键盘输入，然后按回车，输入格式一般就是这类的</li>
</ul>
<p>#创建一个名为 file 的文件，touch是一个命令</p>
<p>touch file</p>
<p>#进入一个目录，cd是一个命令</p>
<p>cd &#x2F;etc&#x2F;</p>
<p>#查看当前所在目录</p>
<p>pwd</p>
<ul>
<li>输出：输出会返回你想要的结果，比如你要看什么文件，就会返回文件的内容。如果只是执行，执行失败会告诉你哪里错了，如果执行成功那么会没有输出，因为 linux 的哲学就是：没有结果就是最好的结果</li>
</ul>
<h4 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h4><p>打开终端后系统会自动运行 Shell 程序，然后我们就可以输入命令让系统来执行了：</p>
<p>echo “hello world”</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/101478169">https://zhuanlan.zhihu.com/p/101478169</a></p>
<h4 id="1-重要快捷键"><a href="#1-重要快捷键" class="headerlink" title="1) 重要快捷键"></a>1) 重要快捷键</h4><p>真正学习命令行之前，你先要掌握几个十分有用、必需掌握的小技巧：</p>
<h5 id="Tab"><a href="#Tab" class="headerlink" title="[Tab]"></a>[Tab]</h5><p>使用Tab键来进行命令补全，Tab键一般是在字母Q旁边，这个技巧给你带来的最大的好处就是当你忘记某个命令的全称时可以只输入它的开头的一部分，然后按下Tab键就可以得到提示或者帮助完成：</p>
<p>当然不止补全命令，补全目录、补全命令参数都是没问题的：</p>
<h5 id="Ctrl-c"><a href="#Ctrl-c" class="headerlink" title="[Ctrl+c]"></a>[Ctrl+c]</h5><p>想想你有没有遇到过这种情况，当你在 Linux 命令行中无意输入了一个不知道的命令，或者错误地使用了一个命令，导致在终端里出现了你无法预料的情况，比如，屏幕上只有光标在闪烁却无法继续输入命令，或者不停地输出一大堆你不想要的结果。你想要立即停止并恢复到你可控的状态，那该怎么办呢？这时候你就可以使用Ctrl+c键来强行终止当前程序（你可以放心它并不会使终端退出）。</p>
<p>尝试输入以下命令：</p>
<p>tail</p>
<p>然后你会发现你接下来的输入都没有任何反应了，只是将你输入的东西显示出来，现在你可以使用Ctrl+c，来中断这个你目前可能还不知道是什么的程序（在后续课程中我们会具体解释这个tail命令是什么）。</p>
<p>又或者输入：</p>
<p>find &#x2F;</p>
<p>显然这不是你想的结果，可以使用Ctrl+c结束。</p>
<p>虽然这个按着很方便，但不要随便按，因为有时候，当你看到终端没有任何反应或提示，也不能接受你的输入时，可能只是运行的程序需要你耐心等一下，就不要急着按Ctrl+c了。</p>
<h5 id="其他一些常用快捷键"><a href="#其他一些常用快捷键" class="headerlink" title="其他一些常用快捷键"></a>其他一些常用快捷键</h5><p><strong>按键****作用</strong></p>
<h4 id="2-学会利用历史输入命令"><a href="#2-学会利用历史输入命令" class="headerlink" title="2) 学会利用历史输入命令"></a>2) 学会利用历史输入命令</h4><p>很简单，你可以使用键盘上的方向上键↑，恢复你之前输入过的命令，你一试便知。</p>
<h4 id="3-学会使用通配符"><a href="#3-学会使用通配符" class="headerlink" title="3) 学会使用通配符"></a>3) 学会使用通配符</h4><p>通配符是一种特殊语句，主要有星号（*）和问号（?），用来对字符串进行模糊匹配（比如文件名、参数名）。当查找文件夹时，可以使用它来代替一个或多个真正字符；当不知道真正字符或者懒得输入完整名字时，常常使用通配符代替一个或多个真正字符。</p>
<p>终端里面输入的通配符是由 Shell 处理的，不是由所涉及的命令语句处理的，它只会出现在命令的“参数值”里（它不能出现在命令名称里， 命令不记得，那就用Tab补全）。当 Shell 在“参数值”中遇到了通配符时，Shell 会将其当作路径或文件名在磁盘上搜寻可能的匹配：若符合要求的匹配存在，则进行代换（路径扩展）；否则就将该通配符作为一个普通字符传递给“命令”，然后再由命令进行处理。总之，通配符实际上就是一种 Shell 实现的路径扩展功能。在通配符被处理后， Shell 会先完成该命令的重组，然后继续处理重组后的命令，直至执行该命令。</p>
<p>首先回到用户家目录：</p>
<p>cd &#x2F;home&#x2F;shiyanlou</p>
<p>然后使用 touch 命令创建 2 个文件，后缀都为 txt：</p>
<p>touch asd.txt fgh.txt</p>
<p>可以给文件随意命名，假如过了很长时间，你已经忘了这两个文件的文件名，现在你想在一大堆文件中找到这两个文件，就可以使用通配符：</p>
<p>ls *.txt</p>
<p>在创建文件的时候，如果需要一次性创建多个文件，比如：<strong>“love_1_linux.txt，love_2_linux.txt，… love_10_linux.txt”</strong>。在 Linux 中十分方便：</p>
<p>touch love_{1..10}_shiyanlou.txt</p>
<p>Shell 常用通配符：</p>
<p><strong>字符****含义</strong></p>
<h4 id="4-学会在命令行中获取帮助"><a href="#4-学会在命令行中获取帮助" class="headerlink" title="4) 学会在命令行中获取帮助"></a>4) 学会在命令行中获取帮助</h4><p>在 Linux 环境中，如果你遇到困难，可以使用man命令，它是Manual pages的缩写。</p>
<p>Manual pages 是 UNIX 或类 UNIX 操作系统中在线软件文档的一种普遍的形式， 内容包括计算机程序（包括库和系统调用）、正式的标准和惯例，甚至是抽象的概念。用户可以通过执行man命令调用手册页。</p>
<p>你可以使用如下方式来获得某个命令的说明和使用方式的详细介绍：</p>
<p>man <command_name></p>
<p>比如你想查看 man 命令本身的使用方式，你可以输入：</p>
<p>man man</p>
<p>通常情况下，man 手册里面的内容都是英文的，这就要求你有一定的英文基础。man 手册的内容很多，涉及了 Linux 使用过程中的方方面面。为了便于查找，man 手册被进行了分册（分区段）处理，在 Research UNIX、BSD、OS X 和 Linux 中，手册通常被分为 8 个区段，安排如下：</p>
<p><strong>区段****说明</strong></p>
<p>要查看相应区段的内容，就在 man 后面加上相应区段的数字即可，如：</p>
<p>man 1 ls</p>
<p>会显示第一区段中的ls命令 man 页面。</p>
<p>所有的手册页遵循一个常见的布局，为了通过简单的 ASCII 文本展示而被优化，而这种情况下可能没有任何形式的高亮或字体控制。一般包括以下部分内容：</p>
<p><strong>NAME（名称）</strong></p>
<p>该命令或函数的名称，接着是一行简介。</p>
<p><strong>SYNOPSIS（概要）</strong></p>
<p>对于命令，正式的描述它如何运行，以及需要什么样的命令行参数。对于函数，介绍函数所需的参数，以及哪个头文件包含该函数的定义。</p>
<p><strong>DESCRIPTION（说明）</strong></p>
<p>命令或函数功能的文本描述。</p>
<p><strong>EXAMPLES（示例）</strong></p>
<p>常用的一些示例。</p>
<p><strong>SEE ALSO（参见）</strong></p>
<p>相关命令或函数的列表。</p>
<p>也可能存在其它部分内容，但这些部分没有得到跨手册页的标准化。常见的例子包括：OPTIONS（选项），EXIT STATUS（退出状态），ENVIRONMENT（环境），BUGS（程序漏洞），FILES（文件），AUTHOR（作者），REPORTING BUGS（已知漏洞），HISTORY（历史）和 COPYRIGHT（版权）。</p>
<p>通常 man 手册中的内容很多，你可能不太容易找到你想要的结果，不过幸运的是你可以在 man 中使用搜索&#x2F;&lt;你要搜索的关键字&gt;，查找完毕后你可以使用n键切换到下一个关键字所在处，shift+n为上一个关键字所在处。使用Space（空格键）翻页，Enter（回车键）向下滚动一行，或者使用k，j（vim 编辑器的移动键）进行向前向后滚动一行。按下h键为显示使用帮助（因为 man 使用 less 作为阅读器，实为less工具的帮助），按下q退出。</p>
<p>想要获得更详细的帮助，你还可以使用info命令，不过通常使用man就足够了。如果你知道某个命令的作用，只是想快速查看一些它的某个具体参数的作用，那么你可以使用–help参数，大部分命令都会带有这个参数，如：</p>
<p>ls –help</p>
<h3 id="命令行一些说明"><a href="#命令行一些说明" class="headerlink" title="命令行一些说明"></a>命令行一些说明</h3><ol>
<li>‘-‘  单横杠一般为短命令行选项，一般单横岗后面跟一些简短的字母 如rm -f， mv -r , npm -D , npm -g，npm -h, npm -v</li>
<li>‘–’ 双横杠一般为长命令行选项，一般双横杠后面跟完整的单词。如 npm –help， npm –version, npm –save-dev, npm –global</li>
<li>上面的npm命令，npm -h和npm –help、npm -v和npm –version 、 npm -D和 npm –save-dev 等命令是等价的，只是写法不同，npm –save-dev中‘save-dev’是一个完整的单词，中间的横杠不是选项</li>
<li>命令行选项参数设置，如npm设置，npm install -g cnpm –registry&#x3D;<a target="_blank" rel="noopener" href="https://registry.npm.taobao.org和npm/">https://registry.npm.taobao.org和npm</a> install -g cnpm –registry <a target="_blank" rel="noopener" href="https://registry.npm.taobao.org/">https://registry.npm.taobao.org</a> , 这两个命令是等价的，–registry后面可以空格设置参数，也可用‘&#x3D;’设置，两者效果一样，区别在终端是否识别</li>
<li>短命令行的参数可以合并，如ls -al，rm -rf两个命令可以写成 ls -a -l 和 rm -r -f</li>
<li>命令行区分大小写，不同的大小写会有不同的功能，如ps -a和ps -A，前一个显示同一终端下的所有程序，后一个显示所有进程，不在局限在同一终端</li>
<li>!! 表示执行上一条命令，如 执行 ls -al 后发现没有权限，可以 sudo !!</li>
<li>linux命令行大小学敏感，既要区分大写</li>
</ol>
<h3 id="常用终端命令"><a href="#常用终端命令" class="headerlink" title="常用终端命令"></a>常用终端命令</h3><ol>
<li>pwd：获取当前所在路径；</li>
<li>ls：列出当前目录下的所有文件；ls -al ：以长格式列出所有目录下文件（包括隐藏的文件）-a代表所有，-l长格式显示</li>
<li>cd： 进行目录之间的相互跳转；cd ~ ：返回家目录；cd - ：打开上一个 cd 过的目录；cd ..&#x2F; : 返回上一层目录，cd - 和cd ..&#x2F;  这两个注意区别</li>
<li>mkdir、rmdir、mv [源地址]：创建、删除、移动文件夹，如果文件夹内不为空，则无法用rmdir删除文件夹；</li>
<li>rm -rf [源地址]：递归删除文件夹及文件夹内所有内容，无法挽回</li>
<li>cp -R [源地址] [目的地址]: 带参数-R是目录复制，不带是文件复制</li>
<li>history：查看之前执行过命令的历史记录，关闭终端后输入history将不会显示上个关闭终端的历史记录</li>
<li>ps：查看当前终端运行的程序；ps -A：显示所有进程</li>
<li>top：显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等，终端按q退出显示</li>
<li>kill [PID]：结束指定进程ID的进程，先使用top命令查看想要结束进程的PID，然后使用命令kill [PID结束进程</li>
<li>touch [文件名] ：创建文件</li>
<li>more [文件名] 或 cat [文件名]：查看文件内容</li>
<li>管道符号 ‘&gt;’ 、’&lt;’ 、’&lt; &gt;’：将文件作为输入内容， &gt; 左边的输出作为右边的输入，&lt; 右边的输出作为左边的输入，如：node test.js &gt; hello.txt，test.js的输出的内容会输入到hello.txt，打开hello.txt文件能看到test.js的输出内容；同理，node test.js &lt; hello.txt中，运行test.js的输入内容是hello.txt的内容；node test.js &lt; hello.txt &gt; world.txt中，test.js使用hello.txt内容作为输出，运行完成后将test.js运行输出写入world.txt，(刚开始时理解成了hello.txt既向 test.js输入内容，也向world.txt输入内容，运行后才发现理解错了)，‘&gt;’会将原有内容覆盖，‘&gt;&gt;’是内容追加，不覆盖</li>
<li>管道符号 ‘|’： 如 node test.js | node test_1.js，将test.js 的输出作为test_1.js的输入；’|’ 和 ‘&gt;’比较像，但有区别，个人理解的区别是，&gt; 一般作用于文件的输入输出，直接使用读取文件，而‘|’ 作用于程序运行的输入输出，读取文件内容需要通过命令如 cat hello.txt | node test_1.js 和 node test_1.js &lt; hello.txt 一样的效果。</li>
<li>tail和head：列出文件的结尾和文件的开头；tail -n 5 hello.txt：显示hello.txt的最后五行内容；tail -n +5 hello.txt：显示hello.txt的从第五行开始到结束的内容，区别在显示的行数添加了一个“+”；tail -f [文件名]：若有程序持续向该文件写入内容，则可以实时查看该文件的内容；head -n 5 hello.txt：显示hello.txt的前五行内容；head -n -5 hello.txt：输出文件除了最后5行的全部内容，head和tail效果相反</li>
<li>grep: 强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来；grep -n hello hello_world.txt：从hello_world.txt中找出hello位置的数据，并在前面加上行号；cat hello_world.txt | grep -n hello：和上面的效果一样，grep的输入内容使用了上面管道命令的输入cat hello_world.txt ；cat hello_world.txt | grep -n o* ：寻找hello_world.txt中包含“o”的行</li>
<li>ifconfig：列出本机所有的网络设备以及其上面的配置，主要使用ip地址和mac地址</li>
<li>which：查看可执行文件的位置，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果；which cd：&#x2F;usr&#x2F;bin&#x2F;cd ； which pwd ：bin&#x2F;pwd</li>
<li>find pathname -options [-print -exec -ok …]：用于在文件树种查找文件；如 find . -name “*.txt”：查找当前目录下一 .txt结尾的文件</li>
<li>man 命令：查看命令是的使用，如man cd、man rm等</li>
</ol>
<h3 id="常用网络命令"><a href="#常用网络命令" class="headerlink" title="常用网络命令"></a>常用网络命令</h3><ol>
<li>curl和wget：利用URL规则在命令行下工作的文件传输工具，；如：curl -o mybaidu.html <a target="_blank" rel="noopener" href="http://www.baidu.com/">http://www.baidu.com</a> 和wget -O mybaidu.html <a href="http://www.baidu.com：会将内容下载到本地baidu.html（mac">http://www.baidu.com：会将内容下载到本地baidu.html（mac</a> 需要下载wget命令）；根据命令行不同选项有强大的功能，具体可以参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/Downtime/p/8068097.html%EF%BC%9Bcurl%E5%92%8Cwget%E5%9F%BA%E7%A1%80%E5%8A%9F%E8%83%BD%E6%9C%89%E8%AF%B8%E5%A4%9A%E9%87%8D%E5%8F%A0%EF%BC%8Ccurl%E7%94%B1%E4%BA%8E%E5%8F%AF%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%84%E7%A7%8D%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%E6%89%80%E4%BB%A5%E5%9C%A8%E6%A8%A1%E6%8B%9Fweb%E8%AF%B7%E6%B1%82%E6%96%B9%E9%9D%A2%E6%9B%B4%E6%93%85%E9%95%BF%EF%BC%9Bwget%E7%94%B1%E4%BA%8E%E6%94%AF%E6%8C%81ftp%E5%92%8CRecursive%E6%89%80%E4%BB%A5%E5%9C%A8%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E6%96%B9%E9%9D%A2%E6%9B%B4%E6%93%85%E9%95%BF">https://www.cnblogs.com/Downtime/p/8068097.html；curl和wget基础功能有诸多重叠，curl由于可自定义各种请求参数所以在模拟web请求方面更擅长；wget由于支持ftp和Recursive所以在下载文件方面更擅长</a></li>
<li>ping：通常用来测试与目标主机的连通性；如ping -c 10 -i 5 <a target="_blank" rel="noopener" href="https://www.baidu.com/">www.baidu.com</a>：“-c”要发送数据包数目，“-i”数据包建个发送多少秒，每5秒向百度发送一个数据包，总共发送10次，</li>
<li>telnet ：通常用来远程登录；telnet <a target="_blank" rel="noopener" href="https://www.baidu.com/">www.baidu.com</a>：尝试登陆百度主机，如果能连上需要输入账号和密码</li>
<li>route：用于显示和操作IP路由表，也提供了路由增加、修改、删除等操作；route -n：显示当前路由信息（mac中route命令不起作用，可以使用netstat -nr查看）；</li>
<li>netstat：用于显示与IP、TCP、UDP和ICMP协议相关的统计数据，一般用于检验本机各端口的网络连接情况；netstat -a：列出所有的端口；netstat -nr：显示路由信息</li>
<li>traceroute：可以知道信息从你的计算机到互联网另一端的主机是走的什么路径；如 traceroute <a target="_blank" rel="noopener" href="https://www.baidu.com/">www.baidu.com</a>：能在信息界面看到路由是如何跳转的</li>
</ol>
<h3 id="Linux-目录结构"><a href="#Linux-目录结构" class="headerlink" title="Linux 目录结构"></a>Linux 目录结构</h3><p> Linux 的目录与 Windows 的目录是有区别的，或许对于一般操作上的感受来说没有多大不同，但从它们的实现机制来说是完全不同的。</p>
<p>一种不同是体现在目录与存储介质（磁盘，内存，DVD 等）的关系上，以往的 Windows 一直是以存储介质为主的，主要以盘符（C 盘，D 盘…）及分区来实现文件管理，然后之下才是目录，目录就显得不是那么重要，除系统文件之外的用户文件放在任何地方任何目录也是没有多大关系。所以通常 Windows 在使用一段时间后，磁盘上面的文件目录会显得杂乱无章（少数善于整理的用户除外吧）。然而 UNIX&#x2F;Linux 恰好相反，UNIX 是以目录为主的，Linux 也继承了这一优良特性。</p>
<p> Linux 是以树形目录结构的形式来构建整个系统的，可以理解为树形目录是一个用户可操作系统的骨架。虽然本质上无论是目录结构还是操作系统内核都是存储在磁盘上的，但从逻辑上来说 Linux 的磁盘是“挂在”（挂载在）目录上的，每一个目录不仅能使用本地磁盘分区的文件系统，也可以使用网络上的文件系统。举例来说，可以利用网络文件系统（Network File System，NFS）服务器载入某特定目录等。</p>
<h4 id="Linux系统各个目录的一些作用"><a href="#Linux系统各个目录的一些作用" class="headerlink" title="Linux系统各个目录的一些作用"></a>Linux系统各个目录的一些作用</h4><p><strong>文件系统的类型</strong></p>
<p>LINUX有四种基本文件系统类型：普通文件、目录文件、链接文件和特殊文件，可用file命令来识别.</p>
<ol>
<li>普通文件：</li>
</ol>
<p>如文本文件、C语言元代码、SHELL脚本、二进制的可执行文件等，可用cat、less、more、vi、emacs来察看内容，用mv来改名。</p>
<ol start="2">
<li>目录文件：</li>
</ol>
<p>包括文件名、子目录名及其指针。它是LINUX储存文件名的唯一地方，可用ls列出目录文件。</p>
<ol start="3">
<li>链接文件：</li>
</ol>
<p>是指向同一索引节点的那些目录条目。用ls来查看是，连接文件的标志用l开头，而文件面后以”-&gt;”指向所连接的文件。</p>
<ol start="4">
<li>特殊文件：</li>
</ol>
<p>LINUX的一些设备如磁盘、终端、打印机等都在文件系统中表示出来，则一类文件就是特殊文件，常放在&#x2F;dev目录内。例如，软驱A称为&#x2F;dev&#x2F;fd0。LINUX无C：的概念，而是用&#x2F;dev&#x2F;had来自第一硬盘。</p>
<p>对于linux新手来说，最感到迷惑的问题之一就是文件都存在哪里呢?特别是对于那些从windows转过来的新手来说，linux的目录结构看起来有些奇怪哦。所以，在这里讲一下linux下的主要目录以及它们都是用来干什么的。</p>
<p><strong>系统目录简介</strong></p>
<ol>
<li>&#x2F;</li>
</ol>
<p>这就是根目录。对你的电脑来说，有且只有一个根目录。所有的东西，我是说所有的东西都是从这里开始。举个例子：当你在终端里输入“&#x2F;home”，你其实是在告诉电脑，先从&#x2F;(根目录)开始，再进入到home目录。</p>
<ol start="2">
<li>&#x2F;root</li>
</ol>
<p>这是系统管理员(root user)的目录。对于系统来说，系统管理员就好比是上帝，它能对系统做任何事情，甚至包括删除你的文件。因此，请小心使用root帐号。</p>
<ol start="3">
<li>&#x2F;bin</li>
</ol>
<p>这里存放了标准的(或者说是缺省的)linux的工具，比如像“ls”、“vi”还有“more”等等。通常来说，这个目录已经包含在你的“path”系 统变量里面了。什么意思呢?就是：当你在终端里输入ls，系统就会去&#x2F;bin目录下面查找是不是有ls这个程序。</p>
<ol start="4">
<li>&#x2F;etc</li>
</ol>
<p>这里主要存放了系统配置方面的文件。举个例子：你安装了samba这个套件，当你想要修改samba配置文件的时候，你会发现它们(配置文件)就在&#x2F;etc&#x2F;samba目录下。</p>
<ol start="5">
<li>&#x2F;dev</li>
</ol>
<p>这里主要存放与设备(包括外设)有关的文件(unix和linux系统均把设备当成文件)。想连线打印机吗?系统就是从这个目录开始工作的。另外还有一些包括磁盘驱动、USB驱动等都放在这个目录。</p>
<ol start="6">
<li>&#x2F;home</li>
</ol>
<p>这里主要存放你的个人数据。具体每个用户的设置文件，用户的桌面文件夹，还有用户的数据都放在这里。每个用户都有自己的用户目录，位置为：&#x2F;home&#x2F;用户名。当然，root用户除外。</p>
<ol start="7">
<li>&#x2F;tmp</li>
</ol>
<p>这是临时目录。对于某些程序来说，有些文件被用了一次两次之后，就不会再被用到，像这样的文件就放在这里。有些linux系统会定期自动对这个目录进行清理，因此，千万不要把重要的数据放在这里。</p>
<ol start="8">
<li>&#x2F;usr</li>
</ol>
<p>在这个目录下，你可以找到那些不适合放在&#x2F;bin或&#x2F;etc目录下的额外的工具。比如像游戏阿，一些打印工具拉等等。&#x2F;usr目录包含了许多子目录： &#x2F;usr&#x2F;bin目录用于存放程序;&#x2F;usr&#x2F;share用于存放一些共享的数据，比如音乐文件或者图标等等;&#x2F;usr&#x2F;lib目录用于存放那些不能直接 运行的，但却是许多程序运行所必需的一些函数库文件。你的软件包管理器(应该是“新立得”吧)会自动帮你管理好&#x2F;usr目录的。</p>
<ol start="9">
<li>&#x2F;opt</li>
</ol>
<p>这里主要存放那些可选的程序。你想尝试最新的firefox测试版吗?那就装到&#x2F;opt目录下吧，这样，当你尝试完，想删掉firefox的时候，你就可 以直接删除它，而不影响系统其他任何设置。安装到&#x2F;opt目录下的程序，它所有的数据、库文件等等都是放在同个目录下面。</p>
<p>举个例子：刚才装的测试版firefox，就可以装到&#x2F;opt&#x2F;firefox_beta目录下，&#x2F;opt&#x2F;firefox_beta目录下面就包含了运 行firefox所需要的所有文件、库、数据等等。要删除firefox的时候，你只需删除&#x2F;opt&#x2F;firefox_beta目录即可，非常简单。</p>
<ol start="10">
<li>&#x2F;usr&#x2F;local</li>
</ol>
<p>这里主要存放那些手动安装的软件，即不是通过“新立得”或apt-get安装的软件。它和&#x2F;usr目录具有相类似的目录结构。让软件包管理器来管理&#x2F;usr目录，而把自定义的脚本(scripts)放到&#x2F;usr&#x2F;local目录下面，我想这应该是个不错的主意。</p>
<ol start="11">
<li>&#x2F;media</li>
</ol>
<p>有些linux的发行版使用这个目录来挂载那些usb接口的移动硬盘(包括U盘)、CD&#x2F;DVD驱动器等等。</p>
<h3 id="目录路径"><a href="#目录路径" class="headerlink" title="目录路径"></a>目录路径</h3><h4 id="路径"><a href="#路径" class="headerlink" title="路径"></a>路径</h4><p>有人可能不明白这路径是指什么，有什么用。顾名思义，路径就是你要去哪儿的路线嘛。如果你想进入某个具体的目录或者想获得某个目录的文件（目录本身也是文件）那就得用路径来找到了。</p>
<p>使用 cd 命令可以切换目录，在 Linux 里面使用 . 表示当前目录，.. 表示上一级目录（<strong>注意，我们上一节介绍过的，以</strong> <strong>.</strong> <strong>开头的文件都是隐藏文件，所以这两个目录必然也是隐藏的，你可以使用</strong> <strong>ls -a</strong> <strong>命令查看隐藏文件</strong>），- 表示上一次所在目录，～ 通常表示当前用户的 home 目录。使用 pwd 命令可以获取当前所在路径（绝对路径）。</p>
<p>进入上一级目录：</p>
<p>cd ..</p>
<p>进入你的 home 目录：</p>
<p>cd ~</p>
<p># 或者 cd &#x2F;home&#x2F;&lt;你的用户名&gt;</p>
<p>使用 pwd 获取当前路径：</p>
<p>pwd</p>
<h4 id="绝对路径"><a href="#绝对路径" class="headerlink" title="绝对路径"></a>绝对路径</h4><p>关于绝对路径，简单地说就是以根” &#x2F; “目录为起点的完整路径，以你所要到的目录为终点，表现形式如： &#x2F;usr&#x2F;local&#x2F;bin，表示根目录下的 usr 目录中的 local 目录中的 bin 目录。</p>
<h4 id="相对路径"><a href="#相对路径" class="headerlink" title="相对路径"></a>相对路径</h4><p>相对路径，也就是相对于你当前的目录的路径，相对路径是以当前目录 . 为起点，以你所要到的目录为终点，表现形式如： usr&#x2F;local&#x2F;bin （这里假设你当前目录为根目录）。你可能注意到，我们表示相对路径实际并没有加上表示当前目录的那个 . ，而是直接以目录名开头，因为这个 usr 目录为 &#x2F; 目录下的子目录，是可以省略这个 . 的（以后会讲到一个类似不能省略的情况）；如果是当前目录的上一级目录，则需要使用 .. ，比如你当前目录为 &#x2F;home&#x2F;shiyanlou 目录下，根目录就应该表示为 ..&#x2F;..&#x2F; ，表示上一级目录（ home 目录）的上一级目录（ &#x2F; 目录）。</p>
<p>下面我们以你的 home 目录为起点，分别以绝对路径和相对路径的方式进入 &#x2F;usr&#x2F;local&#x2F;bin 目录：</p>
<p># 绝对路径</p>
<p>cd &#x2F;usr&#x2F;local&#x2F;bin</p>
<p># 相对路径</p>
<p>cd ..&#x2F;..&#x2F;usr&#x2F;local&#x2F;bin</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455601019-08609119-4211-41db-a859-60a3f560603c.jpg" alt="image"></p>
<p>进入一个目录，可以使用绝对路径也可以使用相对路径，那我们应该在什么时候选择正确的方式进入某个目录呢。就是凭直觉嘛，你觉得怎样方便就使用哪一个，而不用特意只使用某一种。比如假设我当前在 &#x2F;usr&#x2F;local&#x2F;bin 目录，我想进入上一级的 local 目录你说是使用 cd .. 方便还是 cd &#x2F;usr&#x2F;local 方便？而如果要进入的是 usr 目录，那么 cd &#x2F;usr ，就比 cd ..&#x2F;.. 方便一点了。</p>
<p><strong>提示：在进行目录切换的过程中请多使用</strong> <strong>Tab</strong> <strong>键自动补全，可避免输入错误，连续按两次</strong> <strong>Tab</strong> <strong>可以显示全部候选结果。</strong></p>
<h4 id="新建空白文件"><a href="#新建空白文件" class="headerlink" title="新建空白文件"></a>新建空白文件</h4><p>使用 touch 命令创建空白文件，关于 touch 命令，其主要作用是来更改已有文件的时间戳的（比如，最近访问时间，最近修改时间），但其在不加任何参数的情况下，只指定一个文件名，则可以创建一个指定文件名的空白文件（不会覆盖已有同名文件），当然你也可以同时指定该文件的时间戳，更多关于 touch 命令的用法，会在下一讲文件搜索中涉及。</p>
<p>创建名为 test 的空白文件，因为在其它目录没有权限，所以需要先 cd ~ 切换回 shiyanlou 用户的 Home 目录：</p>
<p>cd ~</p>
<p>touch test</p>
<h4 id="新建目录"><a href="#新建目录" class="headerlink" title="新建目录"></a>新建目录</h4><p>使用 mkdir（make directories）命令可以创建一个空目录，也可同时指定创建目录的权限属性。</p>
<p>创建名为“ mydir ”的空目录：</p>
<p>mkdir mydir</p>
<p>使用 -p 参数，同时创建父目录（如果不存在该父目录），如下我们同时创建一个多级目录（这在安装软件、配置安装路径时非常有用）：</p>
<p>mkdir -p father&#x2F;son&#x2F;grandson</p>
<p>这里使用的路径是相对路径，代表在当前目录下生成，当然我们直接以绝对路径的方式表示也是可以的。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455601046-f909b85f-2774-422d-ad28-771d723e1186.jpg" alt="image"></p>
<p>还有一点需要注意的是，若当前目录已经创建了一个 test 文件，再使用 mkdir test 新建同名的文件夹，系统会报错文件已存在。这符合 Linux 一切皆文件的理念。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455601046-146e3315-f551-48f6-b3e7-892b53823ceb.jpg" alt="image"></p>
<p>若当前目录存在一个 test 文件夹，则 touch 命令，则会更改该文件夹的时间戳而不是新建文件。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455601031-78e42cc8-3716-4e6f-b791-e7153cd6de05.jpg" alt="image"></p>
<h4 id="复制文件"><a href="#复制文件" class="headerlink" title="复制文件"></a>复制文件</h4><p>使用 cp 命令（copy）复制一个文件到指定目录。</p>
<p>将之前创建的 test 文件复制到 &#x2F;home&#x2F;shiyanlou&#x2F;father&#x2F;son&#x2F;grandson 目录中：</p>
<p>cp test father&#x2F;son&#x2F;grandson</p>
<p>是不是很方便啊，如果在图形界面则需要先在源目录复制文件，再进到目的目录粘贴文件，而命令行操作步骤就一步到位了嘛。</p>
<h4 id="复制目录"><a href="#复制目录" class="headerlink" title="复制目录"></a>复制目录</h4><p>如果直接使用 cp 命令复制一个目录的话，会出现如下错误：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455601080-c622f4ba-a00d-4bf2-8ea3-aa41a4be5592.jpg" alt="image"></p>
<p>要成功复制目录需要加上 -r 或者 -R 参数，表示递归复制，就是说有点“株连九族”的意思：</p>
<p>cd &#x2F;home&#x2F;shiyanlou</p>
<p>mkdir family</p>
<p>cp -r father family</p>
<h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><h4 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h4><p>使用 rm（remove files or directories）命令删除一个文件：</p>
<p>rm test</p>
<p>有时候你会遇到想要删除一些为只读权限的文件，直接使用 rm 删除会显示一个提示，如下：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455601571-11d87e7a-06a1-48f3-a2e4-2290f3734fd6.jpg" alt="image"></p>
<p>你如果想忽略这提示，直接删除文件，可以使用 -f 参数强制删除：</p>
<p>rm -f test</p>
<h4 id="删除目录"><a href="#删除目录" class="headerlink" title="删除目录"></a>删除目录</h4><p>跟复制目录一样，要删除一个目录，也需要加上 -r 或 -R 参数：</p>
<p>rm -r family</p>
<p>遇到权限不足删除不了的目录也可以和删除文件一样加上 -f 参数：</p>
<p>rm -rf family</p>
<h4 id="移动文件"><a href="#移动文件" class="headerlink" title="移动文件"></a>移动文件</h4><p>使用 mv（move or rename files）命令移动文件（剪切）。命令格式是 mv 源目录文件 目的目录。</p>
<p>例如将文件“ file1 ”移动到 Documents 目录：</p>
<p>mkdir Documents</p>
<p>touch file1</p>
<p>mv file1 Documents</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455601822-dfcddf4b-39df-4f7d-b98f-750d5171f6b2.jpg" alt="image"></p>
<h4 id="重命名文件"><a href="#重命名文件" class="headerlink" title="重命名文件"></a>重命名文件</h4><p>mv 命令除了能移动文件外，还能给文件重命名。命令格式为 mv 旧的文件名 新的文件名。</p>
<p>例如将文件“ file1 ”重命名为“ myfile ”：</p>
<p>mv file1 myfile</p>
<h4 id="批量重命名"><a href="#批量重命名" class="headerlink" title="批量重命名"></a>批量重命名</h4><p>要实现批量重命名，mv 命令就有点力不从心了，我们可以使用一个看起来更专业的命令 rename 来实现。不过它要用 perl 正则表达式来作为参数，关于正则表达式我们要在后面才会介绍到，这里只做演示，你只要记得这个 rename 命令可以批量重命名就好了，以后再重新学习也不会有任何问题，毕竟你已经掌握了一个更常用的 mv 命令。</p>
<p>rename 命令并不是内置命令，若提示无该命令可以使用 sudo apt-get install rename 命令自行安装。</p>
<p>cd &#x2F;home&#x2F;shiyanlou&#x2F;</p>
<p># 使用通配符批量创建 5 个文件:</p>
<p>touch file{1..5}.txt</p>
<p># 批量将这 5 个后缀为 .txt 的文本文件重命名为以 .c 为后缀的文件:</p>
<p>rename ‘s&#x2F;.txt&#x2F;.c&#x2F;‘ *.txt</p>
<p># 批量将这 5 个文件，文件名和后缀改为大写:</p>
<p>rename ‘y&#x2F;a-z&#x2F;A-Z&#x2F;‘ *.c</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455601871-3651fc9a-af04-44ee-96a3-441bc3035d18.jpg" alt="image"></p>
<p>简单解释一下上面的命令，rename 是先使用第二个参数的通配符匹配所有后缀为 .txt 的文件，然后使用第一个参数提供的正则表达式将匹配的这些文件的 .txt 后缀替换为 .c，这一点在我们后面学习了 sed 命令后，相信你会更好地理解。</p>
<p>有的同学可能在输入时出现命令未闭合的状态，命令行会出现 quote&gt; 开头的提示符。这是因为上述命令中的 ‘ 未输入完成，这时按下 ctrl+c 即可退出该模式。还有就是注意 ‘ 必须为英文符号（半角），若输入的是中文符号（全角）也会报错。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455601930-4e858dca-6d50-4f00-8710-f9d3142ba147.jpg" alt="image"></p>
<h3 id="查看文件"><a href="#查看文件" class="headerlink" title="查看文件"></a>查看文件</h3><h4 id="使用-cat，tac-和-nl-命令查看文件"><a href="#使用-cat，tac-和-nl-命令查看文件" class="headerlink" title="使用 cat，tac 和 nl 命令查看文件"></a>使用 cat，tac 和 nl 命令查看文件</h4><p>前两个命令都是用来打印文件内容到标准输出（终端），其中 cat 为正序显示，tac 为倒序显示。</p>
<p>标准输入输出：当我们执行一个 shell 命令行时通常会自动打开三个标准文件，即标准输入文件（stdin），默认对应终端的键盘、标准输出文件（stdout）和标准错误输出文件（stderr），后两个文件都对应被重定向到终端的屏幕，以便我们能直接看到输出内容。进程将从标准输入文件中得到输入数据，将正常输出数据输出到标准输出文件，而将错误信息送到标准错误文件中。</p>
<p>比如我们要查看之前从 &#x2F;etc 目录下拷贝来的 passwd 文件：</p>
<p>cd &#x2F;home&#x2F;shiyanlou</p>
<p>cp &#x2F;etc&#x2F;passwd passwd</p>
<p>cat passwd</p>
<p>可以加上 -n 参数显示行号：</p>
<p>cat -n passwd</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455602464-ae500ab0-f53d-4fcb-9b7e-07132a355f25.jpg" alt="image"></p>
<p>nl 命令，添加行号并打印，这是个比 cat -n 更专业的行号打印命令。</p>
<p>这里简单列举它的常用的几个参数：</p>
<p>-b : 指定添加行号的方式，主要有两种：</p>
<p>   -b a:表示无论是否为空行，同样列出行号(“cat -n”就是这种方式)</p>
<p>   -b t:只列出非空行的编号并列出（默认为这种方式）</p>
<p>-n : 设置行号的样式，主要有三种：</p>
<p>   -n ln:在行号字段最左端显示</p>
<p>   -n rn:在行号字段最右边显示，且不加 0</p>
<p>   -n rz:在行号字段最右边显示，且加 0</p>
<p>-w : 行号字段占用的位数(默认为 6 位)</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455602553-1d7c3d4c-a025-4136-870a-72cd98700e8c.jpg" alt="image"></p>
<p>你会发现使用这几个命令，默认的终端窗口大小，一屏显示不完文本的内容，得用鼠标拖动滚动条或者滑动滚轮才能继续往下翻页，要是可以直接使用键盘操作翻页就好了，那么你就可以使用下面要介绍的命令。</p>
<h4 id="使用-more-和-less-命令分页查看文件"><a href="#使用-more-和-less-命令分页查看文件" class="headerlink" title="使用 more 和 less 命令分页查看文件"></a>使用 more 和 less 命令分页查看文件</h4><p>如果说上面的 cat 是用来快速查看一个文件的内容的，那么这个 more 和 less 就是天生用来”阅读”一个文件的内容的，比如说 man 手册内部就是使用的 less 来显示内容。其中 more 命令比较简单，只能向一个方向滚动，而 less 为基于 more 和 vi （一个强大的编辑器，我们有单独的课程来让你学习）开发，功能更强大。less 的使用基本和 more 一致，具体使用请查看 man 手册，这里只介绍 more 命令的使用。</p>
<p>使用 more 命令打开 passwd 文件：</p>
<p>more passwd</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455602661-6b2c8112-fb39-4067-b55a-cd5d69e704f8.jpg" alt="image"></p>
<p>打开后默认只显示一屏内容，终端底部显示当前阅读的进度。可以使用 Enter 键向下滚动一行，使用 Space 键向下滚动一屏，按下 h 显示帮助，q 退出。</p>
<h4 id="使用-head-和-tail-命令查看文件"><a href="#使用-head-和-tail-命令查看文件" class="headerlink" title="使用 head 和 tail 命令查看文件"></a>使用 head 和 tail 命令查看文件</h4><p>这两个命令，那些性子比较急的人应该会喜欢，因为它们一个是只查看文件的头几行（默认为 10 行，不足 10 行则显示全部）和尾几行。还是拿 passwd 文件举例，比如当我们想要查看最近新增加的用户，那么我们可以查看这个 &#x2F;etc&#x2F;passwd 文件，不过我们前面也看到了，这个文件里面一大堆乱糟糟的东西，看起来实在费神啊。因为系统新增加一个用户，会将用户的信息添加到 passwd 文件的最后，那么这时候我们就可以使用 tail 命令了：</p>
<p>tail &#x2F;etc&#x2F;passwd</p>
<p>甚至更直接的只看一行， 加上 -n 参数，后面紧跟行数：</p>
<p>tail -n 1 &#x2F;etc&#x2F;passwd</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455602695-38e51373-8f46-4700-9ebd-4c54b7a321f2.jpg" alt="image"></p>
<p>关于 tail 命令，不得不提的还有它一个很牛的参数 -f，这个参数可以实现不停地读取某个文件的内容并显示。这可以让我们动态查看日志，达到实时监视的目的。不过我不会在这门基础课程中介绍它的更多细节，感兴趣的用户可以自己去了解。</p>
<h4 id="查看文件类型"><a href="#查看文件类型" class="headerlink" title="查看文件类型"></a>查看文件类型</h4><p>我们可以使用 file 命令查看文件的类型：</p>
<p>file &#x2F;bin&#x2F;ls</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455602906-f3182f67-97cd-423f-897b-398d6218a325.jpg" alt="image"></p>
<p>说明这是一个可执行文件，运行在 64 位平台，并使用了动态链接文件（共享库）。</p>
<p>与 Windows 不同的是，如果你新建了一个 shiyanlou.txt 文件，Windows 会自动把它识别为文本文件，而 file 命令会识别为一个空文件。这个前面我提到过，在 Linux 中文件的类型不是根据文件后缀来判断的。当你在文件里输入内容后才会显示文件类型。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455603731-68b16f90-65aa-4485-b726-7431df25e379.jpg" alt="image"></p>
<h4 id="编辑文件"><a href="#编辑文件" class="headerlink" title="编辑文件"></a>编辑文件</h4><p>在 Linux 下面编辑文件通常我们会直接使用专门的命令行编辑器比如（emacs，vim，nano），由于涉及 Linux 上的编辑器的内容比较多，且非常重要，故我们有一门单独的基础课专门介绍这中一个编辑器 vim 。</p>
<p>强烈建议正在学习这门 Linux 基础课的你先在这里暂停一下，去学习 <a target="_blank" rel="noopener" href="https://www.lanqiao.cn/courses/2">vim 编辑器</a>的使用（至少掌握基本的操作），然后再继续本课程后面的内容，因为后面的内容会假设你已经学会了 vim 编辑器的使用。</p>
<p>如果你想更加快速地入门，可以直接使用 Linux 内部的 vim 学习教程，输入如下命令即可开始：</p>
<p>vimtutor</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455603735-ee4424af-78fe-4764-9268-8468d18cb1df.jpg" alt="image"></p>
<p>#### </p>
<h2 id="挑战：寻找文件"><a href="#挑战：寻找文件" class="headerlink" title="挑战：寻找文件"></a>挑战：寻找文件</h2><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>有一个非常重要的文件（sources.list）但是你忘了它在哪了，你依稀记得它在 &#x2F;etc&#x2F; 目录下，现在要你把这个文件找出来，然后设置成自己（shiyanlou 用户）可以访问，但是其他用户并不能访问。</p>
<h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><ol>
<li>找到 sources.list 文件</li>
<li>把文件所有者改为自己</li>
<li>把权限修改为仅仅只有自己可读可写</li>
</ol>
<h4 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h4><ul>
<li>find</li>
<li>chmod</li>
<li>chown</li>
<li>sudo</li>
</ul>
<p>参考答案</p>
<p>sudo find &#x2F;etc -name sources.list<br>sudo chown shiyanlou &#x2F;etc&#x2F;apt&#x2F;sources.list<br>sudo chmod 600 &#x2F;etc&#x2F;apt&#x2F;sources.list</p>
<h3 id="Linux-用户管理"><a href="#Linux-用户管理" class="headerlink" title="Linux 用户管理"></a>Linux 用户管理</h3><p>Linux 是一个可以实现多用户登录的操作系统，比如“李雷”和“韩梅梅”都可以同时登录同一台主机，他们共享一些主机的资源，但他们也分别有自己的用户空间，用于存放各自的文件。但实际上他们的文件都是放在同一个物理磁盘上的甚至同一个逻辑分区或者目录里，但是由于 Linux 的 <strong>用户管理</strong> 和 <strong>权限机制</strong>，不同用户不可以轻易地查看、修改彼此的文件。</p>
<p>下面我们就来学习一下 Linux 下的账户管理的基础知识。</p>
<h4 id="查看用户"><a href="#查看用户" class="headerlink" title="查看用户"></a>查看用户</h4><p>请打开终端，输入命令：</p>
<p>who am i</p>
<p># 或者</p>
<p>who mom likes</p>
<p>输出的第一列表示打开当前伪终端的用户的用户名（要查看当前登录用户的用户名，去掉空格直接使用 whoami 即可），第二列的 pts&#x2F;0 中 pts 表示伪终端，所谓伪是相对于 &#x2F;dev&#x2F;tty 设备而言的</p>
<p>还有一点需要注意的是，在某些环境中 who am i 和 who mom likes 命令不会输出任何内容，这是因为当前使用的 SHELL 不是登录时的 SHELL，没有用户与 who 的 stdin 相关联，因此不会输出任何内容。例如我在本地的 Ubuntu 系统上输入这个命令就不会有提示。</p>
<p>who 命令其它常用参数</p>
<p><strong>参数****说明</strong></p>
<h4 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h4><p>在 Linux 系统里， root 账户拥有整个系统至高无上的权限，比如新建和添加用户。</p>
<p>root 权限，系统权限的一种，与 SYSTEM 权限可以理解成一个概念，但高于 Administrator 权限，root 是 Linux 和 UNIX 系统中的超级管理员用户帐户，该帐户拥有整个系统至高无上的权力，所有对象他都可以操作，所以很多黑客在入侵系统的时候，都要把权限提升到 root 权限，这个操作等同于在 Windows 下就是将新建的非法帐户添加到 Administrators 用户组。更比如安卓操作系统中（基于 Linux 内核）获得 root 权限之后就意味着已经获得了手机的最高权限，这时候你可以对手机中的任何文件（包括系统文件）执行所有增、删、改、查的操作。</p>
<p>大部分 Linux 系统在安装时都会建议用户新建一个用户而不是直接使用 root 用户进行登录，当然也有直接使用 root 登录的例如 Kali（基于 Debian 的 Linux 发行版，集成大量工具软件，主要用于数字取证的操作系统）。一般我们登录系统时都是以普通账户的身份登录的，要创建用户需要 root 权限，这里就要用到 sudo 这个命令了。不过使用这个命令有两个大前提，一是你要知道当前登录用户的密码，二是当前用户必须在 sudo 用户组。</p>
<p><strong>需要注意 Linux 环境下输入密码是不会显示的。</strong></p>
<p>su <user> 可以切换到用户 user，执行时需要输入目标用户的密码，sudo <cmd> 可以以特权级别运行 cmd 命令，需要当前用户属于 sudo 组，且需要输入当前用户的密码。su - <user> 命令也是切换用户，但是同时用户的环境变量和工作目录也会跟着改变成目标用户所对应的。</p>
<p>现在我们新建一个叫 lilei 的用户：</p>
<p>sudo adduser lilei</p>
<p>环境目前设置为 root 用户执行 sudo 不需要输入密码，通常此处需要按照提示输入 root密码（<strong>Linux 下密码输入是不显示任何内容的</strong>），root用户密码可以通过 sudo passwd root 命令进行设置。然后是给 lilei 用户设置密码，后面的选项的一些内容你可以选择直接回车使用默认值。</p>
<p>这个命令不但可以添加用户到系统，同时也会默认为新用户在 &#x2F;home 目录下创建一个工作目录：</p>
<p>ls &#x2F;home</p>
<p>su root  &#x2F;&#x2F;切换到root</p>
<p>sudo passwd lilei &#x2F;&#x2F; 设置用户密码</p>
<p>现在你已经创建好一个用户，并且你可以使用你创建的用户登录了，使用如下命令切换登录用户：</p>
<p>su -l lilei</p>
<p>输入刚刚设置的 lilei 的密码，然后输入如下命令并查看输出：</p>
<p>who am i</p>
<p>whoami &#x2F;&#x2F; 当前用户</p>
<p>pwd  </p>
<p>你发现了区别了吗？这就是上一小节我们讲到的 who am i 和 whoami 命令的区别。</p>
<p>退出当前用户跟退出终端一样，可以使用 exit 命令或者使用快捷键 Ctrl+D。</p>
<h3 id="文件权限"><a href="#文件权限" class="headerlink" title="文件权限"></a>文件权限</h3><p>文件权限就是文件的访问控制权限，即哪些用户和组群可以访问文件以及可以执行什么样的操作。</p>
<p>Unix&#x2F;Linux 系统是一个典型的多用户系统，不同的用户处于不同的地位，对文件和目录有不同的访问权限。为了保护系统的安全性，Unix&#x2F;Linux 系统除了对用户权限作了严格的界定外，还在用户身份认证、访问控制、传输安全、文件读写权限等方面作了周密的控制。</p>
<p>在 Unix&#x2F;Linux 中的每一个文件或目录都包含有访问权限，这些访问权限决定了谁能访问和如何访问这些文件和目录。</p>
<h4 id="查看文件权限"><a href="#查看文件权限" class="headerlink" title="查看文件权限"></a>查看文件权限</h4><p>我们之前已经很多次用到 ls 命令了，如你所见，我们用它来列出并显示当前目录下的文件，当然这是在不带任何参数的情况下，它能做的当然不止这么多，现在我们就要用它来查看文件权限。</p>
<p>使用较长格式列出文件：</p>
<p>ls -l</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455696548-07cd4ccd-a27d-434c-b05c-42e1e18307c9.jpg" alt="image">)</p>
<p>你可能除了知道最后面那一项是文件名之外，其它项就不太清楚了，那么到底是什么意思呢：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455696666-212504cd-e477-48a8-aa23-bc8d05eb1f7f.jpg" alt="image"></p>
<p>可能你还是不太明白，比如第一项文件类型和权限那一堆东西具体指什么，链接又是什么，何为最后修改时间，下面一一道来：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455696582-5d4d7b32-2462-478d-b1f7-9a15f78e0fa9.jpg" alt="image"></p>
<ul>
<li>文件类型</li>
</ul>
<p>关于文件类型，这里有一点你必需时刻牢记 <strong>Linux 里面一切皆文件</strong>，正因为这一点才有了设备文件（ &#x2F;dev 目录下有各种设备文件，大都跟具体的硬件设备相关）这一说。 socket：网络套接字，具体是什么，感兴趣的用户可以去学习实验楼的后续相关课程。pipe 管道，这个东西很重要，我们以后将会讨论到，这里你先知道有它的存在即可。软链接文件：链接文件是分为两种的，另一种当然是“硬链接”（硬链接不常用，具体内容不作为本课程讨论重点，而软链接等同于 Windows 上的快捷方式，你记住这一点就够了）。</p>
<ul>
<li>文件权限</li>
</ul>
<p>读权限，表示你可以使用 cat <file name> 之类的命令来读取某个文件的内容；写权限，表示你可以编辑和修改某个文件的内容；</p>
<p>执行权限，通常指可以运行的二进制程序文件或者脚本文件，如同 Windows 上的 exe 后缀的文件，不过 Linux 上不是通过文件后缀名来区分文件的类型。你需要注意的一点是，<strong>一个目录同时具有读权限和执行权限才可以打开并查看内部文件，而一个目录要有写权限才允许在其中创建其它文件</strong>，这是因为目录文件实际保存着该目录里面的文件的列表等信息。</p>
<p>所有者权限，这一点相信你应该明白了，至于所属用户组权限，是指你所在的用户组中的所有其它用户对于该文件的权限，比如，你有一个 iPad，那么这个用户组权限就决定了你的兄弟姐妹有没有权限使用它破坏它和占有它。</p>
<ul>
<li>链接数</li>
</ul>
<p>链接到该文件所在的 inode 结点的文件名数目（关于这个概念涉及到 Linux 文件系统的相关概念知识，不在本课程的讨论范围，感兴趣的用户可以查看 <a target="_blank" rel="noopener" href="https://www.ibm.com/developerworks/cn/linux/l-cn-hardandsymb-links/index.html#major2">硬链接和软链接的联系与区别</a>）。</p>
<ul>
<li>文件大小</li>
</ul>
<p>以 inode 结点大小为单位来表示的文件大小，你可以给 ls 加上 -lh 参数来更直观的查看文件的大小。</p>
<p>明白了文件权限的一些概念，我们顺带补充一下关于 ls 命令的一些其它常用的用法：</p>
<ul>
<li>显示除了 .（当前目录）和 ..（上一级目录）之外的所有文件，包括隐藏文件（Linux 下以 . 开头的文件为隐藏文件）。</li>
</ul>
<p>ls -a</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455696573-ef65c612-fd8f-499c-bbf2-3c53cb9700dc.jpg" alt="image">)</p>
<p>当然，你可以同时使用 -a 和 -l 参数：</p>
<p>ls -al</p>
<p>查看某一个目录的完整属性，而不是显示目录里面的文件属性：</p>
<p>ls -dl &lt;目录名&gt;</p>
<ul>
<li>显示所有文件大小，并以普通人类能看懂的方式呈现：</li>
</ul>
<p>ls -asSh</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455696560-279e86d9-e5c6-4d46-8b8f-dd58e24a2ee3.jpg" alt="image"></p>
<p>其中小 s 为显示文件大小，大 S 为按文件大小排序，若需要知道如何按其它方式排序，可以使用 man ls 命令查询。</p>
<h4 id="修改文件权限"><a href="#修改文件权限" class="headerlink" title="修改文件权限"></a>修改文件权限</h4><p>如果你有一个自己的文件不想被其他用户读、写、执行，那么就需要对文件的权限做修改。文件的权限有两种表示方式：</p>
<ul>
<li>方式一：二进制数字表示</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455697076-9f3b57a7-01aa-46ff-9146-2dd142050fc1.jpg" alt="image"></p>
<p>每个文件有三组固定的权限，分别对应拥有者，所属用户组，其他用户，<strong>记住这个顺序是固定的</strong>。文件的读写执行对应字母 rwx，以二进制表示就是 111，用十进制表示就是 7，对进制转换不熟悉的同学可以看看 <a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%BF%9B%E5%88%B6%E8%BD%AC%E6%8D%A2/3117222">进制转换</a>。例如我们刚刚新建的文件 iphone11 的权限是 rw-rw-rw-，换成对应的十进制表示就是 666，这就表示这个文件的拥有者，所属用户组和其他用户具有读写权限，不具有执行权限。</p>
<p>如果我要将文件 iphone11 的权限改为只有我自己可以用那么就可以用这个方法更改它的权限。</p>
<p>为了演示，我先在文件里加点内容：</p>
<p>echo “echo &quot;hello shiyanlou&quot;“ &gt; iphone11</p>
<p>然后修改权限：</p>
<p>chmod 600 iphone11</p>
<p>ls -alh iphone11</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455697096-86d1fb48-6793-4970-963d-8feb6ffa34dd.jpg" alt="image"></p>
<p>切换到 lilei 用户，尝试写入和读取操作，可以看到 lilei 用户已经不能读写这个 iphone11 文件了：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455697114-60a9195b-1db0-425d-8a99-bbde282f8d78.jpg" alt="image"></p>
<ul>
<li>方式二：加减赋值操作</li>
</ul>
<p>要完成上述实验相同的效果，你可以：</p>
<p>chmod go-rw iphone11</p>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/450565/1618455697220-69e5b6aa-899e-4300-8ed8-c1c23b4be3e3.jpg" alt="image"></p>
<p>g、o 还有 u 分别表示 group（用户组）、others（其他用户） 和 user（用户），+ 和 - 分别表示增加和去掉相应的权限。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/LI_001_introduce-linux/" data-id="clkqazufc001ndlbi3ts549e8" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-ML_001_introduce-machine-learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/ML_001_introduce-machine-learning/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/53242081"><em><strong>*《机器学习》 学习笔记（*</strong></em><em><strong>*1*</strong></em><em><strong>*）–*</strong></em><em><strong>*绪论*</strong></em></a></p>
<p>机器学习是目前信息技术中最激动人心的方向之一，其应用已经深入到生活的各个层面且与普通人的日常生活密切相关。本文为清华大学最新出版的《机器学习》教材的Learning Notes，书作者是南京大学周志华教授，多个大陆首位彰显其学术奢华。本篇主要介绍了该教材前两个章节的知识点以及自己一点浅陋的理解。</p>
<p><em><strong>*1 绪论*</strong></em></p>
<p>傍晚小街路面上沁出微雨后的湿润，和熙的细风吹来，抬头看看天边的晚霞，嗯，明天又是一个好天气。走到水果摊旁，挑了个根蒂蜷缩、敲起来声音浊响的青绿西瓜，一边满心期待着皮薄肉厚瓢甜的爽落感，一边愉快地想着，这学期狠下了工夫，基础概念弄得清清楚楚，算法作业也是信手拈来，这门课成绩一定差不了！哈哈，也希望自己这学期的machine learning课程取得一个好成绩！</p>
<p><em><strong>*1.1 机器学习的定义*</strong></em></p>
<p>正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。</p>
<p>另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：</p>
<p>· P：计算机程序在某任务类T上的性能。</p>
<p>· T：计算机程序希望实现的任务类。</p>
<p>· E：表示经验，即历史的数据集。</p>
<p>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。</p>
<p><em><strong>*1.2 机器学习的一些基本术语*</strong></em></p>
<p>假设我们收集了一批西瓜的数据，例如：（色泽&#x3D;青绿;根蒂&#x3D;蜷缩;敲声&#x3D;浊响)， (色泽&#x3D;乌黑;根蒂&#x3D;稍蜷;敲声&#x3D;沉闷)， (色泽&#x3D;浅自;根蒂&#x3D;硬挺;敲声&#x3D;清脆)……每对括号内是一个西瓜的记录，定义：</p>
<p>· 所有记录的集合为：数据集。</p>
<p>· 每一条记录为：一个实例（instance）或样本（sample）。</p>
<p>· 例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。</p>
<p>· 对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。</p>
<p>· 一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。</p>
<p>在计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：</p>
<p>· 所有训练样本的集合为：训练集（trainning set），[特殊]。</p>
<p>· 所有测试样本的集合为：测试集（test set），[一般]。</p>
<p>· 机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。</p>
<p>在西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：</p>
<p>· 预测值为离散值的问题为：分类（classification）。</p>
<p>· 预测值为连续值的问题为：回归（regression）。</p>
<p>在我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：</p>
<p>· 训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。</p>
<p>· 训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。</p>
<p><em><strong>*2 模型的评估与选择*</strong></em></p>
<p><em><strong>*2.1 误差与过拟合*</strong></em></p>
<p>我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：</p>
<p>· 在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。</p>
<p>· 在测试集上的误差称为测试误差（test error）。</p>
<p>· 学习器在所有新样本上的误差称为泛化误差（generalization error）。</p>
<p>显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：</p>
<p>· 学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。</p>
<p>· 学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。</p>
<p>可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。</p>
<p> <img src="https://raw.githubusercontent.com/ebxeax/images/main/wps1.jpg" alt="wps1"></p>
<p><em><strong>*2.2 评估方法*</strong></em></p>
<p>在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。</p>
<p>因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：</p>
<p>假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。</p>
<p><em><strong>*2.3 训练集与测试集的划分方法*</strong></em></p>
<p>如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：</p>
<p><em><strong>*2.3.1 留出法*</strong></em></p>
<p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D&#x3D;S∪T且S∩T&#x3D;∅，常见的划分为：大约2&#x2F;3-4&#x2F;5的样本用作训练，剩下的用作测试。需要注意的是：训练&#x2F;测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p>
<p><em><strong>*2.3.2 交叉验证法*</strong></em></p>
<p>将数据集D划分为k个大小相同的互斥子集，满足D&#x3D;D1∪D2∪…∪Dk，Di∩Dj&#x3D;∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集&#x2F;测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps289.jpg" alt="img"> </p>
<p>与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练&#x2F;测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。</p>
<p><em><strong>*2.3.3 自助法*</strong></em></p>
<p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。</p>
<p>自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps290.jpg" alt="img"> </p>
<p>这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集&#x2F;测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p>
<p><em><strong>*2.4 调参*</strong></em></p>
<p>大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的”参数调节”或简称”调参” (parameter tuning)。</p>
<p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练&#x2F;测试集就有5<em>5</em>5&#x3D; 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。</p>
<p>最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/53242081"><em><strong>*《机器学习》 学习笔记（2）–性能度量*</strong></em></a></p>
<p>本篇主要是对第二章剩余知识的理解，包括：性能度量、比较检验和偏差与方差。在上一篇中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练&#x2F;测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。</p>
<p><em><strong>*2.5 性能度量*</strong></em></p>
<p>性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。本节除2.5.1外，其它主要介绍分类模型的性能度量。</p>
<p><em><strong>*2.5.1 最常见的性能度量*</strong></em></p>
<p>在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数，想必大家都十分熟悉。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps291.jpg" alt="img"> </p>
<p>在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度&#x3D;1。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps292.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps293.jpg" alt="img"></p>
<p><em><strong>*2.5.2 查准率&#x2F;查全率&#x2F;F1*</strong></em></p>
<p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率: precision），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率:recall）。因此，使用查准&#x2F;查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准&#x2F;查全率定义如下：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps294.jpg" alt="img"> </p>
<p>初次接触时，FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negative，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps295.jpg" alt="img"> </p>
<p>正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p>
<p>“P-R曲线”正是描述查准&#x2F;查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps296.jpg" alt="img"> </p>
<p>P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P&#x3D;R时的取值，平衡点的取值越高，性能更优。</p>
<p>P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps297.jpg" alt="img">   <img src="https://raw.githubusercontent.com/ebxeax/images/main/wps298.jpg" alt="img"><br>特别地，当β&#x3D;1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps299.jpg" alt="img">      <img src="https://raw.githubusercontent.com/ebxeax/images/main/wps300.jpg" alt="img"><br>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，再算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps301.jpg" alt="img"></p>
<p><em><strong>*2.5.3 ROC与AUC*</strong></em></p>
<p>如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC（Receiver Operating Characteristic）曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps302.jpg" alt="img"> </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps303.jpg" alt="img"> </p>
<p>简单分析图像，可以得知：当FN&#x3D;0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。限于篇幅，这里不再论述。</p>
<p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps304.jpg" alt="img"> </p>
<p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Under ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps305.jpg" alt="img"> </p>
<p><em><strong>*2.5.4 代价敏感错误率与代价曲线*</strong></em></p>
<p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病–&gt;有疾病只是增多了检查，但有疾病–&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps306.jpg" alt="img"> </p>
<p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps307.jpg" alt="img"> </p>
<p>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps308.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps309.jpg" alt="img"></p>
<p>代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps310.jpg" alt="img"> </p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/53455753"><em><strong>*《机器学习》 学习笔记（3）–假设检验、方差与偏差*</strong></em></a></p>
<p>在上两篇中，我们介绍了多种常见的评估方法和性能度量标准，这样我们就可以根据数据集以及模型任务的特征，选择出最合适的评估和性能度量方法来计算出学习器的“测试误差“。但由于“测试误差”受到很多因素的影响，例如：算法随机性(例如常见的K-Means)或测试集本身的选择，使得同一模型每次得到的结果不尽相同，同时测试误差是作为泛化误差的近似，并不能代表学习器真实的泛化性能，那如何对单个或多个学习器在不同或相同测试集上的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。本篇延续上一篇的内容，主要讨论了比较检验、方差与偏差。</p>
<p><em><strong>*2.6 比较检验*</strong></em></p>
<p>在比较学习器泛化性能的过程中，统计假设检验（hypothesis test）为学习器性能比较提供了重要依据，即若A在某测试集上的性能优于B，那A学习器比B好的把握有多大。 为方便论述，本篇中都是以“错误率”作为性能度量的标准。</p>
<p><em><strong>*2.6.1 假设检验*</strong></em></p>
<p>“假设”指的是对样本总体的分布或已知分布中某个参数值的一种猜想，例如：假设总体服从泊松分布，或假设正态总体的期望u&#x3D;u0。回到本篇中，我们可以通过测试获得测试错误率，但直观上测试错误率和泛化错误率相差不会太远，因此可以通过测试错误率来推测泛化错误率的分布，这就是一种假设检验。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps311.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps312.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps313.jpg" alt="img"></p>
<p><em><strong>*2.6.2 交叉验证t检验*</strong></em></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps314.jpg" alt="img"> </p>
<p><em><strong>*2.6.3 McNemar检验*</strong></em></p>
<p>MaNemar主要用于二分类问题，与成对t检验一样也是用于比较两个学习器的性能大小。主要思想是：若两学习器的性能相同，则A预测正确B预测错误数应等于B预测错误A预测正确数，即e01&#x3D;e10，且|e01-e10|服从N（1，e01+e10）分布。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps315.jpg" alt="img"> </p>
<p>因此，如下所示的变量服从自由度为1的卡方分布，即服从标准正态分布N（0,1）的随机变量的平方和，下式只有一个变量，故自由度为1，检验的方法同上：做出假设–&gt;求出满足显著度的临界点–&gt;给出拒绝域–&gt;验证假设。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps316.jpg" alt="img"> </p>
<p><em><strong>*2.6.4 Friedman检验与Nemenyi后续检验*</strong></em></p>
<p>上述的三种检验都只能在一组数据集上，F检验则可以在多组数据集进行多个学习器性能的比较，基本思想是在同一组数据集上，根据测试结果（例：测试错误率）对学习器的性能进行排序，赋予序值1,2,3…，相同则平分序值，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps317.jpg" alt="img"> </p>
<p>若学习器的性能相同，则它们的平均序值应该相同，且第i个算法的平均序值ri服从正态分布N（（k+1）&#x2F;2，（k+1）(k-1)&#x2F;12），则有：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps318.jpg" alt="img"> </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps319.jpg" alt="img"> </p>
<p>服从自由度为k-1和(k-1)(N-1)的F分布。下面是F检验常用的临界值：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps320.jpg" alt="img"> </p>
<p>若“H0：所有算法的性能相同”这个假设被拒绝，则需要进行后续检验，来得到具体的算法之间的差异。常用的就是Nemenyi后续检验。Nemenyi检验计算出平均序值差别的临界值域，下表是常用的qa值，若两个算法的平均序值差超出了临界值域CD，则相应的置信度1-α拒绝“两个算法性能相同”的假设。<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps321.jpg" alt="img"><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps322.jpg" alt="img"></p>
<p><em><strong>*2.7 偏差与方差*</strong></em></p>
<p>偏差-方差分解是解释学习器泛化性能的重要工具。在学习算法中，偏差指的是预测的期望值与真实值的偏差，方差则是每一次预测值与预测值得期望之间的差均方。实际上，偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。通过对泛化误差的进行分解，可以得到：</p>
<p><strong>·</strong> <em><strong>*期望泛化误差&#x3D;方差+偏差*</strong></em></p>
<p><strong>·</strong> <em><strong>*偏差刻画学习器的拟合能力*</strong></em></p>
<p><strong>·</strong> <em><strong>*方差体现学习器的稳定性*</strong></em></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps323.jpg" alt="img">易知：方差和偏差具有矛盾性，这就是常说的偏差-方差窘境（bias-variance dilamma），随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练也不要贪杯，适度辄止。</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/53573115"><em><strong>*《机器学习》 学习笔记（4）–线性模型*</strong></em></a></p>
<p>前一部分主要是对机器学习预备知识的概括，包括机器学习的定义&#x2F;术语、学习器性能的评估&#x2F;度量以及比较，本篇之后将主要对具体的学习算法进行理解总结，本篇则主要是第3章的内容–线性模型。</p>
<h1 id="3、线性模型"><a href="#3、线性模型" class="headerlink" title="*3、线性模型*"></a><em><strong>*3、线性模型*</strong></em></h1><p>谈及线性模型，其实我们很早就已经与它打过交道：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y&#x3D;ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps324.jpg" alt="img"> </p>
<h2 id="3-1-线性回归"><a href="#3-1-线性回归" class="headerlink" title="*3.1 线性回归*"></a><em><strong>*3.1 线性回归*</strong></em></h2><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000–&gt;13亿…2016–&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）–&gt;15k。</p>
<p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p>
<p>· 若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p>
<p>· 若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p>
<p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y&#x3D;wx+b的两个参数w和b，计算过程如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps325.jpg" alt="img"> </p>
<p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y&#x3D;wx+b需要写成：<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps326.jpg" alt="img"></p>
<p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps327.jpg" alt="img"> </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps328.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps329.jpg" alt="img"></p>
<p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps330.jpg" alt="img">另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps331.jpg" alt="img"> </p>
<p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps332.jpg" alt="img"> </p>
<h2 id="3-2-线性几率回归"><a href="#3-2-线性几率回归" class="headerlink" title="*3.2 线性几率回归*"></a><em><strong>*3.2 线性几率回归*</strong></em></h2><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps333.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps334.jpg" alt="img"></p>
<p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps335.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps336.jpg" alt="img"></p>
<h2 id="3-3-线性判别分析"><a href="#3-3-线性判别分析" class="headerlink" title="*3.3 线性判别分析*"></a><em><strong>*3.3 线性判别分析*</strong></em></h2><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps337.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps338.jpg" alt="img"></p>
<p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p>
<p>· 类内散度矩阵（within-class scatter matrix）<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps339.jpg" alt="img"></p>
<p>· 类间散度矩阵(between-class scaltter matrix)<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps340.jpg" alt="img"></p>
<p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps341.jpg" alt="img"> </p>
<p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps342.jpg" alt="img"> </p>
<p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。</p>
<h2 id="3-4-多分类学习"><a href="#3-4-多分类学习" class="headerlink" title="*3.4 多分类学习*"></a><em><strong>*3.4 多分类学习*</strong></em></h2><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p>
<p>· OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类&#x2F;一个反类），从而产生N（N-1）&#x2F;2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p>
<p>· OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p>
<p>· MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明&#x2F;欧式距离选择距离最小的类别作为最终分类结果。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps343.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps344.jpg" alt="img"></p>
<h2 id="3-5-类别不平衡问题"><a href="#3-5-类别不平衡问题" class="headerlink" title="*3.5 类别不平衡问题*"></a><em><strong>*3.5 类别不平衡问题*</strong></em></h2><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p>
<p>\1. 在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</p>
<p>\2. 在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</p>
<p>\3. 直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps345.jpg" alt="img"></p>
<h1 id="《机器学习》-学习笔记（5）–决策树"><a href="#《机器学习》-学习笔记（5）–决策树" class="headerlink" title="《机器学习》 学习笔记（5）–决策树"></a><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/53606485">《机器学习》 学习笔记（5）–决策树</a></h1><p>上篇主要介绍和讨论了线性模型。首先从最简单的最小二乘法开始，讨论输入属性有一个和多个的情形，接着通过广义线性模型延伸开来，将预测连续值的回归问题转化为分类问题，从而引入了对数几率回归，最后线性判别分析LDA将样本点进行投影，多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。本篇将讨论另一种被广泛使用的分类算法–决策树（Decision Tree）。</p>
<h1 id="4、决策树"><a href="#4、决策树" class="headerlink" title="*4、决策树*"></a><em><strong>*4、决策树*</strong></em></h1><h2 id="4-1-决策树基本概念"><a href="#4-1-决策树基本概念" class="headerlink" title="*4.1 决策树基本概念*"></a><em><strong>*4.1 决策树基本概念*</strong></em></h2><p>顾名思义，决策树是基于树结构来进行决策的，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：</p>
<p> 女儿：多大年纪了？</p>
<p> 母亲：26。</p>
<p> 女儿：长的帅不帅？</p>
<p> 母亲：挺帅的。</p>
<p> 女儿：收入高不？</p>
<p> 母亲：不算很高，中等情况。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps346.jpg" alt="img"> 女儿：是公务员不？</p>
<p> 母亲：是，在税务局上班呢。</p>
<p> 女儿：那好，我去见见。</p>
<p>这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。</p>
<p>在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：</p>
<p>* 每个非叶节点表示一个特征属性测试。</p>
<p>* 每个分支代表这个特征属性在某个值域上的输出。</p>
<p>* 每个叶子节点存放一个类别。</p>
<p>* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。</p>
<h2 id="4-2-决策树的构造"><a href="#4-2-决策树的构造" class="headerlink" title="*4.2 决策树的构造*"></a><em><strong>*4.2 决策树的构造*</strong></em></h2><p>决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps347.jpg" alt="img"> </p>
<p>可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。</p>
<h3 id="4-2-1-ID3算法"><a href="#4-2-1-ID3算法" class="headerlink" title="*4.2.1 ID3算法*"></a><em><strong>*4.2.1 ID3算法*</strong></em></h3><p>ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的<strong>信息熵</strong>定义为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps348.jpg" alt="img"> </p>
<p>假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“<strong>信息增益</strong>”（information gain）。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps349.jpg" alt="img"> </p>
<p>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。</p>
<h3 id="4-2-2-C4-5算法"><a href="#4-2-2-C4-5算法" class="headerlink" title="*4.2.2 C4.5算法*"></a><em><strong>*4.2.2 C4.5算法*</strong></em></h3><p>ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，<strong>增益率</strong>定义为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps350.jpg" alt="img"> </p>
<h3 id="4-2-3-CART算法"><a href="#4-2-3-CART算法" class="headerlink" title="*4.2.3 CART算法*"></a><em><strong>*4.2.3 CART算法*</strong></em></h3><p>CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，基尼指数定义如下：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps351.jpg" alt="img"> </p>
<p>进而，使用属性α划分后的基尼指数为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps352.jpg" alt="img"> </p>
<h2 id="4-3-剪枝处理"><a href="#4-3-剪枝处理" class="headerlink" title="*4.3 剪枝处理*"></a><em><strong>*4.3 剪枝处理*</strong></em></h2><p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：</p>
<p>* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。</p>
<p>* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。</p>
<p>评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps353.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps354.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps355.jpg" alt="img"></p>
<p>上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。</p>
<h2 id="4-4-连续值与缺失值处理"><a href="#4-4-连续值与缺失值处理" class="headerlink" title="*4.4 连续值与缺失值处理*"></a><em><strong>*4.4 连续值与缺失值处理*</strong></em></h2><p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。</p>
<p>* 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。</p>
<p>* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。</p>
<p>* 选择最大信息增益的划分点作为最优划分点。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps356.jpg" alt="img"> </p>
<p>现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps357.jpg" alt="img"> </p>
<p>对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps358.jpg" alt="img"> </p>
<p>对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps359.jpg" alt="img"> </p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/53767428"><em><strong>*《机器学习》 学习笔记（6）–神经网络*</strong></em></a></p>
<p>上篇主要讨论了决策树算法。首先从决策树的基本概念出发，引出决策树基于树形结构进行决策，进一步介绍了构造决策树的递归流程以及其递归终止条件，在递归的过程中，划分属性的选择起到了关键作用，因此紧接着讨论了三种评估属性划分效果的经典算法，介绍了剪枝策略来解决原生决策树容易产生的过拟合问题，最后简述了属性连续值&#x2F;缺失值的处理方法。本篇将讨论现阶段十分热门的另一个经典监督学习算法–神经网络（neural network）。</p>
<h1 id="5、神经网络"><a href="#5、神经网络" class="headerlink" title="*5、神经网络*"></a><em><strong>*5、神经网络*</strong></em></h1><p>在机器学习中，神经网络一般指的是“神经网络学习”，是机器学习与神经网络两个学科的交叉部分。所谓神经网络，目前用得最广泛的一个定义是“神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应”。</p>
<h2 id="5-1-神经元模型"><a href="#5-1-神经元模型" class="headerlink" title="*5.1 神经元模型*"></a><em><strong>*5.1 神经元模型*</strong></em></h2><p>神经网络中最基本的单元是神经元模型（neuron）。在生物神经网络的原始机制中，每个神经元通常都有多个树突（dendrite），一个轴突（axon）和一个细胞体（cell body），树突短而多分支，轴突长而只有一个；在功能上，树突用于传入其它神经元传递的神经冲动，而轴突用于将神经冲动传出到其它神经元，当树突或细胞体传入的神经冲动使得神经元兴奋时，该神经元就会通过轴突向其它神经元传递兴奋。神经元的生物学结构如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps360.jpg" alt="img"> </p>
<p>一直沿用至今的“M-P神经元模型”正是对这一结构进行了抽象，也称“阈值逻辑单元”，其中树突对应于输入部分，每个神经元收到n个其他神经元传递过来的输入信号，这些信号通过带权重的连接传递给细胞体，这些权重又称为连接权（connection weight）。细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和，或者说累积电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数（activation function）的处理，产生输出从轴突传送给其它神经元。M-P神经元模型如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps361.jpg" alt="img"> </p>
<p>与线性分类十分相似，神经元模型最理想的激活函数也是阶跃函数，即将神经元输入值与阈值的差值映射为输出值1或0，若差值大于零输出1，对应兴奋；若差值小于零则输出0，对应抑制。但阶跃函数不连续，不光滑，故在M-P神经元模型中，也采用Sigmoid函数来近似， Sigmoid函数将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为挤压函数（squashing function）。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps362.jpg" alt="img"> </p>
<p>将多个神经元按一定的层次结构连接起来，就得到了神经网络。它是一种包含多个参数的模型，比方说10个神经元两两连接，则有100个参数需要学习（每个神经元有9个连接权以及1个阈值），若将每个神经元都看作一个函数，则整个神经网络就是由这些函数相互嵌套而成。</p>
<h2 id="5-2-感知机与多层网络"><a href="#5-2-感知机与多层网络" class="headerlink" title="*5.2 感知机与多层网络*"></a><em><strong>*5.2 感知机与多层网络*</strong></em></h2><p>感知机（Perceptron）是由两层神经元组成的一个简单模型，但只有输出层是M-P神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。这样一来，感知机与之前线性模型中的对数几率回归的思想基本是一样的，都是通过对属性加权与另一个常数求和，再使用sigmoid函数将这个输出值压缩到0-1之间，从而解决分类问题。不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。</p>
<p>给定训练集，则感知机的n+1个参数（n个权重+1个阈值）都可以通过学习得到。阈值Θ可以看作一个输入值固定为-1的哑结点的权重ωn+1，即假设有一个固定输入xn+1&#x3D;-1的输入层神经元，其对应的权重为ωn+1，这样就把权重和阈值统一为权重的学习了。简单感知机的结构如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps363.jpg" alt="img"> </p>
<p>感知机权重的学习规则如下：对于训练样本（x，y），当该样本进入感知机学习后，会产生一个输出值，若该输出值与样本的真实标记不一致，则感知机会对权重进行调整，若激活函数为阶跃函数，则调整的方法为（基于梯度下降法）：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps364.jpg" alt="img"> </p>
<p>其中 η∈（0，1）称为学习率，可以看出感知机是通过逐个样本输入来更新权重，首先设定好初始权重（一般为随机），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本数据的输出值都与真实标记相同。容易看出：感知机模型总是能将训练数据的每一个样本都预测正确，和决策树模型总是能将所有训练数据都分开一样，感知机模型很容易产生过拟合问题。</p>
<p>由于感知机模型只有一层功能神经元，因此其功能十分有限，只能处理线性可分的问题，对于这类问题，感知机的学习过程一定会收敛（converge），因此总是可以求出适当的权值。但是对于像书上提到的异或问题，只通过一层功能神经元往往不能解决，因此要解决非线性可分问题，需要考虑使用多层功能神经元，即神经网络。多层神经网络的拓扑结构如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps365.jpg" alt="img"> </p>
<p>在神经网络中，输入层与输出层之间的层称为隐含层或隐层（hidden layer），隐层和输出层的神经元都是具有激活函数的功能神经元。只需包含一个隐层便可以称为多层神经网络，常用的神经网络称为“多层前馈神经网络”（multi-layer feedforward neural network），该结构满足以下几个特点：</p>
<p>* 每层神经元与下一层神经元之间完全互连</p>
<p>* 神经元之间不存在同层连接</p>
<p>* 神经元之间不存在跨层连接</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps366.jpg" alt="img"> </p>
<p>根据上面的特点可以得知：这里的“前馈”指的是网络拓扑结构中不存在环或回路，而不是指该网络只能向前传播而不能向后传播（下节中的BP神经网络正是基于前馈神经网络而增加了反馈调节机制）。神经网络的学习过程就是根据训练数据来调整神经元之间的“连接权”以及每个神经元的阈值，换句话说：神经网络所学习到的东西都蕴含在网络的连接权与阈值中。</p>
<h2 id="5-3-BP神经网络算法"><a href="#5-3-BP神经网络算法" class="headerlink" title="*5.3 BP神经网络算法*"></a><em><strong>*5.3 BP神经网络算法*</strong></em></h2><p>由上面可以得知：神经网络的学习主要蕴含在权重和阈值中，多层网络使用上面简单感知机的权重调整规则显然不够用了，BP神经网络算法即误差逆传播算法（error BackPropagation）正是为学习多层前馈神经网络而设计，BP神经网络算法是迄今为止最成功的的神经网络学习算法。</p>
<p>一般而言，只需包含一个足够多神经元的隐层，就能以任意精度逼近任意复杂度的连续函数[Hornik et al.,1989]，故下面以训练单隐层的前馈神经网络为例，介绍BP神经网络的算法思想。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps367.jpg" alt="img"> </p>
<p>上图为一个单隐层前馈神经网络的拓扑结构，BP神经网络算法也使用梯度下降法（gradient descent），以单个样本的均方误差的负梯度方向对权重进行调节。可以看出：BP算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的均方误差，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。BP算法基本的推导过程与感知机的推导过程原理是相同的，下面给出调整隐含层到输出层的权重调整规则的推导过程：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps368.jpg" alt="img"> </p>
<p>学习率η∈（0，1）控制着沿反梯度方向下降的步长，若步长太大则下降太快容易产生震荡，若步长太小则收敛速度太慢，一般地常把η设置为0.1，有时更新权重时会将输出层与隐含层设置为不同的学习率。BP算法的基本流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps369.jpg" alt="img"> </p>
<p>BP算法的更新规则是基于每个样本的预测值与真实类标的均方误差来进行权值调节，即BP算法每次更新只针对于单个样例。需要注意的是：BP算法的最终目标是要最小化整个训练集D上的累积误差，即：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps370.jpg" alt="img"> </p>
<p>如果基于累积误差最小化的更新规则，则得到了累积误差逆传播算法（accumulated error backpropagation），即每次读取全部的数据集一遍，进行一轮学习，从而基于当前的累积误差进行权值调整，因此参数更新的频率相比标准BP算法低了很多，但在很多任务中，尤其是在数据量很大的时候，往往标准BP算法会获得较好的结果。另外对于如何设置隐层神经元个数的问题，至今仍然没有好的解决方案，常使用“试错法”进行调整。</p>
<p>前面提到，BP神经网络强大的学习能力常常容易造成过拟合问题，有以下两种策略来缓解BP网络的过拟合问题：</p>
<p>· 早停：将数据分为训练集与测试集，训练集用于学习，测试集用于评估性能，若在训练过程中，训练集的累积误差降低，而测试集的累积误差升高，则停止训练。</p>
<p>· 引入正则化（regularization）：基本思想是在累积误差函数中增加一个用于描述网络复杂度的部分，例如所有权值与阈值的平方和，其中λ∈（0,1）用于对累积经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps371.jpg" alt="img"></p>
<h2 id="5-4-全局最小与局部最小"><a href="#5-4-全局最小与局部最小" class="headerlink" title="*5.4 全局最小与局部最小*"></a><em><strong>*5.4 全局最小与局部最小*</strong></em></h2><p>模型学习的过程实质上就是一个寻找最优参数的过程，例如BP算法试图通过最速下降来寻找使得累积经验误差最小的权值与阈值，在谈到最优时，一般会提到局部极小（local minimum）和全局最小（global minimum）。</p>
<p>* 局部极小解：参数空间中的某个点，其邻域点的误差函数值均不小于该点的误差函数值。</p>
<p>* 全局最小解：参数空间中的某个点，所有其他点的误差函数值均不小于该点的误差函数值。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps372.jpg" alt="img"> </p>
<p>要成为局部极小点，只要满足该点在参数空间中的梯度为零。局部极小可以有多个，而全局最小只有一个。全局最小一定是局部极小，但局部最小却不一定是全局最小。显然在很多机器学习算法中，都试图找到目标函数的全局最小。梯度下降法的主要思想就是沿着负梯度方向去搜索最优解，负梯度方向是函数值下降最快的方向，若迭代到某处的梯度为0，则表示达到一个局部最小，参数更新停止。因此在现实任务中，通常使用以下策略尽可能地去接近全局最小。</p>
<p>* 以多组不同参数值初始化多个神经网络，按标准方法训练，迭代停止后，取其中误差最小的解作为最终参数。</p>
<p>* 使用“模拟退火”技术，这里不做具体介绍。</p>
<p>* 使用随机梯度下降，即在计算梯度时加入了随机因素，使得在局部最小时，计算的梯度仍可能不为0，从而迭代可以继续进行。</p>
<h2 id="5-5-深度学习"><a href="#5-5-深度学习" class="headerlink" title="*5.5 深度学习*"></a><em><strong>*5.5 深度学习*</strong></em></h2><p>理论上，参数越多，模型复杂度就越高，容量（capability）就越大，从而能完成更复杂的学习任务。深度学习（deep learning）正是一种极其复杂而强大的模型。</p>
<p>怎么增大模型复杂度呢？两个办法，一是增加隐层的数目，二是增加隐层神经元的数目。前者更有效一些，因为它不仅增加了功能神经元的数量，还增加了激活函数嵌套的层数。但是对于多隐层神经网络，经典算法如标准BP算法往往会在误差逆传播时发散（diverge），无法收敛达到稳定状态。</p>
<p>那要怎么有效地训练多隐层神经网络呢？一般来说有以下两种方法：</p>
<p>· 无监督逐层训练（unsupervised layer-wise training）：每次训练一层隐节点，把上一层隐节点的输出当作输入来训练，本层隐结点训练好后，输出再作为下一层的输入来训练，这称为预训练（pre-training）。全部预训练完成后，再对整个网络进行微调（fine-tuning）训练。一个典型例子就是深度信念网络（deep belief network，简称DBN）。这种做法其实可以视为把大量的参数进行分组，先找出每组较好的设置，再基于这些局部最优的结果来训练全局最优。</p>
<p>· 权共享（weight sharing）：令同一层神经元使用完全相同的连接权，典型的例子是卷积神经网络（Convolutional Neural Network，简称CNN）。这样做可以大大减少需要训练的参数数目。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps373.jpg" alt="img"> </p>
<p>深度学习可以理解为一种特征学习（feature learning）或者表示学习（representation learning），无论是DBN还是CNN，都是通过多个隐层来把与输出目标联系不大的初始输入转化为与输出目标更加密切的表示，使原来只通过单层映射难以完成的任务变为可能。即通过多层处理，逐渐将初始的“低层”特征表示转化为“高层”特征表示，从而使得最后可以用简单的模型来完成复杂的学习任务。</p>
<p>传统任务中，样本的特征需要人类专家来设计，这称为特征工程（feature engineering）。特征好坏对泛化性能有至关重要的影响。而深度学习为全自动数据分析带来了可能，可以自动产生更好的特征。</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/54647611"><em><strong>*《机器学习》 学习笔记（7）–支持向量机*</strong></em></a></p>
<p>上篇主要介绍了神经网络。首先从生物学神经元出发，引出了它的数学抽象模型–MP神经元以及由两层神经元组成的感知机模型，并基于梯度下降的方法描述了感知机模型的权值调整规则。由于简单的感知机不能处理线性不可分的情形，因此接着引入了含隐层的前馈型神经网络，BP神经网络则是其中最为成功的一种学习方法，它使用误差逆传播的方法来逐层调节连接权。最后简单介绍了局部&#x2F;全局最小以及目前十分火热的深度学习的概念。本篇围绕的核心则是曾经一度取代过神经网络的另一种监督学习算法–****支持向量机*<em><strong>（Support Vector Machine），简称</strong></em>*SVM****。</p>
<h1 id="6、支持向量机"><a href="#6、支持向量机" class="headerlink" title="*6、支持向量机*"></a><em><strong>*6、支持向量机*</strong></em></h1><p>支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。</p>
<h2 id="6-1-函数间隔与几何间隔"><a href="#6-1-函数间隔与几何间隔" class="headerlink" title="*6.1 函数间隔与几何间隔*"></a><em><strong>*6.1 函数间隔与几何间隔*</strong></em></h2><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps374.jpg" alt="img">对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c&#x3D;0来表示，超平面实际上表示的就是高维的平面，如下图所示：</p>
<p>对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps375.jpg" alt="img"> </p>
<h3 id="6-1-1-函数间隔"><a href="#6-1-1-函数间隔" class="headerlink" title="*6.1.1 函数间隔*"></a><em><strong>*6.1.1 函数间隔*</strong></em></h3><p>在超平面w’x+b&#x3D;0确定的情况下，|w’x*+b|能够代表点x<em>距离超平面的远近，易知：当w’x</em>+b&gt;0时，表示x<em>在超平面的一侧（正类，类标为1），而当w’x</em>+b&lt;0时，则表示x<em>在超平面的另外一侧（负类，类别为-1），因此（w’x</em>+b）y* 的正负性恰能表示数据点x<em>是否被分类正确。于是便引出了</em>***函数间隔****的定义（functional margin）:</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps376.jpg" alt="img"> </p>
<p>而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔：<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps377.jpg" alt="img"></p>
<p>可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b&#x3D;0其实等价于2w1x1+2w2x2+2w3x3+2b&#x3D;0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念–几何间隔（geometrical margin）。</p>
<h3 id="6-1-2-几何间隔"><a href="#6-1-2-几何间隔" class="headerlink" title="*6.1.2 几何间隔*"></a><em><strong>*6.1.2 几何间隔*</strong></em></h3><p>**<em>*几何间隔*<em>*<em>代表的则是数据点到超平面的真实距离，对于超平面w’x+b&#x3D;0，w代表的是该超平面的法向量，设x</em>为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x*&#x3D;x-r(w&#x2F;||w||)，又x</em>在超平面上，即w’x</em>+b&#x3D;0，代入即可得：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps378.jpg" alt="img"> </p>
<p>为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps379.jpg" alt="img"> </p>
<p>从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。</p>
<h2 id="6-2-最大间隔与支持向量"><a href="#6-2-最大间隔与支持向量" class="headerlink" title="*6.2 最大间隔与支持向量*"></a><em><strong>*6.2 最大间隔与支持向量*</strong></em></h2><p>通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps380.jpg" alt="img"> </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps381.jpg" alt="img">一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为：<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps382.jpg" alt="img"></p>
<p>对于y(w’x+b)&#x3D;1的数据点，即右图中位于w’x+b&#x3D;1或w’x+b&#x3D;-1上的数据点，我们称之为**<em>*支持向量*<em><em><em>（support vector），易知：对于所有的支持向量，它们恰好满足y</em>(w’x</em>+b)&#x3D;1，而所有不是支持向量的点，有y</em>(w’x</em>+b)&gt;1。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="6-3-从原始优化问题到对偶问题"><a href="#6-3-从原始优化问题到对偶问题" class="headerlink" title="*6.3 从原始优化问题到对偶问题*"></a><em><strong>*6.3 从原始优化问题到对偶问题*</strong></em></h2><p>对于上述得到的目标函数，求1&#x2F;||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps383.jpg" alt="img"> </p>
<p>即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的****对偶问题****，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：</p>
<p>* 一是因为使用对偶问题更容易求解；</p>
<p>* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。</p>
<p>对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps384.jpg" alt="img"> </p>
<p>上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1&#x2F;2||w||^2（此时令所有的α为0），因此实际上原问题等价于：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps385.jpg" alt="img"> </p>
<p>由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps386.jpg" alt="img"> </p>
<p>这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。</p>
<p>（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps387.jpg" alt="img"> </p>
<p>将上述结果代入L得到：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps388.jpg" alt="img"> </p>
<p>（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps389.jpg" alt="img"> </p>
<p>（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps390.jpg" alt="img"></p>
<p>在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)&#x3D;w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps391.jpg" alt="img"> </p>
<p>这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps392.jpg" alt="img"> </p>
<h2 id="6-4-核函数"><a href="#6-4-核函数" class="headerlink" title="*6.4 核函数*"></a><em><strong>*6.4 核函数*</strong></em></h2><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps393.jpg" alt="img">由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用****映射****的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为：</p>
<p>按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：</p>
<p>（1）原对偶问题变为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps394.jpg" alt="img"> </p>
<p>（2）原分类函数变为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps395.jpg" alt="img"> </p>
<p>求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了****核函数****（Kernel）的概念。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps396.jpg" alt="img"> </p>
<p>因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效****（低维计算，高维表现）****，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：</p>
<p>（1）对偶问题：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps397.jpg" alt="img"> </p>
<p>（2）分类函数：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps398.jpg" alt="img"> </p>
<p>因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps399.jpg" alt="img"> </p>
<p>由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps400.jpg" alt="img"> </p>
<h2 id="6-5-软间隔支持向量机"><a href="#6-5-软间隔支持向量机" class="headerlink" title="*6.5 软间隔支持向量机*"></a><em><strong>*6.5 软间隔支持向量机*</strong></em></h2><p>前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有****噪声*<em><strong>的情形，噪声数据（</strong></em>*outlier****）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps401.jpg" alt="img"> </p>
<p>为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了****“软间隔”支持向量机****的概念</p>
<p>* 允许某些数据点不满足约束y(w’x+b)≥1；</p>
<p>* 同时又使得不满足约束的样本尽可能少。</p>
<p>这样优化目标变为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps402.jpg" alt="img"> </p>
<p>如同阶跃函数，0&#x2F;1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps403.jpg" alt="img"> </p>
<p>支持向量机中的损失函数为****hinge损失*<em><strong>，引入</strong></em>*“松弛变量”****，目标函数与约束条件可以写为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps404.jpg" alt="img"> </p>
<p>其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps405.jpg" alt="img"> </p>
<p>按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps406.jpg" alt="img"> </p>
<p>将w代入L化简，便得到其对偶问题：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps407.jpg" alt="img"> </p>
<p>将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/66525491"><em><strong>*《机器学习》 学习笔记（8）–贝叶斯分类器*</strong></em></a></p>
<p>上篇主要介绍和讨论了支持向量机。从最初的分类函数，通过最大化分类间隔，max(1&#x2F;||w||)，min(1&#x2F;2||w||^2)，凸二次规划，拉格朗日函数，对偶问题，一直到最后的SMO算法求解，都为寻找一个最优解。接着引入核函数将低维空间映射到高维特征空间，解决了非线性可分的情形。最后介绍了软间隔支持向量机，解决了outlier挤歪超平面的问题。本篇将讨论一个经典的统计学习算法–****贝叶斯分类器****。</p>
<h1 id="7、贝叶斯分类器"><a href="#7、贝叶斯分类器" class="headerlink" title="*7、贝叶斯分类器*"></a><em><strong>*7、贝叶斯分类器*</strong></em></h1><p>贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下贝叶斯公式。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps408.jpg" alt="img"> </p>
<h2 id="7-1-贝叶斯决策论"><a href="#7-1-贝叶斯决策论" class="headerlink" title="*7.1 贝叶斯决策论*"></a><em><strong>*7.1 贝叶斯决策论*</strong></em></h2><p>若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“****条件风险****”（conditional risk）。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps409.jpg" alt="img"> </p>
<p>我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了****贝叶斯判定准则****（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps410.jpg" alt="img"></p>
<p>若损失函数λ取0-1损失，则有：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps411.jpg" alt="img"> </p>
<p>即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。一般这里有两种策略来对后验概率进行估计：</p>
<p>* 判别式模型：直接对 P（c | x）进行建模求解。例如我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。</p>
<p>* 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。</p>
<p>贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c | x） 进行一项神奇的变换P（c | x）变身：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps412.jpg" alt="img"> </p>
<p>对于给定的样本x，P（x）与类标无关，P（c）称为类先验概率，p（x | c ）称为类条件概率。这时估计后验概率P（c | x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率，普及一下它们的基本概念。</p>
<p>* 先验概率： 根据以往经验和分析得到的概率。</p>
<p>* 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。</p>
<p>实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率</p>
<p>。<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps413.jpg" alt="img"></p>
<p>回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。</p>
<h2 id="7-2-极大似然法"><a href="#7-2-极大似然法" class="headerlink" title="*7.2 极大似然法*"></a><em><strong>*7.2 极大似然法*</strong></em></h2><p>极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps414.jpg" alt="img"> </p>
<p>所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：</p>
<p>* 1.写出似然函数；</p>
<p>* 2.对似然函数取对数，并整理；</p>
<p>* 3.求导数，令偏导数为0，得到似然方程组；</p>
<p>* 4.解似然方程组，得到所有参数即为所求。</p>
<p>例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps415.jpg" alt="img"> </p>
<p>上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。</p>
<h2 id="7-3-朴素贝叶斯分类器"><a href="#7-3-朴素贝叶斯分类器" class="headerlink" title="*7.3 朴素贝叶斯分类器*"></a><em><strong>*7.3 朴素贝叶斯分类器*</strong></em></h2><p>不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为：<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps416.jpg" alt="img"></p>
<p>这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps417.jpg" alt="img"></p>
<p>相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps418.jpg" alt="img"> </p>
<p>当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/68924781"><em><strong>*《机器学习》 学习笔记（9）–EM算法*</strong></em></a></p>
<p>上篇主要介绍了贝叶斯分类器，从贝叶斯公式到贝叶斯决策论，再到通过极大似然法估计类条件概率，贝叶斯分类器的训练就是参数估计的过程。朴素贝叶斯则是“属性条件独立性假设”下的特例，它避免了假设属性联合分布过于经验性和训练集不足引起参数估计较大偏差两个大问题，最后介绍的拉普拉斯修正将概率值进行平滑处理。本篇将介绍另一个当选为数据挖掘十大算法之一的****EM算法****。</p>
<h1 id="8、EM算法"><a href="#8、EM算法" class="headerlink" title="*8、EM算法*"></a><em><strong>*8、EM算法*</strong></em></h1><p>EM（Expectation-Maximization）算法是一种常用的估计参数隐变量的利器，也称为“期望最大算法”，是数据挖掘的十大经典算法之一。EM算法主要应用于训练集样本不完整即存在隐变量时的情形（例如某个属性值未知），通过其独特的“两步走”策略能较好地估计出隐变量的值。</p>
<h2 id="8-1-EM算法思想"><a href="#8-1-EM算法思想" class="headerlink" title="*8.1 EM算法思想*"></a><em><strong>*8.1 EM算法思想*</strong></em></h2><p>EM是一种迭代式的方法，它的基本思想就是：若样本服从的分布参数θ已知，则可以根据已观测到的训练样本推断出隐变量Z的期望值（E步），若Z的值已知则运用最大似然法估计出新的θ值（M步）。重复这个过程直到Z和θ值不再发生变化。</p>
<p>简单来讲：假设我们想估计A和B这两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps419.jpg" alt="img"> </p>
<p>现在再来回想聚类的代表算法K-Means：【首先随机选择类中心&#x3D;&gt;将样本点划分到类簇中&#x3D;&gt;重新计算类中心&#x3D;&gt;不断迭代直至收敛】，不难发现这个过程和EM迭代的方法极其相似，事实上，若将样本的类别看做为“隐变量”（latent variable）Z，类中心看作样本的分布参数θ，K-Means就是通过EM算法来进行迭代的，与我们这里不同的是，K-Means的目标是最小化样本点到其对应类中心的距离和，上述为极大化似然函数。</p>
<h2 id="8-2-EM算法数学推导"><a href="#8-2-EM算法数学推导" class="headerlink" title="*8.2 EM算法数学推导*"></a><em><strong>*8.2 EM算法数学推导*</strong></em></h2><p>在上篇极大似然法中，当样本属性值都已知时，我们很容易通过极大化对数似然，接着对每个参数求偏导计算出参数的值。但当存在隐变量时，就无法直接求解，此时我们通常最大化已观察数据的对数“边际似然”（marginal likelihood）。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps420.jpg" alt="img"> </p>
<p>这时候，通过边缘似然将隐变量Z引入进来，对于参数估计，现在与最大似然不同的只是似然函数式中多了一个未知的变量Z，也就是说我们的目标是找到适合的θ和Z让L(θ)最大，这样我们也可以分别对未知的θ和Z求偏导，再令其等于0。</p>
<p>然而观察上式可以发现，和的对数（ln(x1+x2+x3)）求导十分复杂，那能否通过变换上式得到一种求导简单的新表达式呢？这时候 Jensen不等式就派上用场了，先回顾一下高等数学凸函数的内容：</p>
<p>****Jensen’s inequality****：过一个凸函数上任意两点所作割线一定在这两点间的函数图象的上方。理解起来也十分简单，对于凸函数f(x)”&gt;0，即曲线的变化率是越来越大单调递增的，所以函数越到后面增长越厉害，这样在一个区间下，函数的均值就会大一些了。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps421.jpg" alt="img"> </p>
<p>因为ln(*)函数为凹函数，故可以将上式“和的对数”变为“对数的和”，这样就很容易求导了。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps422.jpg" alt="img"> </p>
<p>接着求解Qi和θ：首先固定θ（初始值），通过求解Qi使得J（θ，Q）在θ处与L（θ）相等，即求出L（θ）的下界；然后再固定Qi，调整θ，最大化下界J（θ，Q）。不断重复两个步骤直到稳定。通过jensen不等式的性质，Qi的计算公式实际上就是后验概率：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps423.jpg" alt="img"> </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps424.jpg" alt="img">通过数学公式的推导，简单来理解这一过程：固定θ计算Q的过程就是在建立L（θ）的下界，即通过jenson不等式得到的下界（E步）；固定Q计算θ则是使得下界极大化（M步），从而不断推高边缘似然L（θ）。从而循序渐进地计算出L（θ）取得极大值时隐变量Z的估计值。</p>
<p>EM算法也可以看作一种“坐标下降法”，首先固定一个值，对另外一个值求极值，不断重复直到收敛。这时候也许大家就有疑问，问什么不直接这两个家伙求偏导用梯度下降呢？这时候就是坐标下降的优势，有些特殊的函数，例如曲线函数z&#x3D;y^2+x^2+x^2y+xy+…，无法直接求导，这时如果先固定其中的一个变量，再对另一个变量求极值，则变得可行。</p>
<h2 id="8-3-EM算法流程"><a href="#8-3-EM算法流程" class="headerlink" title="*8.3 EM算法流程*"></a><em><strong>*8.3 EM算法流程*</strong></em></h2><p>看完数学推导，算法的流程也就十分简单了，这里有两个版本，版本一来自西瓜书，周天使的介绍十分简洁；版本二来自于大牛的博客。结合着数学推导，自认为版本二更具有逻辑性，两者唯一的区别就在于版本二多出了红框的部分</p>
<p><em><strong>*版本一：*</strong></em></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps425.jpg" alt="img"> </p>
<p><em><strong>*版本二：*</strong></em></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps426.jpg" alt="img"> </p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/70172971"><em><strong>*《机器学习》 学习笔记（10）–集成学习*</strong></em></a></p>
<p>上篇主要介绍了鼎鼎大名的EM算法，从算法思想到数学公式推导（边际似然引入隐变量，Jensen不等式简化求导），EM算法实际上可以理解为一种坐标下降法，首先固定一个变量，接着求另外变量的最优解，通过其优美的“两步走”策略能较好地估计隐变量的值。本篇将继续讨论下一类经典算法–集成学习。</p>
<h1 id="9、集成学习"><a href="#9、集成学习" class="headerlink" title="*9、集成学习*"></a><em><strong>*9、集成学习*</strong></em></h1><p>顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生<del>…</del>，即其泛化性能要能优于其中任何一个学习器。</p>
<h2 id="9-1-个体与集成"><a href="#9-1-个体与集成" class="headerlink" title="*9.1 个体与集成*"></a><em><strong>*9.1 个体与集成*</strong></em></h2><p>集成学习的基本结构为：先产生一组个体学习器，再使用某种策略将它们结合在一起。集成模型如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps427.jpg" alt="img"> </p>
<p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p>
<p>****同质集成****：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。<br>****异质集成****：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。</p>
<p>上面我们已经提到要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：****准确性*<em><strong>和</strong></em>*多样性****（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps428.jpg" alt="img"> </p>
<p>现在考虑二分类的简单情形，假设基分类器之间相互独立（能提供较高的差异度），且错误率相等为 ε，则可以将集成器的预测看做一个伯努利实验，易知当所有基分类器中不足一半预测正确的情况下，集成器预测错误，所以集成器的错误率可以计算为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps429.jpg" alt="img"> </p>
<p>此时，集成器错误率随着基分类器的个数的增加呈指数下降，但前提是基分类器之间相互独立，在实际情形中显然是不可能的，假设训练有A和B两个分类器，对于某个测试样本，显然满足：P（A&#x3D;1 | B&#x3D;1）&gt; P（A&#x3D;1），因为A和B为了解决相同的问题而训练，因此在预测新样本时存在着很大的联系。因此，****个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量*<em><strong>，准确性高意味着牺牲多样性，所以产生“</strong></em>*好而不同****”的个体学习器正是集成学习研究的核心。现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><h2 id="-2"><a href="#-2" class="headerlink" title=""></a></h2><h2 id="9-2-Boosting"><a href="#9-2-Boosting" class="headerlink" title="*9.2 Boosting*"></a><em><strong>*9.2 Boosting*</strong></em></h2><p>Boosting是一种串行的工作机制，即个体学习器的训练存在依赖关系，必须一步一步序列化进行。其基本思想是：增加前一个基学习器在训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p>
<p>Boosting族算法最著名、使用最为广泛的就是AdaBoost，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是****指数损失函数*<em><strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。看到这里回想一下之前的机器学习算法，</strong></em>*不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数****：如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic Regression了。</p>
<p>定义基学习器的集成为加权结合，则有：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps430.jpg" alt="img"> </p>
<p>AdaBoost算法的指数损失函数定义为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps431.jpg" alt="img"> </p>
<p>具体说来，整个Adaboost 迭代算法分为3步：</p>
<p>· 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1&#x2F;N。</p>
<p>· 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</p>
<p>· 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</p>
<p>整个AdaBoost的算法流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps432.jpg" alt="img"> </p>
<p>可以看出：****AdaBoost的核心步骤就是计算基学习器权重和样本权重分布*<em><strong>，那为何是上述的计算公式呢？这就涉及到了我们之前为什么说大部分带参机器学习算法只是改变了损失函数，就是因为</strong></em>*大部分模型的参数都是通过最优化损失函数（可能还加个规则项）而计算（梯度下降，坐标下降等）得到****，这里正是通过最优化指数损失函数从而得到这两个参数的计算公式，具体的推导过程此处不进行展开,建议掌握。（见原书：p174-175）</p>
<p>Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样本分布权重，这里书上提到了两种方法：“重赋权法”（re-weighting）和“重采样法”（re-sampling），书上的解释有些晦涩，这里进行展开一下：</p>
<p><em><strong>*重赋权法*</strong></em> : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。<br><em><strong>*重采样法*</strong></em> : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p>
<p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习提升为强学习器。从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。</p>
<h2 id="9-3-Bagging与Random-Forest"><a href="#9-3-Bagging与Random-Forest" class="headerlink" title="*9.3 Bagging与Random Forest*"></a><em><strong>*9.3 Bagging与Random Forest*</strong></em></h2><p>相比之下，Bagging与随机森林算法就简洁了许多，上面已经提到产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思（tao）想（lu）都是通过“自助采样”的方法来增加多样性。</p>
<h3 id="9-3-1-Bagging"><a href="#9-3-1-Bagging" class="headerlink" title="*9.3.1 Bagging*"></a><em><strong>*9.3.1 Bagging*</strong></em></h3><p>Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序，可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps433.jpg" alt="img"> </p>
<p>Bagging算法的流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps434.jpg" alt="img"> </p>
<p>可以看出Bagging主要通过<em><strong>*样本的扰动*<em><strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：</strong></em>*AdaBoost关注于降低偏差，而Bagging关注于降低方差。*</strong></em></p>
<p><em><strong>*<br>*</strong></em></p>
<h3 id="9-3-2-随机森林"><a href="#9-3-2-随机森林" class="headerlink" title="*9.3.2 随机森林*"></a><em><strong>*9.3.2 随机森林*</strong></em></h3><p>随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种****属性扰动****，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K&#x3D;log2（d）。</p>
<p>这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps435.jpg" alt="img"> </p>
<h2 id="9-4-结合策略"><a href="#9-4-结合策略" class="headerlink" title="*9.4 结合策略*"></a><em><strong>*9.4 结合策略*</strong></em></h2><p>结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略：</p>
<h3 id="9-4-1-平均法（回归问题）"><a href="#9-4-1-平均法（回归问题）" class="headerlink" title="*9.4.1 平均法（回归问题）*"></a><em><strong>*9.4.1 平均法（回归问题）*</strong></em></h3><h3 id="、"><a href="#、" class="headerlink" title="*、*"></a><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps436.jpg" alt="img"><em><strong>*、*</strong></em></h3><p> <img src="https://raw.githubusercontent.com/ebxeax/images/main/wps437.jpg" alt="img"></p>
<p>易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，****一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法****。</p>
<h3 id="9-4-2-投票法（分类问题）"><a href="#9-4-2-投票法（分类问题）" class="headerlink" title="*9.4.2 投票法（分类问题）*"></a><em><strong>*9.4.2 投票法（分类问题）*</strong></em></h3><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps438.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps439.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps440.jpg" alt="img"></p>
<p>绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps441.jpg" alt="img"> </p>
<p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，****一般基于类概率进行结合往往比基于类标记进行结合的效果更好****，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p>
<h3 id="9-4-3-学习法"><a href="#9-4-3-学习法" class="headerlink" title="*9.4.3 学习法*"></a><em><strong>*9.4.3 学习法*</strong></em></h3><p>学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个m<em>T的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：</em>***投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps442.jpg" alt="img"> </p>
<h2 id="9-5-多样性（diversity）"><a href="#9-5-多样性（diversity）" class="headerlink" title="*9.5 多样性（diversity）*"></a><em><strong>*9.5 多样性（diversity）*</strong></em></h2><p>在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。</p>
<p>****数据样本扰动****，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。<br>****输入属性扰动****，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。<br>****输出表示扰动****，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。<br>****算法参数扰动****，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。</p>
<p>在此，集成学习就介绍完毕，看到这里，大家也会发现集成学习实质上是一种通用框架，可以使用任何一种基学习器，从而改进单个学习器的泛化性能。据说数据挖掘竞赛KDDCup历年的冠军几乎都使用了集成学习，看来的确是个好东西~</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/70991604"><em><strong>*《机器学习》 学习笔记（11）–聚类*</strong></em></a></p>
<p>上篇主要介绍了一种机器学习的通用框架–集成学习方法，首先从准确性和差异性两个重要概念引出集成学习“****好而不同****”的四字真言，接着介绍了现阶段主流的三种集成学习方法：AdaBoost、Bagging及Random Forest，AdaBoost采用最小化指数损失函数迭代式更新样本分布权重和计算基学习器权重，Bagging通过自助采样引入样本扰动增加了基学习器之间的差异性，随机森林则进一步引入了属性扰动，最后简单概述了集成模型中的三类结合策略：平均法、投票法及学习法，其中Stacking是学习法的典型代表。本篇将讨论无监督学习中应用最为广泛的学习算法–聚类。</p>
<h1 id="10、聚类算法"><a href="#10、聚类算法" class="headerlink" title="*10、聚类算法*"></a><em><strong>*10、聚类算法*</strong></em></h1><p>聚类是一种经典的****无监督学习*<em><strong>方法，</strong></em>*无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律****，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。</p>
<p>聚类直观上来说是将相似的样本聚在一起，从而形成一个****类簇（cluster）*<em><strong>。那首先的问题是如何来</strong></em>*度量相似性*<em><strong>（similarity measure）呢？这便是</strong></em>*距离度量*<em><strong>，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是</strong></em>*性能度量****，性能度量为评价聚类结果的好坏提供了一系列有效性指标。</p>
<h2 id="10-1-距离度量"><a href="#10-1-距离度量" class="headerlink" title="*10.1 距离度量*"></a><em><strong>*10.1 距离度量*</strong></em></h2><p>谈及距离度量，最熟悉的莫过于欧式距离了，从年头一直用到年尾的距离计算公式：即对应属性之间相减的平方和再开根号。度量距离还有其它的很多经典方法，通常它们需要满足一些基本性质：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps443.jpg" alt="img"> </p>
<p>最常用的距离度量方法是****“闵可夫斯基距离”（Minkowski distance)****：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps444.jpg" alt="img"> </p>
<p>当p&#x3D;1时，闵可夫斯基距离即****曼哈顿距离（Manhattan distance）****：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps445.jpg" alt="img"> </p>
<p>当p&#x3D;2时，闵可夫斯基距离即****欧氏距离（Euclidean distance）****：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps446.jpg" alt="img"> </p>
<p>我们知道属性分为两种：****连续属性*<em><strong>和</strong></em>*离散属性****（有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</p>
<p>若属性值之间****存在序关系*<em><strong>，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1, 0.5, 0}。<br>若属性值之间</strong></em>*不存在序关系****，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0），（0,1）}。</p>
<p>在进行距离度量时，易知****连续属性和存在序关系的离散属性都可以直接参与计算*<em><strong>，因为它们都可以反映一种程度，我们称其为“</strong></em>*有序属性*<em><strong>”；而对于不存在序关系的离散属性，我们称其为：“</strong></em>*无序属性****”，显然无序属性再使用闵可夫斯基距离就行不通了。</p>
<p>****对于无序属性，我们一般采用VDM进行距离的计算****，例如：对于离散属性的两个取值a和b，定义（p200）：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps447.jpg" alt="img"> </p>
<p>于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和VDM混合在一起进行计算：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps448.jpg" alt="img"> </p>
<p>若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：****非度量距离（non-metric distance）****。</p>
<h2 id="10-2-性能度量"><a href="#10-2-性能度量" class="headerlink" title="*10.2 性能度量*"></a><em><strong>*10.2 性能度量*</strong></em></h2><p>由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：****外部指标*<em><strong>和</strong></em>*内部指标****。</p>
<h3 id="10-2-1-外部指标"><a href="#10-2-1-外部指标" class="headerlink" title="*10.2.1 外部指标*"></a><em><strong>*10.2.1 外部指标*</strong></em></h3><p>即将聚类结果与某个参考模型的结果进行比较，****以参考模型的输出作为标准，来评价聚类好坏***<em>。假设聚类给出的结果为λ，参考模型给出的结果是λ</em>，则我们将样本进行两两配对，定义：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps449.jpg" alt="img"> </p>
<p>显然a和b代表着聚类结果好坏的正能量，b和c则表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps450.jpg" alt="img"> </p>
<h3 id="10-2-2-内部指标"><a href="#10-2-2-内部指标" class="headerlink" title="*10.2.2 内部指标*"></a><em><strong>*10.2.2 内部指标*</strong></em></h3><p>内部指标即不依赖任何外部模型，直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开，直观来说：****簇内高内聚紧紧抱团，簇间低耦合老死不相往来****。定义：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps451.jpg" alt="img"> </p>
<p>基于上面的四个距离，可以导出下面这些常用的内部评价指标：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps452.jpg" alt="img"> </p>
<h2 id="10-3-原型聚类"><a href="#10-3-原型聚类" class="headerlink" title="*10.3 原型聚类*"></a><em><strong>*10.3 原型聚类*</strong></em></h2><p>原型聚类即“****基于原型的聚类****”（prototype-based clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，常见的K-Means便是基于簇中心来实现聚类，混合高斯聚类则是基于簇分布来实现聚类。</p>
<h3 id="10-3-1-K-Means"><a href="#10-3-1-K-Means" class="headerlink" title="*10.3.1 K-Means*"></a><em><strong>*10.3.1 K-Means*</strong></em></h3><p>K-Means的思想十分简单，****首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛*<em><strong>。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过</strong></em>*EM算法****的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps453.jpg" alt="img"> </p>
<p>K-Means的算法流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps454.jpg" alt="img"> </p>
<h3 id="10-3-2-学习向量量化（LVQ）"><a href="#10-3-2-学习向量量化（LVQ）" class="headerlink" title="*10.3.2 学习向量量化（LVQ）*"></a><em><strong>*10.3.2 学习向量量化（LVQ）*</strong></em></h3><p>LVQ也是基于原型的聚类算法，与K-Means不同的是，****LVQ使用样本真实类标记辅助聚类*<em><strong>，首先LVQ根据样本的类标记，从各类中分别随机选出一个样本作为该类簇的原型，从而组成了一个</strong></em>*原型特征向量组****，接着从样本集中随机挑选一个样本，计算其与原型向量组中每个向量的距离，并选取距离最小的原型向量所在的类簇作为它的划分结果，再与真实类标比较。</p>
<p><em><strong>*若划分结果正确，则对应原型向量向这个样本靠近一些*</strong></em><br><em><strong>*若划分结果不正确，则对应原型向量向这个样本远离一些*</strong></em></p>
<p>LVQ算法的流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps455.jpg" alt="img"> </p>
<h3 id="10-3-3-高斯混合聚类"><a href="#10-3-3-高斯混合聚类" class="headerlink" title="*10.3.3 高斯混合聚类*"></a><em><strong>*10.3.3 高斯混合聚类*</strong></em></h3><p>现在可以看出K-Means与LVQ都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设****每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成****。</p>
<p>对于多维高斯分布，其概率密度函数如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps456.jpg" alt="img"> </p>
<p>其中u表示均值向量，∑表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。接着定义高斯混合分布为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps457.jpg" alt="img"> </p>
<p>α称为混合系数，这样空间中样本的采集过程则可以抽象为：****（1）先选择一个类簇（高斯分布），（2）再根据对应高斯分布的密度函数进行采样****，这时候贝叶斯公式又能大展身手了：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps458.jpg" alt="img"> </p>
<p>此时只需要选择PM最大时的类簇并将该样本划分到其中，看到这里很容易发现：这和那个传说中的贝叶斯分类不是神似吗，都是通过贝叶斯公式展开，然后计算类先验概率和类条件概率。但遗憾的是：****这里没有真实类标信息，对于类条件概率，并不能像贝叶斯分类那样通过最大似然法计算出来****，因为这里的样本可能属于所有的类簇，这里的似然函数变为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps459.jpg" alt="img"> </p>
<p>可以看出：简单的最大似然法根本无法求出所有的参数，这样PM也就没法计算。****这里就要召唤出之前的EM大法，首先对高斯分布的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps460.jpg" alt="img"> </p>
<p>高斯混合聚类的算法流程如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps461.jpg" alt="img"> </p>
<h2 id="10-4-密度聚类"><a href="#10-4-密度聚类" class="headerlink" title="*10.4 密度聚类*"></a><em><strong>*10.4 密度聚类*</strong></em></h2><p>密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是****DBSCAN****算法，首先定义以下概念：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps462.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps463.jpg" alt="img"></p>
<p>简单来理解DBSCAN便是：****找出一个核心对象所有密度可达的样本集合形成簇****。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps464.jpg" alt="img"> </p>
<h2 id="10-5-层次聚类"><a href="#10-5-层次聚类" class="headerlink" title="*10.5 层次聚类*"></a><em><strong>*10.5 层次聚类*</strong></em></h2><p>层次聚类是一种基于树形结构的聚类方法，常用的是****自底向上*<em><strong>的结合策略（</strong></em>*AGNES算法****）。假设有N个待聚类的样本，其基本步骤是：</p>
<p>1.初始化–&gt;把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；<br>2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；<br>3.重新计算新生成的这个****类与各个旧类之间的相似度****；<br>4.重复2和3直到所有样本点都归为一类，结束。</p>
<p>可以看出其中最关键的一步就是****计算两个类簇的相似度****，这里有多种度量方法：</p>
<p>* 单链接（single-linkage）:取类间最小距离。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps465.jpg" alt="img"> </p>
<p>* 全链接（complete-linkage）:取类间最大距离</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps466.jpg" alt="img"> </p>
<p>* 均链接（average-linkage）:取类间两两的平均距离</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps467.jpg" alt="img"> </p>
<p>很容易看出：****单链接的包容性极强，稍微有点暧昧就当做是自己人了，全链接则是坚持到底，只要存在缺点就坚决不合并，均连接则是从全局出发顾全大局****。层次聚类法的算法流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps468.jpg" alt="img"> </p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/72123031"><em><strong>*《机器学习》 学习笔记（12）–降维与度量学习*</strong></em></a></p>
<p>上篇主要介绍了几种常用的聚类算法，首先从距离度量与性能评估出发，列举了常见的距离计算公式与聚类评价指标，接着分别讨论了K-Means、LVQ、高斯混合聚类、密度聚类以及层次聚类算法。K-Means与LVQ都试图以类簇中心作为原型指导聚类，其中K-Means通过EM算法不断迭代直至收敛，LVQ使用真实类标辅助聚类；高斯混合聚类采用高斯分布来描述类簇原型；密度聚类则是将一个核心对象所有密度可达的样本形成类簇，直到所有核心对象都遍历完；最后层次聚类是一种自底向上的树形聚类方法，不断合并最相近的两个小类簇。本篇将讨论机器学习常用的方法–降维与度量学习。</p>
<h1 id="11、降维与度量学习"><a href="#11、降维与度量学习" class="headerlink" title="*11、降维与度量学习*"></a><em><strong>*11、降维与度量学习*</strong></em></h1><p>样本的特征数称为****维数*<em><strong>（dimensionality），当维数非常大时，也就是现在所说的“</strong></em>*维数灾难*<em><strong>”，具体表现在：在高维情形下，</strong></em>*数据样本将变得十分稀疏*<em><strong>，因为此时要满足训练样本为“</strong></em>*密采样*<em><strong>”的总体样本数目是一个触不可及的天文数字，谓可远观而不可亵玩焉…</strong></em>*训练样本的稀疏使得其代表总体分布的能力大大减弱，从而消减了学习器的泛化能力*<em><strong>；同时当维数很高时，</strong></em>*计算距离也变得十分复杂*<em><strong>，甚至连计算内积都不再容易，这也是为什么支持向量机（SVM）使用核函数</strong></em>*“低维计算，高维表现”****的原因。</p>
<p>缓解维数灾难的一个重要途径就是****降维，即通过某种数学变换将原始高维空间转变到一个低维的子空间*<em><strong>。在这个子空间中，样本的密度将大幅提高，同时距离计算也变得容易。这时也许会有疑问，这样降维之后不是会丢失原始数据的一部分信息吗？这是因为在很多实际的问题中，虽然训练数据是高维的，但是与学习任务相关也许仅仅是其中的一个低维子空间，也称为一个</strong></em>*低维嵌入*<em><strong>，例如：数据属性中存在噪声属性、相似属性或冗余属性等，</strong></em>*对高维数据进行降维能在一定程度上达到提炼低维优质属性或降噪的效果****。</p>
<h2 id="11-1-K近邻学习"><a href="#11-1-K近邻学习" class="headerlink" title="*11.1 K近邻学习*"></a><em><strong>*11.1 K近邻学习*</strong></em></h2><p>k近邻算法简称****kNN（k-Nearest Neighbor）*<em><strong>，是一种经典的监督学习方法，同时也实力担当入选数据挖掘十大算法。其工作机制十分简单粗暴：给定某个测试样本，kNN基于某种</strong></em>*距离度量****在训练集中找出与其距离最近的k个带有真实标记的训练样本，然后给基于这k个邻居的真实标记来进行预测，类似于前面集成学习中所讲到的基学习器结合策略：分类任务采用投票法，回归任务则采用平均法。接下来本篇主要就kNN分类进行讨论。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps469.jpg" alt="img">从左图中我们可以看到，图中有两种类型的样本，一类是蓝色正方形，另一类是红色三角形。而那个绿色圆形是我们待分类的样本。基于kNN算法的思路，我们很容易得到以下结论：</p>
<p>如果K&#x3D;3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。<br>如果K&#x3D;5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。</p>
<p>可以发现：****kNN虽然是一种监督学习方法，但是它却没有显式的训练过程*<em><strong>，而是当有新样本需要预测时，才来计算出最近的k个邻居，因此</strong></em>*kNN是一种典型的懒惰学习方法*<em><strong>，再来回想一下朴素贝叶斯的流程，训练的过程就是参数估计，因此朴素贝叶斯也可以懒惰式学习，此类技术在</strong></em>*训练阶段开销为零*<em><strong>，待收到测试样本后再进行计算。相应地我们称那些一有训练数据立马开工的算法为“</strong></em>*急切学习****”，可见前面我们学习的大部分算法都归属于急切学习。</p>
<p>很容易看出：****kNN算法的核心在于k值的选取以及距离的度量*<em><strong>。k值选取太小，模型很容易受到噪声数据的干扰，例如：极端地取k&#x3D;1，若待分类样本正好与一个噪声数据距离最近，就导致了分类错误；若k值太大， 则在更大的邻域内进行投票，此时模型的预测能力大大减弱，例如：极端取k&#x3D;训练样本数，就相当于模型根本没有学习，所有测试样本的预测结果都是一样的。</strong></em>*一般地我们都通过交叉验证法来选取一个适当的k值****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps470.jpg" alt="img"> </p>
<p>对于距离度量，****不同的度量方法得到的k个近邻不尽相同，从而对最终的投票结果产生了影响*<em><strong>，因此选择一个合适的距离度量方法也十分重要。在上一篇聚类算法中，在度量样本相似性时介绍了常用的几种距离计算方法，包括</strong></em>*闵可夫斯基距离，曼哈顿距离，VDM*<em><strong>等。在实际应用中，</strong></em>*kNN的距离度量函数一般根据样本的特性来选择合适的距离度量，同时应对数据进行去量纲&#x2F;归一化处理来消除大量纲属性的强权政治影响****。</p>
<h2 id="11-2-MDS算法"><a href="#11-2-MDS算法" class="headerlink" title="*11.2 MDS算法*"></a><em><strong>*11.2 MDS算法*</strong></em></h2><p>不管是使用核函数升维还是对数据降维，我们都希望<em><strong>*原始空间样本点之间的距离在新空间中基本保持不变*<em><strong>，这样才不会使得原始空间样本之间的关系及总体分布发生较大的改变。</strong></em>*“多维缩放”（MDS*</strong></em><em><strong>*： multiple Dimession Scaling*</strong></em> ****）*<em><strong>正是基于这样的思想，</strong></em>*MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持****。</p>
<p>假定m个样本在原始空间中任意两两样本之间的距离矩阵为D∈R(m<em>m)，我们的目标便是获得样本在低维空间中的表示Z∈R(d’</em>m , d’&lt; d)，且任意两个样本在低维空间中的欧式距离等于原始空间中的距离，即||zi-zj||&#x3D;Dist(ij)。因此接下来我们要做的就是根据已有的距离矩阵D来求解出降维后的坐标矩阵Z。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps471.jpg" alt="img"> </p>
<p>令降维后的样本坐标矩阵Z被中心化，****中心化是指将每个样本向量减去整个样本集的均值向量，故所有样本向量求和得到一个零向量****。这样易知：矩阵B的每一列以及每一列求和均为0，因为提取公因子后都有一项为所有样本向量的和向量。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps472.jpg" alt="img"> </p>
<p>根据上面矩阵B的特征，我们很容易得到等式（2）、（3）以及（4）：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps473.jpg" alt="img"> </p>
<p>这时根据(1)–(4)式我们便可以计算出bij，即<em><strong>*bij&#x3D;(1)-(2)*</strong></em><em>*<em>*</em>(1&#x2F;m)-(3)***</em>*****(1&#x2F;m)+(4)*(1&#x2F;(m^2))***<em>，再逐一地计算每个b(ij)，就得到了降维后低维空间中的内积矩阵B(B&#x3D;Z’</em>Z)，只需对B进行特征值分解便可以得到Z。MDS的算法流程如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps474.jpg" alt="img"> </p>
<h2 id="11-3-主成分分析（PCA）"><a href="#11-3-主成分分析（PCA）" class="headerlink" title="*11.3 主成分分析（PCA）*"></a><em><strong>*11.3 主成分分析（PCA）*</strong></em></h2><p>不同于MDS采用距离保持的方法，<em><strong>*主成分分析（PCA*</strong></em><em><strong>*: Principle Component Analysis*</strong></em><em><strong>*）直接通过一个线性变换，将原始空间中的样本投影到新的低维空间中*<em><strong>。简单来理解这一过程便是：</strong></em>*PCA采用一组新的基来表示样本点，其中每一个基向量都是原来基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。*</strong></em></p>
<p>假设使用d’个新基向量来表示原来样本，实质上是将样本投影到一个由d’个基向量确定的一个****超平面*<em><strong>上（</strong></em>*即舍弃了一些维度*<em><strong>），要用一个超平面对空间中所有高维样本进行恰当的表达，最理想的情形是：</strong></em>*若这些样本点都能在超平面上表出且这些表出在超平面上都能够很好地分散开来****。但是一般使用较原空间低一些维度的超平面来做到这两点十分不容易，因此我们退一步海阔天空，要求这个超平面应具有如下两个性质：</p>
<p>****最近重构性****：样本点到超平面的距离足够近，即尽可能在超平面附近；<br>****最大可分性****：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。</p>
<p>这里十分神奇的是：****最近重构性与最大可分性虽然从不同的出发点来定义优化问题中的目标函数，但最终这两种特性得到了完全相同的优化问题****：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps475.jpg" alt="img"> </p>
<p>接着使用拉格朗日乘子法求解上面的优化问题，得到：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps476.jpg" alt="img"> </p>
<p>因此只需对协方差矩阵进行特征值分解即可求解出W，PCA算法的整个流程如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps477.jpg" alt="img"> </p>
<p>一篇博客给出更通俗更详细的理解：<a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/57472730">http://blog.csdn.net/u011826404/article/details/57472730</a> </p>
<h2 id="11-4-核化线性降维"><a href="#11-4-核化线性降维" class="headerlink" title="*11.4 核化线性降维*"></a><em><strong>*11.4 核化线性降维*</strong></em></h2><p>说起机器学习你中有我&#x2F;我中有你&#x2F;水乳相融…在这里能够得到很好的体现。正如SVM在处理非线性可分时，通过引入核函数将样本投影到高维特征空间，接着在高维空间再对样本点使用超平面划分。这里也是相同的问题：若我们的样本数据点本身就不是线性分布，那还如何使用一个超平面去近似表出呢？因此也就引入了核函数，****即先将样本映射到高维空间，再在高维空间中使用线性降维的方法*<em><strong>。下面主要介绍</strong></em>*核化主成分分析（KPCA）****的思想。</p>
<p>若核函数的形式已知，即我们知道如何将低维的坐标变换为高维坐标，这时我们只需先将数据映射到高维特征空间，再在高维空间中运用PCA即可。但是一般情况下，我们并不知道核函数具体的映射规则，例如：Sigmoid、高斯核等，我们只知道如何计算高维空间中的样本内积，这时就引出了KPCA的一个重要创新之处：****即空间中的任一向量，都可以由该空间中的所有样本线性表示****。证明过程也十分简单：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps478.jpg" alt="img"> </p>
<p>这样我们便可以将高维特征空间中的投影向量wi使用所有高维样本点线性表出，接着代入PCA的求解问题，得到：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps479.jpg" alt="img"> </p>
<p>化简到最后一步，发现结果十分的美妙，只需对核矩阵K进行特征分解，便可以得出投影向量wi对应的系数向量α，因此选取特征值前d’大对应的特征向量便是d’个系数向量。这时对于需要降维的样本点，只需按照以下步骤便可以求出其降维后的坐标。可以看出：KPCA在计算降维后的坐标表示时，需要与所有样本点计算核函数值并求和，因此该算法的计算开销十分大。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps480.jpg" alt="img"> </p>
<h2 id="11-5-流形学习"><a href="#11-5-流形学习" class="headerlink" title="*11.5 流形学习*"></a><em><strong>*11.5 流形学习*</strong></em></h2><p>****流形学习（manifold learning）是一种借助拓扑流形概念的降维方法*<em><strong>，</strong></em>*流形是指在局部与欧式空间同胚的空间*<em><strong>，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种</strong></em>*“邻域保持”*<em><strong>的思想。其中</strong></em>*等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系****，下面将分别对这两种著名的流行学习方法进行介绍。</p>
<h3 id="11-5-1-等度量映射（Isomap）"><a href="#11-5-1-等度量映射（Isomap）" class="headerlink" title="*11.5.1 等度量映射（Isomap）*"></a><em><strong>*11.5.1 等度量映射（Isomap）*</strong></em></h3><p>等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。****因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离*<em><strong>，即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的</strong></em>*Dijkstra算法*<em><strong>或</strong></em>*Floyd算法****计算最短距离，得到高维空间中任意两点之间的距离后便可以使用MDS算法来其计算低维空间中的坐标。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps481.jpg" alt="img"> </p>
<p>从MDS算法的描述中我们可以知道：MDS先求出了低维空间的内积矩阵B，接着使用特征值分解计算出了样本在低维空间中的坐标，但是并没有给出通用的投影向量w，因此对于需要降维的新样本无从下手，给出的权宜之计是利用已知高&#x2F;低维坐标的样本作为训练集学习出一个“投影器”，便可以用高维坐标预测出低维坐标。Isomap算法流程如下图：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps482.jpg" alt="img"> </p>
<p>对于近邻图的构建，常用的有两种方法：****一种是指定近邻点个数*<em><strong>，像kNN一样选取k个最近的邻居；</strong></em>*另一种是指定邻域半径****，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题：</p>
<p>若****邻域范围指定过大，则会造成“短路问题”*<em><strong>，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。<br>若</strong></em>*邻域范围指定过小，则会造成“断路问题”****，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。</p>
<h3 id="11-5-2-局部线性嵌入-LLE"><a href="#11-5-2-局部线性嵌入-LLE" class="headerlink" title="*11.5.2 局部线性嵌入(LLE)*"></a><em><strong>*11.5.2 局部线性嵌入(LLE)*</strong></em></h3><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps483.jpg" alt="img">不同于Isomap算法去保持邻域距离，LLE算法试图去保持邻域内的线性关系，假定样本xi的坐标可以通过它的邻域样本线性表出：</p>
<p> <img src="https://raw.githubusercontent.com/ebxeax/images/main/wps484.jpg" alt="img"></p>
<p>LLE算法分为两步走，****首先第一步根据近邻关系计算出所有样本的邻域重构系数w****：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps485.jpg" alt="img"> </p>
<p>****接着根据邻域重构系数不变，去求解低维坐标****：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps486.jpg" alt="img"> </p>
<p>这样利用矩阵M，优化问题可以重写为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps487.jpg" alt="img"> </p>
<p>M特征值分解后最小的d’个特征值对应的特征向量组成Z，LLE算法的具体流程如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps488.jpg" alt="img"> </p>
<h2 id="11-6-度量学习"><a href="#11-6-度量学习" class="headerlink" title="*11.6 度量学习*"></a><em><strong>*11.6 度量学习*</strong></em></h2><p>本篇一开始就提到维数灾难，即在高维空间进行机器学习任务遇到样本稀疏、距离难计算等诸多的问题，因此前面讨论的降维方法都试图将原空间投影到一个合适的低维空间中，接着在低维空间进行学习任务从而产生较好的性能。事实上，不管高维空间还是低维空间都潜在对应着一个距离度量，那可不可以直接学习出一个距离度量来等效降维呢？例如：****咋们就按照降维后的方式来进行距离的计算，这便是度量学习的初衷****。</p>
<p>****首先要学习出距离度量必须先定义一个合适的距离度量形式****。对两个样本xi与xj，它们之间的平方欧式距离为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps489.jpg" alt="img"> </p>
<p>若各个属性重要程度不一样即都有一个权重，则得到加权的平方欧式距离：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps490.jpg" alt="img"> </p>
<p>此时各个属性之间都是相互独立无关的，但现实中往往会存在属性之间有关联的情形，例如：身高和体重，一般人越高，体重也会重一些，他们之间存在较大的相关性。这样计算距离就不能分属性单独计算，于是就引入经典的<em><strong>*马氏距离(Mahalanobis distance)*</strong></em>:</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps491.jpg" alt="img"> </p>
<p>****标准的马氏距离中M是协方差矩阵的逆，马氏距离是一种考虑属性之间相关性且尺度无关（即无须去量纲）的距离度量****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps492.jpg" alt="img"> </p>
<p>****矩阵M也称为“度量矩阵”，为保证距离度量的非负性与对称性，M必须为(半)正定对称矩阵*<em><strong>，这样就为度量学习定义好了距离度量的形式，换句话说：</strong></em>*度量学习便是对度量矩阵进行学习*<em><strong>。现在来回想一下前面我们接触的机器学习不难发现：</strong></em>*机器学习算法几乎都是在优化目标函数，从而求解目标函数中的参数****。同样对于度量学习，也需要设置一个优化目标，书中简要介绍了错误率和相似性两种优化目标，此处限于篇幅不进行展开。</p>
<p>在此，降维和度量学习就介绍完毕。<em><strong>*降维是将原高维空间嵌入到一个合适的低维子空间中，接着在低维空间中进行学习任务；度量学习则是试图去学习出一个距离度量来等效降维的效果*<em><strong>，两者都是为了解决维数灾难带来的诸多问题。也许大家最后心存疑惑，那kNN呢，为什么一开头就说了kNN算法，但是好像和后面没有半毛钱关系？正是因为在降维算法中，低维子空间的维数d’通常都由人为指定，因此我们需要使用一些低开销的学习器来选取合适的d’，</strong></em>*kNN这家伙懒到家了根本无心学习，在训练阶段开销为零，测试阶段也只是遍历计算了距离，因此拿kNN来进行交叉验证就十分有优势了~同时降维后样本密度增大同时距离计算变易，更为kNN来展示它独特的十八般手艺提供了用武之地。*</strong></em></p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/72860607"><em><strong>*《机器学习》 学习笔记（13）–特征选择与稀疏学习*</strong></em></a></p>
<p>上篇主要介绍了经典的降维方法与度量学习，首先从“维数灾难”导致的样本稀疏以及距离难计算两大难题出发，引出了降维的概念，即通过某种数学变换将原始高维空间转变到一个低维的子空间，接着分别介绍了kNN、MDS、PCA、KPCA以及两种经典的流形学习方法，k近邻算法的核心在于k值的选取以及距离的度量，MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持，主成分分析试图找到一个低维超平面来表出原空间样本点，核化主成分分析先将样本点映射到高维空间，再在高维空间中使用线性降维的方法，从而解决了原空间样本非线性分布的情形，基于流形学习的降维则是一种“邻域保持”的思想，最后度量学习试图去学习出一个距离度量来等效降维的效果。本篇将讨论另一种常用方法–特征选择与稀疏学习。</p>
<h1 id="12、特征选择与稀疏学习"><a href="#12、特征选择与稀疏学习" class="headerlink" title="*12、特征选择与稀疏学习*"></a><em><strong>*12、特征选择与稀疏学习*</strong></em></h1><p>对于数据集中的一个对象及组成对象的零件元素：</p>
<p>统计学家常称它们为<em><strong>*观测*<em><strong>（</strong></em>*observation*<em><strong>）和</strong></em>*变量*<em><strong>（</strong></em>*variable*<em><strong>）；<br>数据库分析师则称其为</strong></em>*记录*<em><strong>（</strong></em>*record*<em><strong>）和</strong></em>*字段*<em><strong>（</strong></em>*field*<em><strong>）；<br>数据挖掘&#x2F;机器学习学科的研究者则习惯把它们叫做</strong></em>*样本*</strong></em>&#x2F;<em><strong>*示例*<em><strong>（</strong></em>*example*</strong></em>&#x2F;<em><strong>*instance*<em><strong>）和</strong></em>*属性*</strong></em>&#x2F;<em><strong>*特征*<em><strong>（</strong></em>*attribute*</strong></em>&#x2F;****feature****）。</p>
<p>回归正题，在机器学习中特征选择是一个重要的“<em><strong>*数据预处理*<em><strong>”（</strong></em>*data*</strong></em> ****preprocessing****）过程，即试图从数据集的所有特征中挑选出与当前学习任务相关的特征子集，接着再利用数据子集来训练学习器；稀疏学习则是围绕着稀疏矩阵的优良性质，来完成相应的学习任务。</p>
<h2 id="12-1-子集搜索与评价"><a href="#12-1-子集搜索与评价" class="headerlink" title="*12.1 子集搜索与评价*"></a><em><strong>*12.1 子集搜索与评价*</strong></em></h2><p>一般地，我们可以用很多属性&#x2F;特征来描述一个示例，例如对于一个人可以用性别、身高、体重、年龄、学历、专业等属性来描述，那现在想要训练出一个学习器来预测人的收入。根据生活经验易知：并不是所有的特征都与学习任务相关，例如年龄&#x2F;学历&#x2F;专业可能很大程度上影响了收入，身高&#x2F;体重这些外貌属性也有较小的可能性影响收入。因此我们只需要那些与学习任务紧密相关的特征，****特征选择便是从给定的特征集合中选出相关特征子集的过程****。</p>
<p>与上篇中降维技术有着异曲同工之处的是，特征选择也可以有效地解决维数灾难的难题。具体而言：****降维从一定程度起到了提炼优质低维属性和降噪的效果，特征选择则是直接剔除那些与学习任务无关的属性而选择出最佳特征子集*<em><strong>。若直接遍历所有特征子集，显然当维数过多时遭遇指数爆炸就行不通了；若采取从候选特征子集中不断迭代生成更优候选子集的方法，则时间复杂度大大减小。这时就涉及到了两个关键环节：</strong></em>*1.如何生成候选子集；2.如何评价候选子集的好坏****，这便是早期特征选择的常用方法。书本上介绍了贪心算法，分为三种策略：</p>
<p>****前向搜索****：初始将每个特征当做一个候选特征子集，然后从当前所有的候选子集中选择出最佳的特征子集；接着在上一轮选出的特征子集中添加一个新的特征，同样地选出最佳特征子集；最后直至选不出比上一轮更好的特征子集。<br>****后向搜索****：初始将所有特征作为一个候选特征子集；接着尝试去掉上一轮特征子集中的一个特征并选出当前最优的特征子集；最后直到选不出比上一轮更好的特征子集。<br>****双向搜索****：将前向搜索与后向搜索结合起来，即在每一轮中既有添加操作也有剔除操作。</p>
<p>对于特征子集的评价，书中给出了一些想法及基于信息熵的方法。假设数据集的属性皆为离散属性，这样给定一个特征子集，便可以通过这个特征子集的取值将数据集合划分为V个子集。例如：A1&#x3D;{男,女}，A2&#x3D;{本科,硕士}就可以将原数据集划分为2*2&#x3D;4个子集，其中每个子集的取值完全相同。这时我们就可以像决策树选择划分属性那样，通过计算信息增益来评价该属性子集的好坏。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps493.jpg" alt="img"> </p>
<p>此时，信息增益越大表示该属性子集包含有助于分类的特征越多，使用上述这种****子集搜索与子集评价相结合的机制，便可以得到特征选择方法****。值得一提的是若将前向搜索策略与信息增益结合在一起，与前面我们讲到的ID3决策树十分地相似。事实上，决策树也可以用于特征选择，树节点划分属性组成的集合便是选择出的特征子集。</p>
<h2 id="12-2-过滤式选择（Relief）"><a href="#12-2-过滤式选择（Relief）" class="headerlink" title="*12.2 过滤式选择（Relief）*"></a><em><strong>*12.2 过滤式选择（Relief）*</strong></em></h2><p>过滤式方法是一种将特征选择与学习器训练相分离的特征选择技术，即首先将相关特征挑选出来，再使用选择出的数据子集来训练学习器。Relief是其中著名的代表性算法，它使用一个“****相关统计量****”来度量特征的重要性，该统计量是一个向量，其中每个分量代表着相应特征的重要性，因此我们最终可以根据这个统计量各个分量的大小来选择出合适的特征子集。</p>
<p>易知Relief算法的核心在于如何计算出该相关统计量。对于数据集中的每个样例xi，Relief首先找出与xi同类别的最近邻与不同类别的最近邻，分别称为****猜中近邻（near-hit）*<em><strong>与</strong></em>*猜错近邻（near-miss）****，接着便可以分别计算出相关统计量中的每个分量。对于j分量：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps494.jpg" alt="img"> </p>
<p>直观上理解：对于猜中近邻，两者j属性的距离越小越好，对于猜错近邻，j属性距离越大越好。更一般地，若xi为离散属性，diff取海明距离，即相同取0，不同取1；若xi为连续属性，则diff为曼哈顿距离，即取差的绝对值。分别计算每个分量，最终取平均便得到了整个相关统计量。</p>
<p>标准的Relief算法只用于二分类问题，后续产生的拓展变体Relief-F则解决了多分类问题。对于j分量，新的计算公式如下：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps495.jpg" alt="img"> </p>
<p>其中pl表示第l类样本在数据集中所占的比例，易知两者的不同之处在于：****标准Relief 只有一个猜错近邻，而Relief-F有多个猜错近邻****。</p>
<h2 id="12-3-包裹式选择（LVW）"><a href="#12-3-包裹式选择（LVW）" class="headerlink" title="*12.3 包裹式选择（LVW）*"></a><em><strong>*12.3 包裹式选择（LVW）*</strong></em></h2><p>与过滤式选择不同的是，包裹式选择将后续的学习器也考虑进来作为特征选择的评价准则。因此包裹式选择可以看作是为某种学习器****量身定做*<em><strong>的特征选择方法，由于在每一轮迭代中，包裹式选择都需要训练学习器，因此在获得较好性能的同时也产生了较大的开销。下面主要介绍一种经典的包裹式特征选择方法 –</strong>LVW（Las Vegas Wrapper）</em>*，它在拉斯维加斯框架下使用随机策略来进行特征子集的搜索。怀着好奇科普一下，结果又顺带了一个赌场：</p>
<p>****蒙特卡罗算法****：采样越多，越近似最优解，一定会给出解，但给出的解不一定是正确解；<br>****拉斯维加斯算法****：采样越多，越有机会找到最优解，不一定会给出解，且给出的解一定是正确解。</p>
<p>举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找较好的，但不保证是最好的。</p>
<p>而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（正确解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。</p>
<p>LVW算法的具体流程如下所示，其中比较特别的是停止条件参数T的设置，即在每一轮寻找最优特征子集的过程中，若随机T次仍没找到，算法就会停止，从而保证了算法运行时间的可行性。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps496.jpg" alt="img"> </p>
<h2 id="12-4-嵌入式选择与正则化"><a href="#12-4-嵌入式选择与正则化" class="headerlink" title="*12.4 嵌入式选择与正则化*"></a><em><strong>*12.4 嵌入式选择与正则化*</strong></em></h2><p>前面提到了的两种特征选择方法：****过滤式中特征选择与后续学习器完全分离，包裹式则是使用学习器作为特征选择的评价准则；嵌入式是一种将特征选择与学习器训练完全融合的特征选择方法，即将特征选择融入学习器的优化过程中*<em><strong>。在之前《经验风险与结构风险》中已经提到：经验风险指的是模型与训练数据的契合度，结构风险则是模型的复杂程度，机器学习的核心任务就是：</strong></em>*在模型简单的基础上保证模型的契合度****。例如：岭回归就是加上了L2范数的最小二乘法，有效地解决了奇异矩阵、过拟合等诸多问题，下面的嵌入式特征选择则是在损失函数后加上了L1范数。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps497.jpg" alt="img"> </p>
<p>L1范数美名又约****Lasso Regularization****，指的是向量中每个元素的绝对值之和，这样在优化目标函数的过程中，就会使得w尽可能地小，在一定程度上起到了防止过拟合的作用，同时与L2范数（Ridge Regularization ）不同的是，L1范数会使得部分w变为0， 从而达到了特征选择的效果。</p>
<p>总的来说：****L1范数会趋向产生少量的特征，其他特征的权值都是0；L2会选择更多的特征，这些特征的权值都会接近于0****。这样L1范数在特征选择上就十分有用，而L2范数则具备较强的控制过拟合能力。可以从下面两个方面来理解：</p>
<p>（1）****下降速度****：L1范数按照绝对值函数来下降，L2范数按照二次函数来下降。因此在0附近，L1范数的下降速度大于L2范数，故L1范数能很快地下降到0，而L2范数在0附近的下降速度非常慢，因此较大可能收敛在0的附近。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps498.jpg" alt="img"> </p>
<p>（2）****空间限制****：L1范数与L2范数都试图在最小化损失函数的同时，让权值W也尽可能地小。我们可以将原优化问题看做为下面的问题，即让后面的规则则都小于某个阈值。这样从图中可以看出：L1范数相比L2范数更容易得到稀疏解。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps499.jpg" alt="img"> </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps500.jpg" alt="img"></p>
<h2 id="12-5-稀疏表示与字典学习"><a href="#12-5-稀疏表示与字典学习" class="headerlink" title="*12.5 稀疏表示与字典学习*"></a><em><strong>*12.5 稀疏表示与字典学习*</strong></em></h2><p>当样本数据是一个稀疏矩阵时，对学习任务来说会有不少的好处，例如很多问题变得线性可分，储存更为高效等。这便是稀疏表示与字典学习的基本出发点。稀疏矩阵即矩阵的每一行&#x2F;列中都包含了大量的零元素，且这些零元素没有出现在同一行&#x2F;列，对于一个给定的稠密矩阵，若我们能****通过某种方法找到其合适的稀疏表示*<em><strong>，则可以使得学习任务更加简单高效，我们称之为</strong></em>*稀疏编码（sparse coding）*<em><strong>或</strong></em>*字典学习（dictionary learning）****。</p>
<p>给定一个数据集，字典学习&#x2F;稀疏编码指的便是通过一个字典将原数据转化为稀疏表示，因此最终的目标就是求得字典矩阵B及稀疏表示α，书中使用变量交替优化的策略能较好地求得解，深感陷进去短时间无法自拔，故先不进行深入…</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps501.jpg" alt="img"> </p>
<h2 id="12-6-压缩感知"><a href="#12-6-压缩感知" class="headerlink" title="*12.6 压缩感知*"></a><em><strong>*12.6 压缩感知*</strong></em></h2><p>压缩感知与特征选择、稀疏表示不同的是：它关注的是通过欠采样信息来恢复全部信息。在实际问题中，为了方便传输和存储，我们一般将数字信息进行压缩，这样就有可能损失部分信息，如何根据已有的信息来重构出全部信号，这便是压缩感知的来历，压缩感知的前提是已知的信息具有稀疏表示。下面是关于压缩感知的一些背景：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps502.jpg" alt="img"> </p>
<p>在很多实际情形中，选了好的特征比选了好的模型更为重要.</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/73351162"><em><strong>*《机器学习》 学习笔记（14）–计算学习理论*</strong></em></a></p>
<p>上篇主要介绍了常用的特征选择方法及稀疏学习。首先从相关&#x2F;无关特征出发引出了特征选择的基本概念，接着分别介绍了子集搜索与评价、过滤式、包裹式以及嵌入式四种类型的特征选择方法。子集搜索与评价使用的是一种优中生优的贪婪算法，即每次从候选特征子集中选出最优子集；过滤式方法计算一个相关统计量来评判特征的重要程度；包裹式方法将学习器作为特征选择的评价准则；嵌入式方法则是通过L1正则项将特征选择融入到学习器参数优化的过程中。最后介绍了稀疏表示与压缩感知的核心思想：稀疏表示利用稀疏矩阵的优良性质，试图通过某种方法找到原始稠密矩阵的合适稀疏表示；压缩感知则试图利用可稀疏表示的欠采样信息来恢复全部信息。本篇将讨论一种为机器学习提供理论保证的学习方法–计算学习理论。</p>
<h1 id="13、计算学习理论"><a href="#13、计算学习理论" class="headerlink" title="*13、计算学习理论*"></a><em><strong>*13、计算学习理论*</strong></em></h1><p>计算学习理论（computational learning theory）是通过“计算”来研究机器学习的理论，简而言之，其目的是分析学习任务的本质，例如：****在什么条件下可进行有效的学习，需要多少训练样本能获得较好的精度等，从而为机器学习算法提供理论保证****。</p>
<p>首先来谈谈经验误差和泛化误差。假设给定训练集D，其中所有的训练样本都服从一个未知的分布T，且它们都是在总体分布T中独立采样得到，即****独立同分布****（independent and identically distributed，i.i.d.），在《贝叶斯分类器》中我们已经提到：独立同分布是很多统计学习算法的基础假设，例如最大似然法，贝叶斯分类器，高斯混合聚类等，简单来理解独立同分布：每个样本都是从总体分布中独立采样得到水。</p>
<p>****泛化误差指的是学习器在总体上的预测误差，经验误差则是学习器在某个特定数据集D上的预测误差****。在实际问题中，往往我们并不能得到总体且数据集D是通过独立同分布采样得到的，因此我们常常使用经验误差作为泛化误差的近似。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps503.jpg" alt="img"> </p>
<h2 id="13-1-PAC学习"><a href="#13-1-PAC学习" class="headerlink" title="*13.1 PAC学习*"></a><em><strong>*13.1 PAC学习*</strong></em></h2><p>****对于机器学习算法，学习器也正是为了寻找合适的映射规则*<em><strong>，即如何从条件属性得到目标属性。从样本空间到标记空间存在着很多的映射，我们将每个映射称之为</strong></em>*概念****（concept），定义：</p>
<p>若概念c对任何样本x满足c(x)&#x3D;y，则称c为****目标概念*<em><strong>，即最理想的映射，所有的目标概念构成的集合称为</strong></em>*“概念类”*<em><strong>；<br>给定学习算法，它所有可能映射&#x2F;概念的集合称为</strong></em>*“假设空间”*<em><strong>，其中单个的概念称为</strong></em>*“假设”*<em><strong>（hypothesis）；<br>若一个算法的假设空间包含目标概念，则称该数据集对该算法是</strong></em>*可分*<em><strong>（separable）的，亦称</strong></em>*一致*<em><strong>（consistent）的；<br>若一个算法的假设空间不包含目标概念，则称该数据集对该算法是</strong></em>*不可分*<em><strong>（non-separable）的，或称</strong></em>*不一致****（non-consistent）的。</p>
<p>举个简单的例子：对于非线性分布的数据集，若使用一个线性分类器，则该线性分类器对应的假设空间就是空间中所有可能的超平面，显然假设空间不包含该数据集的目标概念，所以称数据集对该学习器是不可分的。给定一个数据集D，我们希望模型学得的假设h尽可能地与目标概念一致，这便是<em><strong>*概率近似正确*</strong></em>(<strong>Probably Approximately Correct</strong>，简称PAC)的来源，即以较大的概率学得模型满足误差的预设上限。 </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps504.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps505.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps506.jpg" alt="img"> </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps507.jpg" alt="img"></p>
<p>上述关于PAC的几个定义层层相扣：定义12.1表达的是对于某种学习算法，如果能以一个置信度学得假设满足泛化误差的预设上限，则称该算法能PAC辨识概念类，即该算法的输出假设已经十分地逼近目标概念。定义12.2则将样本数量考虑进来，当样本超过一定数量时，学习算法总是能PAC辨识概念类，则称概念类为PAC可学习的。定义12.3将学习器运行时间也考虑进来，若运行时间为多项式时间，则称PAC学习算法。</p>
<p>显然，PAC学习中的一个关键因素就是****假设空间的复杂度*<em><strong>，对于某个学习算法，</strong></em>*若假设空间越大，则其中包含目标概念的可能性也越大，但同时找到某个具体概念的难度也越大****，一般假设空间分为有限假设空间与无限假设空间。</p>
<h2 id="13-2-有限假设空间"><a href="#13-2-有限假设空间" class="headerlink" title="*13.2 有限假设空间*"></a><em><strong>*13.2 有限假设空间*</strong></em></h2><h3 id="13-2-1-可分情形"><a href="#13-2-1-可分情形" class="headerlink" title="*13.2.1 可分情形*"></a><em><strong>*13.2.1 可分情形*</strong></em></h3><p>可分或一致的情形指的是：****目标概念包含在算法的假设空间中*<em><strong>。对于目标概念，在训练集D中的经验误差一定为0，因此首先我们可以想到的是：不断地剔除那些出现预测错误的假设，直到找到经验误差为0的假设即为目标概念。但</strong></em>*由于样本集有限，可能会出现多个假设在D上的经验误差都为0，因此问题转化为：需要多大规模的数据集D才能让学习算法以置信度的概率从这些经验误差都为0的假设中找到目标概念的有效近似****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps508.jpg" alt="img"> </p>
<p>通过上式可以得知：<em><strong>*对于可分情形的有限假设空间，目标概念都是PAC可学习的，即当样本数量满足上述条件之后，在与训练集一致的假设中总是可以在1-σ概率下找到目标概念的有效近似。*</strong></em></p>
<h3 id="13-2-2-不可分情形"><a href="#13-2-2-不可分情形" class="headerlink" title="*13.2.2 不可分情形*"></a><em><strong>*13.2.2 不可分情形*</strong></em></h3><p>不可分或不一致的情形指的是：<em><strong>*目标概念不存在于假设空间中*<em><strong>，这时我们就不能像可分情形时那样从假设空间中寻找目标概念的近似。但</strong></em>*当假设空间给定时，必然存一个假设的泛化误差最小，若能找出此假设的有效近似也不失为一个好的目标，这便是不可知学习(agnostic learning)的来源。*</strong></em></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps509.jpg" alt="img"> </p>
<p>这时候便要用到****Hoeffding不等式****：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps510.jpg" alt="img"> </p>
<p>对于假设空间中的所有假设，出现泛化误差与经验误差之差大于e的概率和为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps511.jpg" alt="img"> </p>
<p>因此，可令不等式的右边小于（等于）σ，便可以求出满足泛化误差与经验误差相差小于e所需的最少样本数，同时也可以求出泛化误差界。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps512.jpg" alt="img"> </p>
<h2 id="13-3-VC维"><a href="#13-3-VC维" class="headerlink" title="*13.3 VC维*"></a><em><strong>*13.3 VC维*</strong></em></h2><p>现实中的学习任务通常都是无限假设空间，例如d维实数域空间中所有的超平面等，因此要对此种情形进行可学习研究，需要度量****假设空间的复杂度*<em><strong>。这便是</strong></em>*VC维****（Vapnik-Chervonenkis dimension）的来源。在介绍VC维之前，需要引入两个概念：</p>
<p><em><strong>*增长函数*<em><strong>：对于给定数据集D，假设空间中的每个假设都能对数据集的样本赋予标记，因此一个假设对应着一种打标结果，不同假设对D的打标结果可能是相同的，也可能是不同的。随着样本数量m的增大，假设空间对样本集D的打标结果也会增多，增长函数则表示假设空间对m个样本的数据集D打标的最大可能结果数，因此</strong></em>*增长函数描述了假设空间的表示能力与复杂度。*</strong></em><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps513.jpg" alt="img"></p>
<p>****打散*<em><strong>：例如对二分类问题来说，m个样本最多有2^m个可能结果，每种可能结果称为一种</strong></em>*“对分”****，若假设空间能实现数据集D的所有对分，则称数据集能被该假设空间打散。</p>
<p>****因此尽管假设空间是无限的，但它对特定数据集打标的不同结果数是有限的，假设空间的VC维正是它能打散的最大数据集大小****。通常这样来计算假设空间的VC维：若存在大小为d的数据集能被假设空间打散，但不存在任何大小为d+1的数据集能被假设空间打散，则其VC维为d。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps514.jpg" alt="img"> </p>
<p>同时给出了假设空间VC维与增长函数的两个关系：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps515.jpg" alt="img"> </p>
<p>直观来理解（1）式也十分容易： 首先假设空间的VC维是d，说明当m&lt;&#x3D;d时，增长函数与2^m相等，例如：当m&#x3D;d时，右边的组合数求和刚好等于2^d；而当m&#x3D;d+1时，右边等于2^(d+1)-1，十分符合VC维的定义，同时也可以使用数学归纳法证明；（2）式则是由（1）式直接推导得出。</p>
<p>在有限假设空间中，根据Hoeffding不等式便可以推导得出学习算法的泛化误差界；但在无限假设空间中，由于假设空间的大小无法计算，只能通过增长函数来描述其复杂度，因此无限假设空间中的泛化误差界需要引入增长函数。<br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps516.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps517.jpg" alt="img"></p>
<p>上式给出了基于VC维的泛化误差界，同时也可以计算出满足条件需要的样本数（样本复杂度）。若学习算法满足<em><strong>*经验风险最小化原则（ERM）*<em><strong>，即学习算法的输出假设h在数据集D上的经验误差最小，可证明：</strong></em>*任何VC维有限的假设空间都是（不可知）PAC可学习的，换而言之：若假设空间的最小泛化误差为0即目标概念包含在假设空间中，则是PAC可学习，若最小泛化误差不为0，则称为不可知PAC可学习。*</strong></em></p>
<h2 id="13-4-稳定性"><a href="#13-4-稳定性" class="headerlink" title="*13.4 稳定性*"></a><em><strong>*13.4 稳定性*</strong></em></h2><p>稳定性考察的是当算法的输入发生变化时，输出是否会随之发生较大的变化，输入的数据集D有以下两种变化：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps518.jpg" alt="img"> </p>
<p>若对数据集中的任何样本z，满足：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps519.jpg" alt="img"> </p>
<p>即原学习器和剔除一个样本后生成的学习器对z的损失之差保持β稳定，称学习器关于损失函数满足****β-均匀稳定性****。同时若损失函数有上界，即原学习器对任何样本的损失函数不超过M，则有如下定理：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps520.jpg" alt="img"> </p>
<p>事实上，****若学习算法符合经验风险最小化原则（ERM）且满足β-均匀稳定性，则假设空间是可学习的****。稳定性通过损失函数与假设空间的可学习联系在了一起，区别在于：假设空间关注的是经验误差与泛化误差，需要考虑到所有可能的假设；而稳定性只关注当前的输出假设。</p>
<p> <a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/74358913"><strong>《机器学习》 学习笔记（15）–半监督学习</strong></a></p>
<p>上篇主要介绍了机器学习的理论基础，首先从独立同分布引入泛化误差与经验误差，接着介绍了PAC可学习的基本概念，即以较大的概率学习出与目标概念近似的假设（泛化误差满足预设上限），对于有限假设空间：（1）可分情形时，假设空间都是PAC可学习的，即当样本满足一定的数量之后，总是可以在与训练集一致的假设中找出目标概念的近似；（2）不可分情形时，假设空间都是不可知PAC可学习的，即以较大概率学习出与当前假设空间中泛化误差最小的假设的有效近似（Hoeffding不等式）。对于无限假设空间，通过增长函数与VC维来描述其复杂度，若学习算法满足经验风险最小化原则，则任何VC维有限的假设空间都是（不可知）PAC可学习的，同时也给出了泛化误差界与样本复杂度。稳定性则考察的是输入发生变化时输出的波动，稳定性通过损失函数与假设空间的可学习理论联系在了一起。本篇将讨论一种介于监督与非监督学习之间的学习算法–半监督学习。</p>
<h1 id="14、半监督学习"><a href="#14、半监督学习" class="headerlink" title="*14、半监督学习*"></a><em><strong>*14、半监督学习*</strong></em></h1><p>前面我们一直围绕的都是监督学习与无监督学习，监督学习指的是训练样本包含标记信息的学习任务，例如：常见的分类与回归算法；无监督学习则是训练样本不包含标记信息的学习任务，例如：聚类算法。在实际生活中，常常会出现一部分样本有标记和较多样本无标记的情形，例如：做网页推荐时需要让用户标记出感兴趣的网页，但是少有用户愿意花时间来提供标记。若直接丢弃掉无标记样本集，使用传统的监督学习方法，常常会由于训练样本的不充足，使得其刻画总体分布的能力减弱，从而影响了学习器泛化性能。那如何利用未标记的样本数据呢？</p>
<p>一种简单的做法是通过专家知识对这些未标记的样本进行打标，但随之而来的就是巨大的人力耗费。若我们先使用有标记的样本数据集训练出一个学习器，再基于该学习器对未标记的样本进行预测，从中****挑选出不确定性高或分类置信度低的样本来咨询专家并进行打标*<em><strong>，最后使用扩充后的训练集重新训练学习器，这样便能大幅度降低标记成本，这便是</strong></em>*主动学习*<em><strong>（active learning），其目标是</strong></em>*使用尽量少的&#x2F;有价值的咨询来获得更好的性能****。</p>
<p>显然，****主动学习需要与外界进行交互&#x2F;查询&#x2F;打标，其本质上仍然属于一种监督学习*<em><strong>。事实上，无标记样本虽未包含标记信息，但它们与有标记样本一样都是从总体中独立同分布采样得到，因此</strong></em>*它们所包含的数据分布信息对学习器的训练大有裨益*<em><strong>。如何让学习过程不依赖外界的咨询交互，自动利用未标记样本所包含的分布信息的方法便是</strong></em>*半监督学习*<em><strong>（semi-supervised learning），</strong></em>*即训练集同时包含有标记样本数据和未标记样本数据****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps521.jpg" alt="img"> </p>
<p>此外，半监督学习还可以进一步划分为****纯半监督学习*<em><strong>和</strong></em>*直推学习****，两者的区别在于：前者假定训练数据集中的未标记数据并非待预测数据，而后者假定学习过程中的未标记数据就是待预测数据。主动学习、纯半监督学习以及直推学习三者的概念如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps522.jpg" alt="img"> </p>
<h2 id="14-1-生成式方法"><a href="#14-1-生成式方法" class="headerlink" title="*14.1 生成式方法*"></a><em><strong>*14.1 生成式方法*</strong></em></h2><p>****生成式方法*<em><strong>（generative methods）是基于生成式模型的方法，即先对联合分布P（x,c）建模，从而进一步求解 P（c | x），</strong></em>*此类方法假定样本数据服从一个潜在的分布，因此需要充分可靠的先验知识****。例如：前面已经接触到的贝叶斯分类器与高斯混合聚类，都属于生成式模型。现假定总体是一个高斯混合分布，即由多个高斯分布组合形成，从而一个子高斯分布就代表一个类簇（类别）。高斯混合分布的概率密度函数如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps523.jpg" alt="img"> </p>
<p>不失一般性，假设类簇与真实的类别按照顺序一一对应，即第i个类簇对应第i个高斯混合成分。与高斯混合聚类类似地，这里的主要任务也是估计出各个高斯混合成分的参数以及混合系数，不同的是：对于有标记样本，不再是可能属于每一个类簇，而是只能属于真实类标对应的特定类簇。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps524.jpg" alt="img"> </p>
<p>直观上来看，****基于半监督的高斯混合模型有机地整合了贝叶斯分类器与高斯混合聚类的核心思想****，有效地利用了未标记样本数据隐含的分布信息，从而使得参数的估计更加准确。同样地， 用EM进行求解，首先对各个高斯混合成分的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类，有标记样本则直接属于特定类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps525.jpg" alt="img"> </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps526.jpg" alt="img">当参数迭代更新收敛后，对于待预测样本x，便可以像贝叶斯分类器那样计算出样本属于每个类簇的后验概率，接着找出概率最大的即可：</p>
<p>可以看出：基于生成式模型的方法十分依赖于对潜在数据分布的假设，即假设的分布要能和真实分布相吻合，否则利用未标记的样本数据反倒会在错误的道路上渐行渐远，从而降低学习器的泛化性能。</p>
<h2 id="14-2-半监督SVM"><a href="#14-2-半监督SVM" class="headerlink" title="*14.2 半监督SVM*"></a><em><strong>*14.2 半监督SVM*</strong></em></h2><p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps527.jpg" alt="img">监督学习中的SVM（Semi-Supervised Support Vector Machine）试图找到一个划分超平面，使得两侧支持向量之间的间隔最大，即“****最大划分间隔****”思想。对于半监督学习，S3VM则考虑超平面需穿过数据低密度的区域。TSVM是半监督支持向量机中的最著名代表，其核心思想是：尝试为未标记样本找到合适的标记指派，使得超平面划分后的间隔最大化。TSVM(Transductive SVM)采用局部搜索的策略来进行迭代求解，即首先使用有标记样本集训练出一个初始SVM，接着使用该学习器对未标记样本进行打标，这样所有样本都有了标记，并基于这些有标记的样本重新训练SVM，之后再寻找易出错样本不断调整。整个算法流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps528.jpg" alt="img"> </p>
<p>缺图半监督学习</p>
<h2 id="14-3-基于分歧的方法"><a href="#14-3-基于分歧的方法" class="headerlink" title="*14.3 基于分歧的方法*"></a><em><strong>*14.3 基于分歧的方法*</strong></em></h2><p>基于分歧的方法通过多个学习器之间的<em><strong>*分歧（disagreement）&#x2F;多样性（diversity）*<em><strong>来利用未标记样本数据，协同训练就是其中的一种经典方法。</strong></em>*协同训练最初是针对于多视图（*</strong></em><em><strong>*multi-view*</strong></em>****）数据而设计的，多视图数据指的是样本对象具有多个属性集，每个属性集则对应一个试图****。例如：电影数据中就包含画面类属性和声音类属性，这样画面类属性的集合就对应着一个视图。首先引入两个关于视图的重要性质：</p>
<p>****相容性****：即使用单个视图数据训练出的学习器的输出空间是一致的。例如都是{好，坏}、{+1,-1}等。<br>****互补性****：即不同视图所提供的信息是互补&#x2F;相辅相成的，实质上这里体现的就是集成学习的思想。</p>
<p>协同训练正是很好地利用了多视图数据的“****相容互补性*<em><strong>”，其基本的思想是：首先基于有标记样本数据在每个视图上都训练一个初始分类器，然后让每个分类器去挑选分类置信度最高的样本并赋予标记，并将带有伪标记的样本数据传给另一个分类器去学习，从而</strong></em>*共同进步****。<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps529.jpg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps530.jpg" alt="img"></p>
<h2 id="14-4-半监督聚类"><a href="#14-4-半监督聚类" class="headerlink" title="*14.4 半监督聚类*"></a><em><strong>*14.4 半监督聚类*</strong></em></h2><p>前面提到的几种方法都是借助无标记样本数据来辅助监督学习的训练过程，从而使得学习更加充分&#x2F;泛化性能得到提升；半监督聚类则是借助已有的监督信息来辅助聚类的过程。一般而言，监督信息大致有两种类型：</p>
<p>****必连与勿连约束****：必连指的是两个样本必须在同一个类簇，勿连则是必不在同一个类簇。<br>****标记信息****：少量的样本带有真实的标记。</p>
<p>下面主要介绍两种基于半监督的K-Means聚类算法：第一种是数据集包含一些必连与勿连关系，另外一种则是包含少量带有标记的样本。两种算法的基本思想都十分的简单：对于带有约束关系的k-均值算法，在迭代过程中对每个样本划分类簇时，需要****检测当前划分是否满足约束关系****，若不满足则会将该样本划分到距离次小对应的类簇中，再继续检测是否满足约束关系，直到完成所有样本的划分。算法流程如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps531.jpg" alt="img"> </p>
<p>对于带有少量标记样本的k-均值算法，则可以****利用这些有标记样本进行类中心的指定，同时在对样本进行划分时，不需要改变这些有标记样本的簇隶属关系****，直接将其划分到对应类簇即可。算法流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps532.jpg" alt="img"> </p>
<p>半监督学习将前面许多知识模块联系在了一起，足以体现了作者编排的用心。</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/75090562"><em><strong>*《机器学习》 学习笔记（16）–概率图模型*</strong></em></a></p>
<p>上篇主要介绍了半监督学习，首先从如何利用未标记样本所蕴含的分布信息出发，引入了半监督学习的基本概念，即训练数据同时包含有标记样本和未标记样本的学习方法；接着分别介绍了几种常见的半监督学习方法：生成式方法基于对数据分布的假设，利用未标记样本隐含的分布信息，使得对模型参数的估计更加准确；TSVM给未标记样本赋予伪标记，并通过不断调整易出错样本的标记得到最终输出；基于分歧的方法结合了集成学习的思想，通过多个学习器在不同视图上的协作，有效利用了未标记样本数据 ；最后半监督聚类则是借助已有的监督信息来辅助聚类的过程，带约束k-均值算法需检测当前样本划分是否满足约束关系，带标记k-均值算法则利用有标记样本指定初始类中心。本篇将讨论一种基于图的学习算法–概率图模型。</p>
<h1 id="15、概率图模型"><a href="#15、概率图模型" class="headerlink" title="*15、概率图模型*"></a><em><strong>*15、概率图模型*</strong></em></h1><p>现在来谈谈机器学习的核心价值观，可以通俗地理解为：****根据一些已观察到的证据来推断未知*<em><strong>。其中</strong></em>*基于概率的模型将学习任务归结为计算变量的概率分布****，正如之前已经提到的：生成式模型先对联合分布进行建模，从而再来求解后验概率，例如：贝叶斯分类器先对联合分布进行最大似然估计，从而便可以计算类条件概率；判别式模型则是直接对条件分布进行建模。</p>
<p>****概率图模型*<em><strong>（probabilistic graphical model）是一类用</strong></em>*图结构*<em><strong>来表达各属性之间相关关系的概率模型，一般而言：</strong></em>*图中的一个结点表示一个或一组随机变量，结点之间的边则表示变量间的相关关系*<em><strong>，从而形成了一张“</strong></em>*变量关系图*<em><strong>”。若使用有向的边来表达变量之间的依赖关系，这样的有向关系图称为</strong></em>*贝叶斯网*<em><strong>（Bayesian nerwork）或有向图模型；若使用无向边，则称为</strong></em>*马尔可夫网****（Markov network）或无向图模型。</p>
<h2 id="15-1-隐马尔可夫模型-HMM"><a href="#15-1-隐马尔可夫模型-HMM" class="headerlink" title="*15.1 隐马尔可夫模型(HMM)*"></a><em><strong>*15.1 隐马尔可夫模型(HMM)*</strong></em></h2><p>隐马尔可夫模型（Hidden Markov Model，简称HMM）是结构最简单的一种贝叶斯网，在语音识别与自然语言处理领域上有着广泛的应用。HMM中的变量分为两组：****状态变量*<em><strong>与</strong></em>*观测变量*<em><strong>，其中状态变量一般是未知的，因此又称为“</strong></em>*隐变量****”，观测变量则是已知的输出值。在隐马尔可夫模型中，变量之间的依赖关系遵循如下两个规则：</p>
<p>****1. 观测变量的取值仅依赖于状态变量****；<br>****2. 下一个状态的取值仅依赖于当前状态*<em><strong>，通俗来讲：</strong></em>*现在决定未来，未来与过去无关*<em><strong>，这就是著名的</strong></em>*马尔可夫性****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps533.jpg" alt="img"> </p>
<p>基于上述变量之间的依赖关系，我们很容易写出隐马尔可夫模型中所有变量的联合概率分布：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps534.jpg" alt="img"> </p>
<p>易知：****欲确定一个HMM模型需要以下三组参数****：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps535.jpg" alt="img"> </p>
<p>当确定了一个HMM模型的三个参数后，便按照下面的规则来生成观测值序列：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps536.jpg" alt="img"> </p>
<p>在实际应用中，HMM模型的发力点主要体现在下述三个问题上：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps537.jpg" alt="img"> </p>
<h3 id="15-1-1-HMM评估问题"><a href="#15-1-1-HMM评估问题" class="headerlink" title="*15.1.1 HMM评估问题*"></a><em><strong>*15.1.1 HMM评估问题*</strong></em></h3><p>HMM评估问题指的是：****给定了模型的三个参数与观测值序列，求该观测值序列出现的概率*<em><strong>。例如：对于赌场问题，便可以依据骰子掷出的结果序列来计算该结果序列出现的可能性，若小概率的事件发生了则可认为赌场的骰子有作弊的可能。解决该问题使用的是</strong></em>*前向算法*<em><strong>，即步步为营，自底向上的方式逐步增加序列的长度，直到获得目标概率值。在前向算法中，定义了一个</strong></em>*前向变量****，即给定观察值序列且t时刻的状态为Si的概率：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps538.jpg" alt="img"> </p>
<p>基于前向变量，很容易得到该问题的递推关系及终止条件：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps539.jpg" alt="img"> </p>
<p>因此可使用动态规划法，从最小的子问题开始，通过填表格的形式一步一步计算出目标结果。</p>
<h3 id="15-1-2-HMM解码问题"><a href="#15-1-2-HMM解码问题" class="headerlink" title="*15.1.2 HMM解码问题*"></a><em><strong>*15.1.2 HMM解码问题*</strong></em></h3><p>HMM解码问题指的是：****给定了模型的三个参数与观测值序列，求可能性最大的状态序列*<em><strong>。例如：在语音识别问题中，人说话形成的数字信号对应着观测值序列，对应的具体文字则是状态序列，从数字信号转化为文字正是对应着根据观测值序列推断最有可能的状态值序列。解决该问题使用的是</strong></em>*Viterbi算法*<em><strong>，与前向算法十分类似地，Viterbi算法定义了一个</strong></em>*Viterbi变量****，也是采用动态规划的方法，自底向上逐步求解。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps540.jpg" alt="img"> </p>
<h3 id="15-1-3-HMM学习问题"><a href="#15-1-3-HMM学习问题" class="headerlink" title="*15.1.3 HMM学习问题*"></a><em><strong>*15.1.3 HMM学习问题*</strong></em></h3><p>HMM学习问题指的是：****给定观测值序列，如何调整模型的参数使得该序列出现的概率最大*<em><strong>。这便转化成了机器学习问题，即从给定的观测值序列中学习出一个HMM模型，</strong></em>*该问题正是EM算法的经典案例之一****。其思想也十分简单：对于给定的观测值序列，如果我们能够按照该序列潜在的规律来调整模型的三个参数，则可以使得该序列出现的可能性最大。假设状态值序列也已知，则很容易计算出与该序列最契合的模型参数：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps541.jpg" alt="img"> </p>
<p>但一般状态值序列都是不可观测的，且****即使给定观测值序列与模型参数，状态序列仍然遭遇组合爆炸****。因此上面这种简单的统计方法就行不通了，若将状态值序列看作为隐变量，这时便可以考虑使用EM算法来对该问题进行求解：</p>
<p>【1】首先对HMM模型的三个参数进行随机初始化；<br>【2】根据模型的参数与观测值序列，计算t时刻状态为i且t+1时刻状态为j的概率以及t时刻状态为i的概率。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps542.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps543.jpg" alt="img"></p>
<p>【3】接着便可以对模型的三个参数进行重新估计：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps544.jpg" alt="img"> </p>
<p>【4】重复步骤2-3，直至三个参数值收敛，便得到了最终的HMM模型。</p>
<h2 id="15-2-马尔可夫随机场（MRF）"><a href="#15-2-马尔可夫随机场（MRF）" class="headerlink" title="*15.2 马尔可夫随机场（MRF）*"></a><em><strong>*15.2 马尔可夫随机场（MRF）*</strong></em></h2><p>马尔可夫随机场（Markov Random Field）是一种典型的马尔可夫网，即使用无向边来表达变量间的依赖关系。在马尔可夫随机场中，对于关系图中的一个子集，****若任意两结点间都有边连接，则称该子集为一个团；若再加一个结点便不能形成团，则称该子集为极大团*<em><strong>。MRF使用</strong></em>*势函数*<em><strong>来定义多个变量的概率分布函数，其中</strong></em>*每个（极大）团对应一个势函数****，一般团中的变量关系也体现在它所对应的极大团中，因此常常基于极大团来定义变量的联合概率分布函数。具体而言，若所有变量构成的极大团的集合为C，则MRF的联合概率函数可以定义为：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps545.jpg" alt="img"> </p>
<p>对于条件独立性，****马尔可夫随机场通过分离集来实现条件独立****，若A结点集必须经过C结点集才能到达B结点集，则称C为分离集。书上给出了一个简单情形下的条件独立证明过程，十分贴切易懂，此处不再展开。基于分离集的概念，得到了MRF的三个性质：</p>
<p>****全局马尔可夫性****：给定两个变量子集的分离集，则这两个变量子集条件独立。<br>****局部马尔可夫性****：给定某变量的邻接变量，则该变量与其它变量条件独立。<br>****成对马尔可夫性****：给定所有其他变量，两个非邻接变量条件独立。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps546.jpg" alt="img"> </p>
<p>对于MRF中的势函数，势函数主要用于描述团中变量之间的相关关系，且要求为非负函数，直观来看：势函数需要在偏好的变量取值上函数值较大，例如：若x1与x2成正相关，则需要将这种关系反映在势函数的函数值中。一般我们常使用指数函数来定义势函数：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps547.jpg" alt="img"> </p>
<h2 id="15-3-条件随机场（CRF）"><a href="#15-3-条件随机场（CRF）" class="headerlink" title="*15.3 条件随机场（CRF）*"></a><em><strong>*15.3 条件随机场（CRF）*</strong></em></h2><p>前面所讲到的****隐马尔可夫模型和马尔可夫随机场都属于生成式模型，即对联合概率进行建模，条件随机场则是对条件分布进行建模*<em><strong>。CRF（Conditional Random Field）试图在给定观测值序列后，对状态序列的概率分布进行建模，即P(y | x)。直观上看：CRF与HMM的解码问题十分类似，都是在给定观测值序列后，研究状态序列可能的取值。CRF可以有多种结构，只需保证状态序列满足马尔可夫性即可，一般我们常使用的是</strong></em>*链式条件随机场****：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps548.jpg" alt="img"> </p>
<p>与马尔可夫随机场定义联合概率类似地，CRF也通过团以及势函数的概念来定义条件概率P(y | x)。在给定观测值序列的条件下，链式条件随机场主要包含两种团结构：单个状态团及相邻状态团，通过引入两类特征函数便可以定义出目标条件概率：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps549.jpg" alt="img"> </p>
<p>以词性标注为例，如何判断给出的一个标注序列靠谱不靠谱呢？****转移特征函数主要判定两个相邻的标注是否合理*<em><strong>，例如：动词+动词显然语法不通；</strong></em>*状态特征函数则判定观测值与对应的标注是否合理*<em><strong>，例如： ly结尾的词–&gt;副词较合理。因此我们可以定义一个特征函数集合，用这个特征函数集合来为一个标注序列打分，并据此选出最靠谱的标注序列。也就是说，每一个特征函数（对应一种规则）都可以用来为一个标注序列评分，把集合中所有特征函数对同一个标注序列的评分综合起来，就是这个标注序列最终的评分值。可以看出：</strong></em>*特征函数是一些经验的特性****。</p>
<h2 id="15-4-学习与推断"><a href="#15-4-学习与推断" class="headerlink" title="*15.4 学习与推断*"></a><em><strong>*15.4 学习与推断*</strong></em></h2><p>对于生成式模型，通常我们都是先对变量的联合概率分布进行建模，接着再求出目标变量的****边际分布*<em><strong>（marginal distribution），那如何从联合概率得到边际分布呢？这便是学习与推断。下面主要介绍两种精确推断的方法：</strong></em>*变量消去*<em><strong>与</strong></em>*信念传播****。</p>
<h3 id="15-4-1-变量消去"><a href="#15-4-1-变量消去" class="headerlink" title="*15.4.1 变量消去*"></a><em><strong>*15.4.1 变量消去*</strong></em></h3><p>变量消去利用条件独立性来消减计算目标概率值所需的计算量，它通过运用****乘法与加法的分配率*<em><strong>，将对变量的积的求和问题转化为对部分变量交替进行求积与求和的问题，从而将每次的</strong></em>*运算控制在局部****，达到简化运算的目的。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps550.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps551.jpg" alt="img"></p>
<h3 id="15-4-2-信念传播"><a href="#15-4-2-信念传播" class="headerlink" title="*15.4.2 信念传播*"></a><em><strong>*15.4.2 信念传播*</strong></em></h3><p>若将变量求和操作看作是一种消息的传递过程，信念传播可以理解成：****一个节点在接收到所有其它节点的消息后才向另一个节点发送消息****，同时当前节点的边际概率正比于他所接收的消息的乘积：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps552.jpg" alt="img"> </p>
<p>因此只需要经过下面两个步骤，便可以完成所有的消息传递过程。利用动态规划法的思想记录传递过程中的所有消息，当计算某个结点的边际概率分布时，只需直接取出传到该结点的消息即可，从而避免了计算多个边际分布时的冗余计算问题。</p>
<p>1.指定一个根节点，从所有的叶节点开始向根节点传递消息，直到根节点收到所有邻接结点的消息****（从叶到根）*<em><strong>；<br>2.从根节点开始向叶节点传递消息，直到所有叶节点均收到消息</strong></em>*（从根到叶）****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps553.jpg" alt="img"> </p>
<h2 id="15-5-LDA话题模型"><a href="#15-5-LDA话题模型" class="headerlink" title="*15.5 LDA话题模型*"></a><em><strong>*15.5 LDA话题模型*</strong></em></h2><p>话题模型主要用于处理文本类数据，其中****隐狄利克雷分配模型****（Latent Dirichlet Allocation，简称LDA）是话题模型的杰出代表。在话题模型中，有以下几个基本概念：词（word）、文档（document）、话题（topic）。</p>
<p>****词****：最基本的离散单元；<br>****文档****：由一组词组成，词在文档中不计顺序；<br>****话题****：由一组特定的词组成，这组词具有较强的相关关系。</p>
<p>在现实任务中，一般我们可以得出一个文档的词频分布，但不知道该文档对应着哪些话题，LDA话题模型正是为了解决这个问题。具体来说：****LDA认为每篇文档包含多个话题，且其中每一个词都对应着一个话题****。因此可以假设文档是通过如下方式生成：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps554.jpg" alt="img"> </p>
<p>这样一个文档中的所有词都可以认为是通过话题模型来生成的，当已知一个文档的词频分布后（即一个N维向量，N为词库大小），则可以认为：****每一个词频元素都对应着一个话题，而话题对应的词频分布则影响着该词频元素的大小****。因此很容易写出LDA模型对应的联合概率函数：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps555.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps556.jpg" alt="img"></p>
<p>从上图可以看出，LDA的三个表示层被三种颜色表示出来：</p>
<p><em><strong>*corpus-level（红色）：*</strong></em> α和β表示语料级别的参数，也就是每个文档都一样，因此生成过程只采样一次。<br><em><strong>*document-level（橙色）：*</strong></em> θ是文档级别的变量，每个文档对应一个θ。<br><em><strong>*word-level（绿色）：*</strong></em> z和w都是单词级别变量，z由θ生成，w由z和β共同生成，一个单词w对应一个主题z。</p>
<p>通过上面对LDA生成模型的讨论，可以知道****LDA模型主要是想从给定的输入语料中学习训练出两个控制参数α和β****，当学习出了这两个控制参数就确定了模型，便可以用来生成文档。其中α和β分别对应以下各个信息：</p>
<p><em><strong>*α：分布p(θ)需要一个向量参数，即Dirichlet分布的参数，用于生成一个主题θ向量；*</strong></em> <em><strong>*<br>*</strong></em><em><strong>*β：各个主题对应的单词概率分布矩阵p(w|z)。*</strong></em></p>
<p>把w当做观察变量，θ和z当做隐藏变量，就可以通过EM算法学习出α和β，求解过程中遇到后验概率p(θ,z|w)无法直接求解，需要找一个似然函数下界来近似求解，原作者使用基于分解（factorization）假设的变分法（varialtional inference）进行计算，用到了EM算法。每次E-step输入α和β，计算似然函数，M-step最大化这个似然函数，算出α和β，不断迭代直到收敛。</p>
<h1 id="《机器学习》-学习笔记（17）–强化学习"><a href="#《机器学习》-学习笔记（17）–强化学习" class="headerlink" title="《机器学习》 学习笔记（17）–强化学习"></a><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011826404/article/details/75576856">《机器学习》 学习笔记（17）–强化学习</a></h1><p>上篇主要介绍了概率图模型，首先从生成式模型与判别式模型的定义出发，引出了概率图模型的基本概念，即利用图结构来表达变量之间的依赖关系；接着分别介绍了隐马尔可夫模型、马尔可夫随机场、条件随机场、精确推断方法以及LDA话题模型：HMM主要围绕着评估&#x2F;解码&#x2F;学习这三个实际问题展开论述；MRF基于团和势函数的概念来定义联合概率分布；CRF引入两种特征函数对状态序列进行评价打分；变量消去与信念传播在给定联合概率分布后计算特定变量的边际分布；LDA话题模型则试图去推断给定文档所蕴含的话题分布。本篇将介绍最后一种学习算法–强化学习。</p>
<h1 id="16、强化学习"><a href="#16、强化学习" class="headerlink" title="*16、强化学习*"></a><em><strong>*16、强化学习*</strong></em></h1><p>****强化学习*<em><strong>（Reinforcement Learning，简称</strong></em>*RL*<em><strong>）是机器学习的一个重要分支。在强化学习中，包含两种基本的元素：</strong></em>*状态*<em><strong>与</strong></em>*动作*<em><strong>，</strong></em>*在某个状态下执行某种动作，这便是一种策略****，学习器要做的就是通过不断地探索学习，从而获得一个好的策略。例如：在围棋中，一种落棋的局面就是一种状态，若能知道每种局面下的最优落子动作，那就攻无不克&#x2F;百战不殆了~</p>
<p>若将状态看作为属性，动作看作为标记，易知：****监督学习和强化学习都是在试图寻找一个映射，从已知属性&#x2F;状态推断出标记&#x2F;动作*<em><strong>，这样强化学习中的策略相当于监督学习中的分类&#x2F;回归器。但在实际问题中，</strong></em>*强化学习并没有监督学习那样的标记信息*<em><strong>，通常都是在</strong></em>*尝试动作后才能获得结果****，因此强化学习是通过反馈的结果信息不断调整之前的策略，从而算法能够学习到：在什么样的状态下选择什么样的动作可以获得最好的结果。</p>
<h2 id="16-1-基本要素"><a href="#16-1-基本要素" class="headerlink" title="*16.1 基本要素*"></a><em><strong>*16.1 基本要素*</strong></em></h2><p>强化学习任务通常使用****马尔可夫决策过程*<em><strong>（Markov Decision Process，简称</strong></em>*MDP****）来描述，具体而言：机器处在一个环境中，每个状态为机器对当前环境的感知；机器只能通过动作来影响环境，当机器执行一个动作后，会使得环境按某种概率转移到另一个状态；同时，环境会根据潜在的奖赏函数反馈给机器一个奖赏。综合而言，强化学习主要包含四个要素：状态、动作、转移概率以及奖赏函数。</p>
<p>****状态（X）****：机器对环境的感知，所有可能的状态称为状态空间；<br>****动作（A）****：机器所采取的动作，所有能采取的动作构成动作空间；<br>****转移概率（P）****：当执行某个动作后，当前状态会以某种概率转移到另一个状态；<br>****奖赏函数（R）****：在状态转移的同时，环境给反馈给机器一个奖赏。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps557.jpg" alt="img"> </p>
<p>因此，****强化学习的主要任务就是通过在环境中不断地尝试，根据尝试获得的反馈信息调整策略，最终生成一个较好的策略π，机器根据这个策略便能知道在什么状态下应该执行什么动作****。常见的策略表示方法有以下两种：</p>
<p>****确定性策略****：π（x）&#x3D;a，即在状态x下执行a动作； </p>
<p>****随机性策略****：P&#x3D;π（x,a），即在状态x下执行a动作的概率。</p>
<p>****一个策略的优劣取决于长期执行这一策略后的累积奖赏****，换句话说：可以使用累积奖赏来评估策略的好坏，最优策略则表示在初始状态下一直执行该策略后，最后的累积奖赏值最高。长期累积奖赏通常使用下述两种计算方法：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps558.jpg" alt="img"> </p>
<h2 id="16-2-K摇摆赌博机"><a href="#16-2-K摇摆赌博机" class="headerlink" title="*16.2 K摇摆赌博机*"></a><em><strong>*16.2 K摇摆赌博机*</strong></em></h2><p>首先我们考虑强化学习最简单的情形：仅考虑一步操作，即在状态x下只需执行一次动作a便能观察到奖赏结果。易知：欲最大化单步奖赏，我们需要知道每个动作带来的期望奖赏值，这样便能选择奖赏值最大的动作来执行。若每个动作的奖赏值为确定值，则只需要将每个动作尝试一遍即可，但大多数情形下，一个动作的奖赏值来源于一个概率分布，因此需要进行多次的尝试。</p>
<p>单步强化学习实质上是****K-摇臂赌博机*<em><strong>（K-armed bandit）的原型，一般我们</strong></em>*尝试动作的次数是有限的****，那如何利用有限的次数进行有效地探索呢？这里有两种基本的想法：</p>
<p>****仅探索法****：将尝试的机会平均分给每一个动作，即轮流执行，最终将每个动作的平均奖赏作为期望奖赏的近似值。 </p>
<p>****仅利用法****：将尝试的机会分给当前平均奖赏值最大的动作，隐含着让一部分人先富起来的思想。</p>
<p>可以看出：上述****两种方法是相互矛盾的****，仅探索法能较好地估算每个动作的期望奖赏，但是没能根据当前的反馈结果调整尝试策略；仅利用法在每次尝试之后都更新尝试策略，符合强化学习的思（tao）维（lu），但容易找不到最优动作。因此需要在这两者之间进行折中。</p>
<h3 id="16-2-1-ε-贪心"><a href="#16-2-1-ε-贪心" class="headerlink" title="*16.2.1 ε-贪心*"></a><em><strong>*16.2.1 ε-贪心*</strong></em></h3><p>****ε-贪心法基于一个概率来对探索和利用进行折中****，具体而言：在每次尝试时，以ε的概率进行探索，即以均匀概率随机选择一个动作；以1-ε的概率进行利用，即选择当前最优的动作。ε-贪心法只需记录每个动作的当前平均奖赏值与被选中的次数，便可以增量式更新。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps559.jpg" alt="img"> </p>
<h3 id="16-2-2-Softmax"><a href="#16-2-2-Softmax" class="headerlink" title="*16.2.2 Softmax*"></a><em><strong>*16.2.2 Softmax*</strong></em></h3><p>****Softmax算法则基于当前每个动作的平均奖赏值来对探索和利用进行折中，Softmax函数将一组值转化为一组概率****，值越大对应的概率也越高，因此当前平均奖赏值越高的动作被选中的几率也越大。Softmax函数如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps560.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps561.jpg" alt="img"></p>
<h2 id="16-3-有模型学习"><a href="#16-3-有模型学习" class="headerlink" title="*16.3 有模型学习*"></a><em><strong>*16.3 有模型学习*</strong></em></h2><p>若学习任务中的四个要素都已知，即状态空间、动作空间、转移概率以及奖赏函数都已经给出，这样的情形称为“****有模型学习****”。假设状态空间和动作空间均为有限，即均为离散值，这样我们不用通过尝试便可以对某个策略进行评估。</p>
<h3 id="16-3-1-策略评估"><a href="#16-3-1-策略评估" class="headerlink" title="*16.3.1 策略评估*"></a><em><strong>*16.3.1 策略评估*</strong></em></h3><p>前面提到：****在模型已知的前提下，我们可以对任意策略的进行评估****（后续会给出演算过程）。一般常使用以下两种值函数来评估某个策略的优劣：</p>
<p>****状态值函数（V）****：V（x），即从状态x出发，使用π策略所带来的累积奖赏； </p>
<p>****状态-动作值函数（Q）****：Q（x,a），即从状态x出发，执行动作a后再使用π策略所带来的累积奖赏。</p>
<p>根据累积奖赏的定义，我们可以引入T步累积奖赏与r折扣累积奖赏：<img src="https://raw.githubusercontent.com/ebxeax/images/main/wps562.jpg" alt="img"> </p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps563.jpg" alt="img"> </p>
<p>由于MDP具有马尔可夫性，即现在决定未来，将来和过去无关，我们很容易找到值函数的递归关系：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps564.jpg" alt="img"> </p>
<p>类似地，对于r折扣累积奖赏可以得到：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps565.jpg" alt="img"> </p>
<p>易知：****当模型已知时，策略的评估问题转化为一种动态规划问题****，即以填表格的形式自底向上，先求解每个状态的单步累积奖赏，再求解每个状态的两步累积奖赏，一直迭代逐步求解出每个状态的T步累积奖赏。算法流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps566.jpg" alt="img"> </p>
<p>对于状态-动作值函数，只需通过简单的转化便可得到：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps567.jpg" alt="img"> </p>
<h3 id="16-3-2-策略改进"><a href="#16-3-2-策略改进" class="headerlink" title="*16.3.2 策略改进*"></a><em><strong>*16.3.2 策略改进*</strong></em></h3><p>理想的策略应能使得每个状态的累积奖赏之和最大，简单来理解就是：不管处于什么状态，只要通过该策略执行动作，总能得到较好的结果。因此对于给定的某个策略，我们需要对其进行改进，从而得到****最优的值函数****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps568.jpg" alt="img"><br><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps569.jpg" alt="img"></p>
<p>最优Bellman等式改进策略的方式为：****将策略选择的动作改为当前最优的动作****，而不是像之前那样对每种可能的动作进行求和。易知：选择当前最优动作相当于将所有的概率都赋给累积奖赏值最大的动作，因此每次改进都会使得值函数单调递增。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps570.jpg" alt="img"> </p>
<p>将策略评估与策略改进结合起来，我们便得到了生成最优策略的方法：先给定一个随机策略，现对该策略进行评估，然后再改进，接着再评估&#x2F;改进一直到策略收敛、不再发生改变。这便是策略迭代算法，算法流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps571.jpg" alt="img"> </p>
<p>可以看出：策略迭代法在每次改进策略后都要对策略进行重新评估，因此比较耗时。若从最优化值函数的角度出发，即先迭代得到最优的值函数，再来计算如何改变策略，这便是值迭代算法，算法流程如下所示：</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps572.jpg" alt="img"> </p>
<h2 id="16-4-蒙特卡罗强化学习"><a href="#16-4-蒙特卡罗强化学习" class="headerlink" title="*16.4 蒙特卡罗强化学习*"></a><em><strong>*16.4 蒙特卡罗强化学习*</strong></em></h2><p>在现实的强化学习任务中，****环境的转移函数与奖赏函数往往很难得知*<em><strong>，因此我们需要考虑在不依赖于环境参数的条件下建立强化学习模型，这便是</strong></em>*免模型学习****。蒙特卡罗强化学习便是其中的一种经典方法。</p>
<p>由于模型参数未知，状态值函数不能像之前那样进行全概率展开，从而运用动态规划法求解。一种直接的方法便是通过采样来对策略进行评估&#x2F;估算其值函数，****蒙特卡罗强化学习正是基于采样来估计状态-动作值函数*<em><strong>：对采样轨迹中的每一对状态-动作，记录其后的奖赏值之和，作为该状态-动作的一次累积奖赏，通过多次采样后，使用累积奖赏的平均作为状态-动作值的估计，并</strong></em>*引入ε-贪心策略保证采样的多样性****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps573.jpg" alt="img"> </p>
<p>在上面的算法流程中，被评估和被改进的都是同一个策略，因此称为****同策略蒙特卡罗强化学习算法*<em><strong>。引入ε-贪心仅是为了便于采样评估，而在使用策略时并不需要ε-贪心，那能否仅在评估时使用ε-贪心策略，而在改进时使用原始策略呢？这便是</strong></em>*异策略蒙特卡罗强化学习算法****。</p>
<p><img src="https://raw.githubusercontent.com/ebxeax/images/main/wps574.jpg" alt="img"> </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/ML_001_introduce-machine-learning/" data-id="clkqazuog001odlbi09xdbi6i" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-数据表示" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/1970/01/01/%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%A4%BA/" class="article-date">
  <time class="dt-published" datetime="1970-01-01T00:00:00.000Z" itemprop="datePublished">1970-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="数据表示和运算"><a href="#数据表示和运算" class="headerlink" title="数据表示和运算"></a>数据表示和运算</h1><h2 id="进制"><a href="#进制" class="headerlink" title="进制"></a>进制</h2><ul>
<li><strong>r进制</strong><br> $K_{n} K_{n-1} K_{n-2} \dots K_{0} K_{-1} \dots K_{-m}$<br>数值表示 $K_{n} r^{n} + K_{n-1} r^{n-1} + \dots + K_{0} r^{0} + K_{-1} r^{-1} + \dots + K_{-m} r^{-m} &#x3D; \sum_{i&#x3D;n}^{-m} K_{i} r^{i}$</li>
<li>二进制<br>$01 \space r&#x3D;2$</li>
<li>八进制<br>$01234567 \space r&#x3D;8&#x3D;2^3$</li>
<li>十六进制<br>$0123456789ABCDEF \space r&#x3D;16&#x3D;2^4$</li>
</ul>
<h2 id="进制转换"><a href="#进制转换" class="headerlink" title="进制转换"></a>进制转换</h2><p>4位二进制数码与1位十六进制数码相对应<br>3位二进制数码与1位八进制数码相对应  </p>
<h2 id="原反补移"><a href="#原反补移" class="headerlink" title="原反补移"></a>原反补移</h2><ul>
<li><p>原码</p>
</li>
<li><p>反码</p>
</li>
<li><p>补码</p>
</li>
<li><p>移码</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://ebxeax.github.io/1970/01/01/%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%A4%BA/" data-id="clkqb3hgw0000dwbib1478xxo" data-title="" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/4/">&laquo; zurück</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Kategorien</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/interpreter-C/">interpreter C</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/02/13/CP_001_introduce-Interpreter/">write a C interpreter</a>
          </li>
        
          <li>
            <a href="/1970/01/01/2022-04-15-MarkdownGraph/">(no title)</a>
          </li>
        
          <li>
            <a href="/1970/01/01/2022-04-15-Win-KeX/">(no title)</a>
          </li>
        
          <li>
            <a href="/1970/01/01/2022-04-15-c%E5%8F%98%E9%95%BF%E5%8F%82%E6%95%B0%E5%88%97%E8%A1%A8/">(no title)</a>
          </li>
        
          <li>
            <a href="/1970/01/01/2022-06-15-015Perceptron/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 ebxeax<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>